# Epic 7: å®æ—¶è¯­éŸ³äº¤äº’ç³»ç»Ÿ

**Epic ID**: EPIC-007-VOICE-INTERACTION  
**ä¼˜å…ˆçº§**: é«˜ (P1)  
**é¢„ä¼°å·¥æœŸ**: 6-8å‘¨  
**è´Ÿè´£å›¢é˜Ÿ**: AIå›¢é˜Ÿ + å‰ç«¯å›¢é˜Ÿ  
**åˆ›å»ºæ—¥æœŸ**: 2025-08-19

## ğŸ“‹ Epicæ¦‚è¿°

æ„å»ºå®Œæ•´çš„å®æ—¶è¯­éŸ³äº¤äº’ç³»ç»Ÿï¼Œå®ç°è¯­éŸ³è½¬æ–‡æœ¬ã€æ–‡æœ¬è½¬è¯­éŸ³ã€è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«å’Œå®æ—¶å¯¹è¯èƒ½åŠ›ï¼Œè®©AI Agentæ”¯æŒè‡ªç„¶çš„è¯­éŸ³äº¤äº’ä½“éªŒï¼Œå®Œå–„å¤šæ¨¡æ€AIçš„è¯­éŸ³ç»´åº¦ã€‚

### ğŸ¯ ä¸šåŠ¡ä»·å€¼
- **è‡ªç„¶äº¤äº’**: æä¾›æ›´æ¥è¿‘äººç±»å¯¹è¯çš„äº¤äº’ä½“éªŒ
- **å¤šæ¨¡æ€å®Œå–„**: è¡¥å…¨é¡¹ç›®å¤šæ¨¡æ€AIèƒ½åŠ›(æ–‡æœ¬+å›¾åƒ+è¯­éŸ³)
- **æ— éšœç¢è®¿é—®**: æ”¯æŒè§†è§‰éšœç¢ç”¨æˆ·å’Œè§£æ”¾åŒæ‰‹åœºæ™¯
- **æŠ€æœ¯ç«äº‰åŠ›**: æŒæ¡å®æ—¶éŸ³é¢‘å¤„ç†å’Œè¯­éŸ³AIæŠ€æœ¯

## ğŸš€ æ ¸å¿ƒåŠŸèƒ½æ¸…å•

### 1. **å®æ—¶è¯­éŸ³è½¬æ–‡æœ¬(ASR)**
- Whisperæ¨¡å‹é›†æˆå’Œä¼˜åŒ–
- å®æ—¶æµå¼è¯­éŸ³è¯†åˆ«
- å¤šè¯­è¨€æ”¯æŒå’Œè‡ªåŠ¨æ£€æµ‹
- å™ªéŸ³æŠ‘åˆ¶å’ŒéŸ³é¢‘é¢„å¤„ç†

### 2. **æ–‡æœ¬è½¬è¯­éŸ³åˆæˆ(TTS)**
- é«˜è´¨é‡è¯­éŸ³åˆæˆå¼•æ“
- å¤šéŸ³è‰²å’Œæƒ…æ„Ÿè¡¨è¾¾
- å®æ—¶æµå¼è¯­éŸ³ç”Ÿæˆ
- è¯­éŸ³ä¸ªæ€§åŒ–å®šåˆ¶

### 3. **è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«**
- éŸ³é¢‘æƒ…æ„Ÿç‰¹å¾æå–
- æƒ…æ„ŸçŠ¶æ€åˆ†ç±»å’Œè¯„åˆ†
- å®æ—¶æƒ…æ„Ÿè·Ÿè¸ª
- æƒ…æ„Ÿé©±åŠ¨çš„å“åº”è°ƒæ•´

### 4. **è¯­éŸ³æ‰“æ–­å’Œæµæ§åˆ¶**
- è¯­éŸ³æ´»åŠ¨æ£€æµ‹(VAD)
- æ™ºèƒ½æ‰“æ–­å¤„ç†æœºåˆ¶
- å¯¹è¯è½®æ¬¡ç®¡ç†
- é™éŸ³æ£€æµ‹å’Œè¶…æ—¶å¤„ç†

### 5. **å¤šè½®å¯¹è¯ç®¡ç†**
- è¯­éŸ³ä¸Šä¸‹æ–‡ç†è§£
- å¯¹è¯çŠ¶æ€è·Ÿè¸ª
- è¯­éŸ³äº¤äº’å†å²è®°å½•
- è¯é¢˜è¿è´¯æ€§ç»´æŠ¤

### 6. **éŸ³é¢‘å¤„ç†å’Œä¼˜åŒ–**
- å›å£°æ¶ˆé™¤å’Œé™å™ª
- éŸ³é¢‘ç¼–è§£ç ä¼˜åŒ–
- ç½‘ç»œä¼ è¾“ä¼˜åŒ–
- è®¾å¤‡é€‚é…å’Œå…¼å®¹

## ğŸ—ï¸ ç”¨æˆ·æ•…äº‹åˆ†è§£

### Story 7.1: å®æ—¶è¯­éŸ³è½¬æ–‡æœ¬ç³»ç»Ÿ
**ä¼˜å…ˆçº§**: P1 | **å·¥æœŸ**: 2å‘¨
- é›†æˆOpenAI Whisperæˆ–äº‘ç«¯ASRæœåŠ¡
- å®ç°å®æ—¶æµå¼éŸ³é¢‘å¤„ç†
- æ”¯æŒå¤šè¯­è¨€è¯†åˆ«å’Œè¯­è¨€æ£€æµ‹
- å®ç°å™ªéŸ³æŠ‘åˆ¶å’ŒéŸ³é¢‘é¢„å¤„ç†

### Story 7.2: æ–‡æœ¬è½¬è¯­éŸ³åˆæˆå¼•æ“
**ä¼˜å…ˆçº§**: P1 | **å·¥æœŸ**: 2å‘¨
- é›†æˆé«˜è´¨é‡TTSæœåŠ¡(Azure/AWS/æœ¬åœ°æ¨¡å‹)
- å®ç°å¤šéŸ³è‰²é€‰æ‹©å’Œæƒ…æ„Ÿè°ƒèŠ‚
- æ”¯æŒæµå¼è¯­éŸ³ç”Ÿæˆå’Œæ’­æ”¾
- å®ç°è¯­éŸ³ç¼“å­˜å’Œä¼˜åŒ–

### Story 7.3: è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿ
**ä¼˜å…ˆçº§**: P2 | **å·¥æœŸ**: 1-2å‘¨
- å®ç°éŸ³é¢‘æƒ…æ„Ÿç‰¹å¾æå–
- è®­ç»ƒ/é›†æˆæƒ…æ„Ÿåˆ†ç±»æ¨¡å‹
- å®ç°å®æ—¶æƒ…æ„ŸçŠ¶æ€è·Ÿè¸ª
- é›†æˆæƒ…æ„Ÿåé¦ˆåˆ°å¯¹è¯ç³»ç»Ÿ

### Story 7.4: æ™ºèƒ½è¯­éŸ³æ‰“æ–­å¤„ç†
**ä¼˜å…ˆçº§**: P1 | **å·¥æœŸ**: 1å‘¨
- å®ç°è¯­éŸ³æ´»åŠ¨æ£€æµ‹(VAD)
- è®¾è®¡æ™ºèƒ½æ‰“æ–­ç­–ç•¥
- å®ç°å¯¹è¯è½®æ¬¡æ§åˆ¶
- å¤„ç†é™éŸ³å’Œè¶…æ—¶æƒ…å†µ

### Story 7.5: å¤šè½®è¯­éŸ³å¯¹è¯ç®¡ç†
**ä¼˜å…ˆçº§**: P1 | **å·¥æœŸ**: 2å‘¨
- é›†æˆè¯­éŸ³äº¤äº’åˆ°ç°æœ‰å¯¹è¯ç³»ç»Ÿ
- å®ç°è¯­éŸ³ä¸Šä¸‹æ–‡ç†è§£
- ç»´æŠ¤å¤šè½®å¯¹è¯çŠ¶æ€
- å®ç°è¯­éŸ³äº¤äº’å†å²ç®¡ç†

### Story 7.6: å‰ç«¯è¯­éŸ³äº¤äº’ç•Œé¢
**ä¼˜å…ˆçº§**: P1 | **å·¥æœŸ**: 2å‘¨
- å®ç°è¯­éŸ³è¾“å…¥UIç»„ä»¶
- è¯­éŸ³æ³¢å½¢å¯è§†åŒ–
- å®æ—¶è½¬å½•æ˜¾ç¤º
- è¯­éŸ³è®¾ç½®å’Œæ§åˆ¶é¢æ¿

### Story 7.7: éŸ³é¢‘ä¼˜åŒ–å’Œéƒ¨ç½²
**ä¼˜å…ˆçº§**: P2 | **å·¥æœŸ**: 1å‘¨
- éŸ³é¢‘è´¨é‡ä¼˜åŒ–å’Œè°ƒè¯•
- ç½‘ç»œä¼ è¾“ä¼˜åŒ–
- è®¾å¤‡å…¼å®¹æ€§æµ‹è¯•
- æ€§èƒ½ç›‘æ§å’Œå‘Šè­¦

## ğŸ¯ æˆåŠŸæ ‡å‡† (Definition of Done)

### æŠ€æœ¯æŒ‡æ ‡
- âœ… **è¯­éŸ³è¯†åˆ«å‡†ç¡®ç‡**: >95% (æ¸…æ™°ç¯å¢ƒä¸‹)
- âœ… **è¯­éŸ³åˆæˆè‡ªç„¶åº¦**: MOSè¯„åˆ†>4.0
- âœ… **ç«¯åˆ°ç«¯å»¶è¿Ÿ**: <800ms (è¯­éŸ³è¾“å…¥åˆ°è¯­éŸ³è¾“å‡º)
- âœ… **æƒ…æ„Ÿè¯†åˆ«å‡†ç¡®ç‡**: >85%
- âœ… **è¯­éŸ³æ´»åŠ¨æ£€æµ‹**: å‡†ç¡®ç‡>90%, å»¶è¿Ÿ<100ms

### åŠŸèƒ½æŒ‡æ ‡
- âœ… **å¤šè¯­è¨€æ”¯æŒ**: æ”¯æŒ5ç§ä»¥ä¸Šä¸»è¦è¯­è¨€
- âœ… **éŸ³è‰²é€‰æ‹©**: æä¾›10ç§ä»¥ä¸Šä¸åŒéŸ³è‰²
- âœ… **è®¾å¤‡å…¼å®¹**: æ”¯æŒä¸»æµæµè§ˆå™¨å’Œç§»åŠ¨è®¾å¤‡
- âœ… **å¹¶å‘ç”¨æˆ·**: æ”¯æŒ100+å¹¶å‘è¯­éŸ³äº¤äº’
- âœ… **ç¨³å®šæ€§**: 99.5%å¯ç”¨æ€§ï¼Œæ— æ˜æ˜¾éŸ³é¢‘æ•…éšœ

### ç”¨æˆ·ä½“éªŒæŒ‡æ ‡
- âœ… **ç”¨æˆ·æ»¡æ„åº¦**: è¯­éŸ³äº¤äº’æ»¡æ„åº¦>4.0/5.0
- âœ… **ä½¿ç”¨ä¾¿åˆ©æ€§**: ç”¨æˆ·èƒ½åœ¨3æ­¥å†…å¼€å§‹è¯­éŸ³å¯¹è¯
- âœ… **å“åº”è‡ªç„¶åº¦**: å¯¹è¯æµç•…æ€§è¯„åˆ†>4.0/5.0
- âœ… **é”™è¯¯æ¢å¤**: è¯†åˆ«é”™è¯¯å2æ¬¡å†…å®Œæˆçº æ­£

## ğŸ”§ æŠ€æœ¯å®ç°äº®ç‚¹

### å®æ—¶è¯­éŸ³è½¬æ–‡æœ¬ç³»ç»Ÿ
```python
import asyncio
import numpy as np
import torch
import whisper
from typing import AsyncGenerator, Optional
import webrtcvad
import pyaudio

class RealTimeASR:
    """å®æ—¶è¯­éŸ³è½¬æ–‡æœ¬"""
    
    def __init__(self, model_name: str = "base", language: Optional[str] = None):
        self.model = whisper.load_model(model_name)
        self.language = language
        self.vad = webrtcvad.Vad(3)  # é«˜çµæ•åº¦è¯­éŸ³æ´»åŠ¨æ£€æµ‹
        
        # éŸ³é¢‘å‚æ•°
        self.sample_rate = 16000
        self.frame_duration = 30  # ms
        self.frame_size = int(self.sample_rate * self.frame_duration / 1000)
        
        # ç¼“å†²åŒº
        self.audio_buffer = []
        self.silence_threshold = 1.0  # é™éŸ³é˜ˆå€¼(ç§’)
        self.min_speech_duration = 0.5  # æœ€å°è¯­éŸ³é•¿åº¦
        
    async def start_streaming(self) -> AsyncGenerator[str, None]:
        """å¼€å§‹æµå¼è¯†åˆ«"""
        
        # åˆå§‹åŒ–éŸ³é¢‘æµ
        audio_stream = pyaudio.PyAudio().open(
            format=pyaudio.paInt16,
            channels=1,
            rate=self.sample_rate,
            input=True,
            frames_per_buffer=self.frame_size,
            stream_callback=self._audio_callback
        )
        
        try:
            while True:
                # ç­‰å¾…è¯­éŸ³æ®µå®Œæˆ
                speech_segment = await self._wait_for_speech_segment()
                
                if len(speech_segment) > 0:
                    # è½¬å½•è¯­éŸ³
                    text = await self._transcribe_audio(speech_segment)
                    if text.strip():
                        yield text
                
                # çŸ­æš‚ä¼‘çœ é¿å…CPUè¿‡è½½
                await asyncio.sleep(0.01)
                
        finally:
            audio_stream.stop_stream()
            audio_stream.close()
    
    def _audio_callback(self, in_data, frame_count, time_info, status):
        """éŸ³é¢‘å›è°ƒå‡½æ•°"""
        audio_data = np.frombuffer(in_data, dtype=np.int16)
        
        # VADæ£€æµ‹
        is_speech = self.vad.is_speech(in_data, self.sample_rate)
        
        # æ·»åŠ åˆ°ç¼“å†²åŒº
        self.audio_buffer.append({
            'data': audio_data,
            'is_speech': is_speech,
            'timestamp': time.time()
        })
        
        # ä¿æŒç¼“å†²åŒºå¤§å°
        if len(self.audio_buffer) > 1000:  # çº¦30ç§’ç¼“å†²
            self.audio_buffer.pop(0)
        
        return (in_data, pyaudio.paContinue)
    
    async def _wait_for_speech_segment(self) -> np.ndarray:
        """ç­‰å¾…å®Œæ•´çš„è¯­éŸ³æ®µ"""
        speech_frames = []
        silence_duration = 0
        in_speech = False
        
        while True:
            if not self.audio_buffer:
                await asyncio.sleep(0.01)
                continue
            
            frame = self.audio_buffer.pop(0)
            
            if frame['is_speech']:
                speech_frames.append(frame['data'])
                in_speech = True
                silence_duration = 0
            else:
                if in_speech:
                    silence_duration += self.frame_duration / 1000.0
                    
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é™éŸ³é˜ˆå€¼
                    if silence_duration >= self.silence_threshold:
                        # è¯­éŸ³æ®µç»“æŸ
                        if len(speech_frames) > 0:
                            speech_audio = np.concatenate(speech_frames)
                            duration = len(speech_audio) / self.sample_rate
                            
                            if duration >= self.min_speech_duration:
                                return speech_audio
                        
                        # é‡ç½®çŠ¶æ€
                        speech_frames = []
                        in_speech = False
                        silence_duration = 0
    
    async def _transcribe_audio(self, audio_data: np.ndarray) -> str:
        """è½¬å½•éŸ³é¢‘æ•°æ®"""
        # éŸ³é¢‘é¢„å¤„ç†
        audio_float = audio_data.astype(np.float32) / 32768.0
        
        # Whisperè½¬å½•
        result = self.model.transcribe(
            audio_float,
            language=self.language,
            task='transcribe',
            fp16=False,
            verbose=False
        )
        
        return result['text']

class VoiceEmotionRecognizer:
    """è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«"""
    
    def __init__(self, model_path: Optional[str] = None):
        if model_path:
            self.model = torch.load(model_path)
        else:
            # ä½¿ç”¨é¢„è®­ç»ƒçš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹
            from transformers import pipeline
            self.classifier = pipeline(
                "audio-classification",
                model="superb/wav2vec2-base-superb-er"
            )
        
        self.emotion_history = []
        self.window_size = 5  # æƒ…æ„Ÿå¹³æ»‘çª—å£
    
    def extract_features(self, audio_data: np.ndarray) -> np.ndarray:
        """æå–éŸ³é¢‘ç‰¹å¾"""
        import librosa
        
        # MFCCç‰¹å¾
        mfccs = librosa.feature.mfcc(y=audio_data, sr=16000, n_mfcc=13)
        
        # é¢‘è°±è´¨å¿ƒ
        spectral_centroids = librosa.feature.spectral_centroid(y=audio_data, sr=16000)
        
        # è¿‡é›¶ç‡
        zero_crossing_rate = librosa.feature.zero_crossing_rate(audio_data)
        
        # éŸ³é«˜ç‰¹å¾
        pitches, magnitudes = librosa.piptrack(y=audio_data, sr=16000)
        
        # ç»„åˆç‰¹å¾
        features = np.concatenate([
            np.mean(mfccs.T, axis=0),
            np.mean(spectral_centroids.T, axis=0),
            np.mean(zero_crossing_rate.T, axis=0),
            np.mean(pitches, axis=1),
        ])
        
        return features
    
    def recognize_emotion(self, audio_data: np.ndarray) -> Dict[str, float]:
        """è¯†åˆ«è¯­éŸ³æƒ…æ„Ÿ"""
        
        # ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
        emotion_result = self.classifier(audio_data)
        
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        emotions = {}
        for item in emotion_result:
            emotion_name = item['label'].lower()
            confidence = item['score']
            emotions[emotion_name] = confidence
        
        # æ·»åŠ åˆ°å†å²è®°å½•
        self.emotion_history.append(emotions)
        if len(self.emotion_history) > self.window_size:
            self.emotion_history.pop(0)
        
        # å¹³æ»‘å¤„ç†
        smoothed_emotions = self._smooth_emotions()
        
        return smoothed_emotions
    
    def _smooth_emotions(self) -> Dict[str, float]:
        """æƒ…æ„Ÿå¹³æ»‘å¤„ç†"""
        if not self.emotion_history:
            return {}
        
        # è®¡ç®—ç§»åŠ¨å¹³å‡
        emotion_keys = self.emotion_history[0].keys()
        smoothed = {}
        
        for emotion in emotion_keys:
            values = [frame.get(emotion, 0) for frame in self.emotion_history]
            smoothed[emotion] = np.mean(values)
        
        return smoothed

class StreamingTTS:
    """æµå¼æ–‡æœ¬è½¬è¯­éŸ³"""
    
    def __init__(self, voice_id: str = "en-US-AriaNeural", rate: str = "+0%"):
        import azure.cognitiveservices.speech as speechsdk
        
        # Azure Speeché…ç½®
        self.speech_config = speechsdk.SpeechConfig(
            subscription="your_key",
            region="your_region"
        )
        self.speech_config.speech_synthesis_voice_name = voice_id
        self.speech_config.set_speech_synthesis_output_format(
            speechsdk.SpeechSynthesisOutputFormat.Raw16Khz16BitMonoPcm
        )
        
        # ç¼“å­˜é…ç½®
        self.audio_cache = {}
        self.max_cache_size = 1000
        
    async def synthesize_streaming(
        self, 
        text: str, 
        emotion: Optional[str] = None
    ) -> AsyncGenerator[bytes, None]:
        """æµå¼è¯­éŸ³åˆæˆ"""
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{text}_{emotion}"
        if cache_key in self.audio_cache:
            cached_audio = self.audio_cache[cache_key]
            chunk_size = 1024
            for i in range(0, len(cached_audio), chunk_size):
                yield cached_audio[i:i+chunk_size]
                await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿæµå¼æ’­æ”¾
            return
        
        # SSMLæ„å»º
        ssml_text = self._build_ssml(text, emotion)
        
        # è¯­éŸ³åˆæˆ
        synthesizer = speechsdk.SpeechSynthesizer(
            speech_config=self.speech_config,
            audio_config=None
        )
        
        # å¼‚æ­¥åˆæˆ
        result = await asyncio.get_event_loop().run_in_executor(
            None, 
            synthesizer.speak_ssml, 
            ssml_text
        )
        
        if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
            audio_data = result.audio_data
            
            # ç¼“å­˜ç»“æœ
            self._cache_audio(cache_key, audio_data)
            
            # æµå¼è¿”å›
            chunk_size = 1024
            for i in range(0, len(audio_data), chunk_size):
                yield audio_data[i:i+chunk_size]
                await asyncio.sleep(0.01)
        else:
            raise Exception(f"TTS synthesis failed: {result.reason}")
    
    def _build_ssml(self, text: str, emotion: Optional[str] = None) -> str:
        """æ„å»ºSSML"""
        ssml = f"""
        <speak version="1.0" xml:lang="en-US">
            <voice name="{self.speech_config.speech_synthesis_voice_name}">
        """
        
        if emotion:
            emotion_mapping = {
                'happy': 'cheerful',
                'sad': 'sad',
                'angry': 'angry',
                'neutral': 'friendly'
            }
            
            style = emotion_mapping.get(emotion, 'friendly')
            ssml += f'<mstts:express-as style="{style}">'
            ssml += text
            ssml += '</mstts:express-as>'
        else:
            ssml += text
        
        ssml += """
            </voice>
        </speak>
        """
        
        return ssml
    
    def _cache_audio(self, key: str, audio_data: bytes):
        """ç¼“å­˜éŸ³é¢‘æ•°æ®"""
        if len(self.audio_cache) >= self.max_cache_size:
            # ç§»é™¤æœ€æ—§çš„ç¼“å­˜
            oldest_key = next(iter(self.audio_cache))
            del self.audio_cache[oldest_key]
        
        self.audio_cache[key] = audio_data

class VoiceInteractionManager:
    """è¯­éŸ³äº¤äº’ç®¡ç†å™¨"""
    
    def __init__(self):
        self.asr = RealTimeASR()
        self.tts = StreamingTTS()
        self.emotion_recognizer = VoiceEmotionRecognizer()
        
        self.is_listening = False
        self.conversation_context = []
        self.current_emotion = "neutral"
        
    async def start_voice_conversation(self):
        """å¼€å§‹è¯­éŸ³å¯¹è¯"""
        self.is_listening = True
        
        try:
            # å¯åŠ¨è¯­éŸ³è¯†åˆ«æµ
            async for transcribed_text in self.asr.start_streaming():
                if not self.is_listening:
                    break
                
                print(f"ç”¨æˆ·è¯´: {transcribed_text}")
                
                # è¯†åˆ«æƒ…æ„Ÿ
                # audio_data = self.asr.get_last_audio_segment()
                # emotions = self.emotion_recognizer.recognize_emotion(audio_data)
                # dominant_emotion = max(emotions, key=emotions.get)
                
                # ç”Ÿæˆå›å¤
                response = await self._generate_response(
                    transcribed_text, 
                    context=self.conversation_context
                )
                
                print(f"AIå›å¤: {response}")
                
                # è¯­éŸ³åˆæˆå’Œæ’­æ”¾
                async for audio_chunk in self.tts.synthesize_streaming(
                    response, 
                    emotion=self.current_emotion
                ):
                    # æ’­æ”¾éŸ³é¢‘å—
                    await self._play_audio_chunk(audio_chunk)
                
                # æ›´æ–°å¯¹è¯ä¸Šä¸‹æ–‡
                self.conversation_context.append({
                    'user': transcribed_text,
                    'assistant': response,
                    'emotion': self.current_emotion,
                    'timestamp': datetime.now()
                })
                
                # ä¿æŒä¸Šä¸‹æ–‡é•¿åº¦
                if len(self.conversation_context) > 10:
                    self.conversation_context.pop(0)
        
        except Exception as e:
            print(f"è¯­éŸ³å¯¹è¯é”™è¯¯: {e}")
        finally:
            self.is_listening = False
    
    async def _generate_response(self, user_input: str, context: List[Dict]) -> str:
        """ç”Ÿæˆå›å¤"""
        # é›†æˆåˆ°ç°æœ‰çš„å¯¹è¯ç³»ç»Ÿ
        # è¿™é‡Œåº”è¯¥è°ƒç”¨ä½ çš„AI Agentç³»ç»Ÿ
        
        # ç®€åŒ–ç¤ºä¾‹
        context_text = ""
        for turn in context[-3:]:  # æœ€è¿‘3è½®å¯¹è¯
            context_text += f"User: {turn['user']}\nAssistant: {turn['assistant']}\n"
        
        # è°ƒç”¨LLMç”Ÿæˆå›å¤
        prompt = f"{context_text}User: {user_input}\nAssistant:"
        
        # è¿™é‡Œé›†æˆä½ çš„ç°æœ‰LLMæœåŠ¡
        response = await self._call_llm(prompt)
        
        return response
    
    async def _play_audio_chunk(self, audio_chunk: bytes):
        """æ’­æ”¾éŸ³é¢‘å—"""
        # è¿™é‡Œå®ç°éŸ³é¢‘æ’­æ”¾é€»è¾‘
        # å¯ä»¥ä½¿ç”¨WebRTCã€WebSocketç­‰å‘é€åˆ°å‰ç«¯æ’­æ”¾
        pass
    
    def stop_conversation(self):
        """åœæ­¢å¯¹è¯"""
        self.is_listening = False
```

### å‰ç«¯è¯­éŸ³äº¤äº’ç•Œé¢
```typescript
// apps/web/src/components/voice/VoiceInteractionPanel.tsx
import React, { useState, useRef, useEffect } from 'react';
import { Mic, MicOff, Volume2, VolumeX } from 'lucide-react';

interface VoiceInteractionPanelProps {
  onTranscription: (text: string) => void;
  onVoiceCommand: (command: string) => void;
}

export const VoiceInteractionPanel: React.FC<VoiceInteractionPanelProps> = ({
  onTranscription,
  onVoiceCommand
}) => {
  const [isListening, setIsListening] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [currentTranscription, setCurrentTranscription] = useState('');
  const [voiceLevel, setVoiceLevel] = useState(0);
  
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const wsRef = useRef<WebSocket | null>(null);
  
  useEffect(() => {
    // åˆå§‹åŒ–WebSocketè¿æ¥
    initializeWebSocket();
    
    return () => {
      if (wsRef.current) {
        wsRef.current.close();
      }
    };
  }, []);
  
  const initializeWebSocket = () => {
    const ws = new WebSocket(`ws://localhost:8000/api/v1/voice/ws`);
    
    ws.onopen = () => {
      console.log('Voice WebSocket connected');
    };
    
    ws.onmessage = (event) => {
      const data = JSON.parse(event.data);
      
      switch (data.type) {
        case 'transcription':
          setCurrentTranscription(data.text);
          onTranscription(data.text);
          break;
        case 'audio_response':
          playAudioResponse(data.audio_data);
          break;
        case 'voice_level':
          setVoiceLevel(data.level);
          break;
      }
    };
    
    ws.onerror = (error) => {
      console.error('Voice WebSocket error:', error);
    };
    
    wsRef.current = ws;
  };
  
  const startListening = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      });
      
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus'
      });
      
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
          
          // å‘é€éŸ³é¢‘æ•°æ®åˆ°åç«¯
          if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
            wsRef.current.send(JSON.stringify({
              type: 'audio_chunk',
              data: Array.from(new Uint8Array(event.data))
            }));
          }
        }
      };
      
      mediaRecorder.onstop = () => {
        // åœæ­¢å½•éŸ³å¤„ç†
        stream.getTracks().forEach(track => track.stop());
      };
      
      // å¼€å§‹å½•éŸ³
      mediaRecorder.start(100); // æ¯100mså‘é€ä¸€æ¬¡æ•°æ®
      mediaRecorderRef.current = mediaRecorder;
      setIsListening(true);
      
      // å¯åŠ¨éŸ³é‡æ£€æµ‹
      startVolumeDetection(stream);
      
    } catch (error) {
      console.error('Error starting voice recording:', error);
    }
  };
  
  const stopListening = () => {
    if (mediaRecorderRef.current) {
      mediaRecorderRef.current.stop();
      mediaRecorderRef.current = null;
    }
    
    setIsListening(false);
    setVoiceLevel(0);
    
    // å‘é€åœæ­¢ä¿¡å·
    if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
      wsRef.current.send(JSON.stringify({
        type: 'stop_listening'
      }));
    }
  };
  
  const startVolumeDetection = (stream: MediaStream) => {
    const audioContext = new AudioContext();
    const analyser = audioContext.createAnalyser();
    const microphone = audioContext.createMediaStreamSource(stream);
    
    microphone.connect(analyser);
    analyser.fftSize = 256;
    
    const bufferLength = analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);
    
    const updateVolume = () => {
      if (isListening) {
        analyser.getByteFrequencyData(dataArray);
        
        const sum = dataArray.reduce((a, b) => a + b, 0);
        const average = sum / bufferLength;
        const normalizedLevel = Math.min(100, (average / 128) * 100);
        
        setVoiceLevel(normalizedLevel);
        requestAnimationFrame(updateVolume);
      }
    };
    
    updateVolume();
  };
  
  const playAudioResponse = (audioData: string) => {
    setIsSpeaking(true);
    
    // è§£ç base64éŸ³é¢‘æ•°æ®
    const binaryString = atob(audioData);
    const bytes = new Uint8Array(binaryString.length);
    for (let i = 0; i < binaryString.length; i++) {
      bytes[i] = binaryString.charCodeAt(i);
    }
    
    // åˆ›å»ºéŸ³é¢‘ä¸Šä¸‹æ–‡å¹¶æ’­æ”¾
    const audioContext = new AudioContext();
    audioContext.decodeAudioData(bytes.buffer).then(audioBuffer => {
      const source = audioContext.createBufferSource();
      source.buffer = audioBuffer;
      source.connect(audioContext.destination);
      
      source.onended = () => {
        setIsSpeaking(false);
      };
      
      source.start();
    }).catch(error => {
      console.error('Error playing audio:', error);
      setIsSpeaking(false);
    });
  };
  
  return (
    <div className="voice-interaction-panel bg-white dark:bg-gray-800 rounded-lg shadow-lg p-6">
      {/* è¯­éŸ³æ§åˆ¶æŒ‰é’® */}
      <div className="flex justify-center items-center space-x-4 mb-6">
        <button
          onClick={isListening ? stopListening : startListening}
          className={`p-4 rounded-full transition-all duration-200 ${
            isListening
              ? 'bg-red-500 hover:bg-red-600 text-white animate-pulse'
              : 'bg-blue-500 hover:bg-blue-600 text-white'
          }`}
          disabled={isSpeaking}
        >
          {isListening ? (
            <MicOff className="w-8 h-8" />
          ) : (
            <Mic className="w-8 h-8" />
          )}
        </button>
        
        <div className="flex flex-col items-center">
          <div className={`p-2 rounded-full ${isSpeaking ? 'bg-green-500' : 'bg-gray-300'}`}>
            {isSpeaking ? (
              <Volume2 className="w-6 h-6 text-white" />
            ) : (
              <VolumeX className="w-6 h-6 text-gray-600" />
            )}
          </div>
          <span className="text-sm text-gray-500 mt-1">
            {isSpeaking ? 'æ­£åœ¨æ’­æ”¾' : 'é™éŸ³'}
          </span>
        </div>
      </div>
      
      {/* éŸ³é‡æŒ‡ç¤ºå™¨ */}
      {isListening && (
        <div className="mb-4">
          <div className="flex justify-center items-center">
            <div className="w-64 h-2 bg-gray-200 rounded-full overflow-hidden">
              <div
                className="h-full bg-blue-500 transition-all duration-100"
                style={{ width: `${voiceLevel}%` }}
              />
            </div>
          </div>
          <p className="text-center text-sm text-gray-500 mt-2">
            éŸ³é‡: {Math.round(voiceLevel)}%
          </p>
        </div>
      )}
      
      {/* å®æ—¶è½¬å½•æ˜¾ç¤º */}
      <div className="voice-transcription bg-gray-50 dark:bg-gray-700 rounded-lg p-4 min-h-[100px]">
        <h3 className="text-lg font-semibold mb-2 text-gray-800 dark:text-gray-200">
          å®æ—¶è½¬å½•
        </h3>
        {currentTranscription ? (
          <p className="text-gray-700 dark:text-gray-300">
            {currentTranscription}
          </p>
        ) : (
          <p className="text-gray-400 italic">
            {isListening ? 'æ­£åœ¨å¬...' : 'ç‚¹å‡»éº¦å…‹é£å¼€å§‹è¯­éŸ³è¾“å…¥'}
          </p>
        )}
      </div>
      
      {/* è¯­éŸ³æ³¢å½¢å¯è§†åŒ– */}
      {isListening && (
        <div className="mt-4">
          <div className="flex justify-center items-end space-x-1 h-16">
            {Array.from({ length: 20 }, (_, i) => (
              <div
                key={i}
                className="bg-blue-500 w-2 rounded-t transition-all duration-100"
                style={{
                  height: `${Math.random() * voiceLevel + 5}%`,
                  opacity: voiceLevel > 10 ? 1 : 0.3
                }}
              />
            ))}
          </div>
        </div>
      )}
      
      {/* è¯­éŸ³è®¾ç½® */}
      <div className="mt-6 pt-4 border-t border-gray-200 dark:border-gray-600">
        <h4 className="text-md font-medium mb-3 text-gray-800 dark:text-gray-200">
          è¯­éŸ³è®¾ç½®
        </h4>
        <div className="grid grid-cols-2 gap-4">
          <div>
            <label className="block text-sm text-gray-600 dark:text-gray-400">
              è¯­è¨€é€‰æ‹©
            </label>
            <select className="mt-1 block w-full rounded border-gray-300 dark:border-gray-600 dark:bg-gray-700">
              <option value="zh-CN">ä¸­æ–‡</option>
              <option value="en-US">English</option>
              <option value="ja-JP">æ—¥æœ¬èª</option>
            </select>
          </div>
          <div>
            <label className="block text-sm text-gray-600 dark:text-gray-400">
              éŸ³è‰²é€‰æ‹©
            </label>
            <select className="mt-1 block w-full rounded border-gray-300 dark:border-gray-600 dark:bg-gray-700">
              <option value="female1">å¥³å£°1</option>
              <option value="male1">ç”·å£°1</option>
              <option value="child1">ç«¥å£°</option>
            </select>
          </div>
        </div>
      </div>
    </div>
  );
};

export default VoiceInteractionPanel;
```

## ğŸš¦ é£é™©è¯„ä¼°ä¸ç¼“è§£

### é«˜é£é™©é¡¹
1. **å®æ—¶æ€§èƒ½è¦æ±‚é«˜**
   - ç¼“è§£: ä¼˜åŒ–éŸ³é¢‘å¤„ç†ç®¡é“ï¼Œä½¿ç”¨é«˜æ•ˆç¼–è§£ç å™¨
   - éªŒè¯: å»¶è¿Ÿå‹åŠ›æµ‹è¯•ï¼Œç¡®ä¿<800msç«¯åˆ°ç«¯å“åº”

2. **éŸ³é¢‘è´¨é‡å—ç¯å¢ƒå½±å“**
   - ç¼“è§£: å®ç°å™ªéŸ³æŠ‘åˆ¶ã€å›å£°æ¶ˆé™¤ã€è‡ªåŠ¨å¢ç›Šæ§åˆ¶
   - éªŒè¯: ä¸åŒç¯å¢ƒä¸‹çš„è¯­éŸ³è¯†åˆ«å‡†ç¡®ç‡æµ‹è¯•

3. **å¤šè¯­è¨€æ”¯æŒå¤æ‚æ€§**
   - ç¼“è§£: é€æ­¥æ”¯æŒä¸»è¦è¯­è¨€ï¼Œä½¿ç”¨æˆç†Ÿçš„å¤šè¯­è¨€æ¨¡å‹
   - éªŒè¯: å„è¯­è¨€çš„è¯†åˆ«å’Œåˆæˆè´¨é‡æµ‹è¯•

### ä¸­é£é™©é¡¹
1. **ç½‘ç»œä¼ è¾“ä¼˜åŒ–**
   - ç¼“è§£: éŸ³é¢‘å‹ç¼©ã€æµå¼ä¼ è¾“ã€æ–­çº¿é‡è¿æœºåˆ¶
   - éªŒè¯: å¼±ç½‘ç¯å¢ƒä¸‹çš„ç¨³å®šæ€§æµ‹è¯•

2. **è®¾å¤‡å…¼å®¹æ€§**
   - ç¼“è§£: æ¸è¿›å¼å¢å¼ºã€è®¾å¤‡èƒ½åŠ›æ£€æµ‹ã€é™çº§æ–¹æ¡ˆ
   - éªŒè¯: ä¸»æµè®¾å¤‡å’Œæµè§ˆå™¨çš„å…¼å®¹æ€§æµ‹è¯•

## ğŸ“… å®æ–½è·¯çº¿å›¾

### Phase 1: åŸºç¡€è¯­éŸ³èƒ½åŠ› (Week 1-3)
- å®æ—¶è¯­éŸ³è½¬æ–‡æœ¬ç³»ç»Ÿ
- æ–‡æœ¬è½¬è¯­éŸ³åˆæˆå¼•æ“
- åŸºç¡€éŸ³é¢‘å¤„ç†

### Phase 2: æ™ºèƒ½äº¤äº’ (Week 4-5)
- è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«
- æ™ºèƒ½æ‰“æ–­å¤„ç†
- å¯¹è¯ç®¡ç†é›†æˆ

### Phase 3: å‰ç«¯ç•Œé¢ (Week 6-7)
- è¯­éŸ³äº¤äº’UIç»„ä»¶
- å®æ—¶å¯è§†åŒ–
- ç”¨æˆ·è®¾ç½®é¢æ¿

### Phase 4: ä¼˜åŒ–ä¸Šçº¿ (Week 8)
- æ€§èƒ½ä¼˜åŒ–è°ƒè¯•
- å…¼å®¹æ€§æµ‹è¯•
- ç”Ÿäº§éƒ¨ç½²

---

**æ–‡æ¡£çŠ¶æ€**: âœ… å®Œæˆ  
**ä¸‹ä¸€æ­¥**: å¼€å§‹Story 7.1çš„å®æ—¶è¯­éŸ³è½¬æ–‡æœ¬ç³»ç»Ÿå®æ–½  
**ä¾èµ–Epic**: å¯ä¸å…¶ä»–Epicå¹¶è¡Œå¼€å‘