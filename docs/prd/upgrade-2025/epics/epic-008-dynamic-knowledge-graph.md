# Epic 8: åŠ¨æ€çŸ¥è¯†å›¾è°±ç³»ç»Ÿ

**Epic ID**: EPIC-008-DYNAMIC-KNOWLEDGE-GRAPH  
**ä¼˜å…ˆçº§**: é«˜ (P1)  
**é¢„ä¼°å·¥æœŸ**: 10-12å‘¨  
**è´Ÿè´£å›¢é˜Ÿ**: AIå›¢é˜Ÿ + åç«¯å›¢é˜Ÿ  
**åˆ›å»ºæ—¥æœŸ**: 2025-08-19

## ğŸ“‹ Epicæ¦‚è¿°

æ„å»ºåŠ¨æ€çŸ¥è¯†å›¾è°±ç³»ç»Ÿï¼Œå®ç°å®ä½“è¯†åˆ«ä¸å…³ç³»æŠ½å–ã€åŠ¨æ€å›¾è°±æ„å»ºä¸æ›´æ–°ã€å›¾è°±æ¨ç†ä¸æŸ¥è¯¢ï¼Œä»¥åŠä¸RAGç³»ç»Ÿçš„æ·±åº¦èåˆ(GraphRAG)ï¼Œè®©AI Agentå…·å¤‡ç»“æ„åŒ–çŸ¥è¯†è¡¨ç¤ºã€æ¨ç†å’ŒåŠ¨æ€å­¦ä¹ èƒ½åŠ›ã€‚

### ğŸ¯ ä¸šåŠ¡ä»·å€¼
- **ç»“æ„åŒ–çŸ¥è¯†**: å°†éç»“æ„åŒ–æ–‡æœ¬è½¬æ¢ä¸ºå¯æ¨ç†çš„çŸ¥è¯†å›¾è°±
- **åŠ¨æ€æ›´æ–°**: å®æ—¶å­¦ä¹ å’Œæ›´æ–°çŸ¥è¯†ï¼Œä¿æŒçŸ¥è¯†åº“çš„æ—¶æ•ˆæ€§
- **æ·±åº¦æ¨ç†**: åŸºäºå›¾ç»“æ„çš„å¤šè·³æ¨ç†å’Œå…³è”åˆ†æ
- **æŠ€æœ¯ç«äº‰åŠ›**: æŒæ¡çŸ¥è¯†å›¾è°±å’ŒGraphRAGçš„å…ˆè¿›æŠ€æœ¯

## ğŸš€ æ ¸å¿ƒåŠŸèƒ½æ¸…å•

### 1. **å®ä½“è¯†åˆ«ä¸å…³ç³»æŠ½å–(NER+RE)**
- å‘½åå®ä½“è¯†åˆ«(äººç‰©ã€åœ°ç‚¹ã€ç»„ç»‡ã€æ—¶é—´ç­‰)
- å…³ç³»ä¸‰å…ƒç»„æŠ½å–(ä¸»è¯­-è°“è¯­-å®¾è¯­)
- å®ä½“é“¾æ¥å’Œæ¶ˆæ­§
- å¤šè¯­è¨€å®ä½“æŠ½å–æ”¯æŒ

### 2. **åŠ¨æ€çŸ¥è¯†å›¾è°±æ„å»º**
- å¢é‡å¼å›¾è°±æ„å»º
- å®ä½“å’Œå…³ç³»çš„åŠ¨æ€æ›´æ–°
- çŸ¥è¯†å†²çªæ£€æµ‹å’Œè§£å†³
- å›¾è°±è´¨é‡è¯„ä¼°å’Œä¼˜åŒ–

### 3. **å›¾è°±æ¨ç†å¼•æ“**
- åŸºäºè§„åˆ™çš„æ¨ç†(SWRL)
- åŸºäºåµŒå…¥çš„æ¨ç†(TransEã€RotatE)
- å¤šè·³å…³ç³»æ¨ç†
- ä¸ç¡®å®šæ€§æ¨ç†å’Œç½®ä¿¡åº¦è®¡ç®—

### 4. **GraphRAGç³»ç»Ÿé›†æˆ**
- å›¾è°±å¢å¼ºçš„æ–‡æ¡£æ£€ç´¢
- å®ä½“å’Œå…³ç³»çš„ä¸Šä¸‹æ–‡æ‰©å±•
- å›¾è°±å¼•å¯¼çš„é—®é¢˜åˆ†è§£
- å¤šæºçŸ¥è¯†èåˆ

### 5. **å¯è§†åŒ–å’ŒæŸ¥è¯¢æ¥å£**
- äº¤äº’å¼çŸ¥è¯†å›¾è°±å¯è§†åŒ–
- è‡ªç„¶è¯­è¨€åˆ°å›¾æŸ¥è¯¢è½¬æ¢
- SPARQLæŸ¥è¯¢æ¥å£
- çŸ¥è¯†æ¢ç´¢å’Œå‘ç°å·¥å…·

### 6. **çŸ¥è¯†å›¾è°±ç®¡ç†**
- å›¾è°±ç‰ˆæœ¬ç®¡ç†å’Œå›æº¯
- çŸ¥è¯†æ¥æºè¿½è¸ª
- å›¾è°±ç»Ÿè®¡å’Œåˆ†æ
- æ•°æ®å¯¼å…¥å¯¼å‡ºå·¥å…·

## ğŸ—ï¸ ç”¨æˆ·æ•…äº‹åˆ†è§£

### Story 8.1: å®ä½“è¯†åˆ«ä¸å…³ç³»æŠ½å–å¼•æ“
**ä¼˜å…ˆçº§**: P1 | **å·¥æœŸ**: 3å‘¨
- é›†æˆspaCyã€Stanzaç­‰NERæ¨¡å‹
- å®ç°å…³ç³»æŠ½å–æ¨¡å‹(BERT-based)
- æ„å»ºå®ä½“é“¾æ¥å’Œæ¶ˆæ­§ç®—æ³•
- æ”¯æŒä¸­è‹±æ–‡ç­‰å¤šè¯­è¨€å¤„ç†

### Story 8.2: åŠ¨æ€çŸ¥è¯†å›¾è°±å­˜å‚¨ç³»ç»Ÿ
**ä¼˜å…ˆçº§**: P1 | **å·¥æœŸ**: 2-3å‘¨
- é€‰æ‹©å’Œé›†æˆå›¾æ•°æ®åº“(Neo4j/ArangoDB)
- è®¾è®¡çŸ¥è¯†å›¾è°±æ•°æ®æ¨¡å‹
- å®ç°å¢é‡æ›´æ–°å’Œå†²çªè§£å†³
- æ„å»ºå›¾è°±è´¨é‡è¯„ä¼°æ¡†æ¶

### Story 8.3: å›¾è°±æ¨ç†å¼•æ“
**ä¼˜å…ˆçº§**: P1 | **å·¥æœŸ**: 3-4å‘¨
- å®ç°åŸºäºè§„åˆ™çš„æ¨ç†å¼•æ“
- é›†æˆçŸ¥è¯†å›¾è°±åµŒå…¥æ¨¡å‹
- æ„å»ºå¤šè·³æ¨ç†ç®—æ³•
- å®ç°ç½®ä¿¡åº¦è®¡ç®—å’Œä¸ç¡®å®šæ€§å¤„ç†

### Story 8.4: GraphRAGç³»ç»Ÿé›†æˆ
**ä¼˜å…ˆçº§**: P1 | **å·¥æœŸ**: 2-3å‘¨
- æ‰©å±•ç°æœ‰RAGç³»ç»Ÿæ”¯æŒå›¾è°±
- å®ç°å®ä½“å’Œå…³ç³»çš„ä¸Šä¸‹æ–‡æ‰©å±•
- æ„å»ºå›¾è°±å¼•å¯¼çš„é—®é¢˜åˆ†è§£
- é›†æˆå¤šæºçŸ¥è¯†èåˆç®—æ³•

### Story 8.5: çŸ¥è¯†å›¾è°±å¯è§†åŒ–ç•Œé¢
**ä¼˜å…ˆçº§**: P2 | **å·¥æœŸ**: 2å‘¨
- å®ç°äº¤äº’å¼å›¾è°±å¯è§†åŒ–(D3.js/Cytoscape)
- æ„å»ºè‡ªç„¶è¯­è¨€æŸ¥è¯¢ç•Œé¢
- å®ç°çŸ¥è¯†æ¢ç´¢å’Œå‘ç°å·¥å…·
- åˆ›å»ºå›¾è°±ç»Ÿè®¡ä»ªè¡¨æ¿

### Story 8.6: çŸ¥è¯†ç®¡ç†å’ŒAPIæ¥å£
**ä¼˜å…ˆçº§**: P2 | **å·¥æœŸ**: 1-2å‘¨
- å®ç°SPARQLæŸ¥è¯¢æ¥å£
- æ„å»ºçŸ¥è¯†å›¾è°±ç®¡ç†API
- å®ç°æ•°æ®å¯¼å…¥å¯¼å‡ºåŠŸèƒ½
- åˆ›å»ºç‰ˆæœ¬ç®¡ç†å’Œè¿½è¸ªç³»ç»Ÿ

### Story 8.7: ç³»ç»Ÿä¼˜åŒ–å’Œéƒ¨ç½²
**ä¼˜å…ˆçº§**: P1 | **å·¥æœŸ**: 1-2å‘¨
- æ€§èƒ½è°ƒä¼˜å’Œæ‰©å®¹å‡†å¤‡
- é›†æˆæµ‹è¯•å’Œè´¨é‡ä¿è¯
- ç›‘æ§å‘Šè­¦ç³»ç»Ÿé›†æˆ
- ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

## ğŸ¯ æˆåŠŸæ ‡å‡† (Definition of Done)

### æŠ€æœ¯æŒ‡æ ‡
- âœ… **å®ä½“è¯†åˆ«å‡†ç¡®ç‡**: >90% (æ ‡å‡†æ•°æ®é›†)
- âœ… **å…³ç³»æŠ½å–F1å€¼**: >85% (æ ‡å‡†æ•°æ®é›†)
- âœ… **å›¾è°±æŸ¥è¯¢å»¶è¿Ÿ**: <500ms (å•è·³æŸ¥è¯¢)
- âœ… **å¤šè·³æ¨ç†å‡†ç¡®ç‡**: >80% (3è·³å†…æ¨ç†)
- âœ… **GraphRAGæå‡**: ç›¸æ¯”ä¼ ç»ŸRAGå‡†ç¡®ç‡æå‡25%+

### åŠŸèƒ½æŒ‡æ ‡
- âœ… **å®ä½“ç±»å‹è¦†ç›–**: æ”¯æŒ20ç§ä»¥ä¸Šå®ä½“ç±»å‹
- âœ… **å…³ç³»ç±»å‹è¦†ç›–**: æ”¯æŒ50ç§ä»¥ä¸Šå…³ç³»ç±»å‹
- âœ… **è¯­è¨€æ”¯æŒ**: ä¸­æ–‡ã€è‹±æ–‡åŒè¯­æ”¯æŒ
- âœ… **å¹¶å‘æŸ¥è¯¢**: æ”¯æŒ1000+å¹¶å‘å›¾è°±æŸ¥è¯¢
- âœ… **æ•°æ®è§„æ¨¡**: æ”¯æŒç™¾ä¸‡çº§å®ä½“å’Œåƒä¸‡çº§å…³ç³»

### è´¨é‡æ ‡å‡†
- âœ… **æµ‹è¯•è¦†ç›–ç‡â‰¥85%**: å•å…ƒæµ‹è¯• + é›†æˆæµ‹è¯• + E2Eæµ‹è¯•
- âœ… **å›¾è°±è´¨é‡åˆ†æ•°**: >8.0/10.0 (å®Œæ•´æ€§ã€ä¸€è‡´æ€§ã€å‡†ç¡®æ€§ç»¼åˆè¯„åˆ†)
- âœ… **ç³»ç»Ÿç¨³å®šæ€§**: 99.5%å¯ç”¨æ€§ï¼ŒMTTR<15åˆ†é’Ÿ
- âœ… **çŸ¥è¯†æ—¶æ•ˆæ€§**: 90%çŸ¥è¯†åœ¨24å°æ—¶å†…æ›´æ–°

## ğŸ”§ æŠ€æœ¯å®ç°äº®ç‚¹

### å®ä½“è¯†åˆ«ä¸å…³ç³»æŠ½å–å¼•æ“
```python
import spacy
import torch
from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline
import networkx as nx
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
import json

@dataclass
class Entity:
    text: str
    label: str
    start: int
    end: int
    confidence: float
    canonical_form: Optional[str] = None
    linked_entity: Optional[str] = None

@dataclass
class Relation:
    subject: Entity
    predicate: str
    object: Entity
    confidence: float
    context: str

class EntityRecognizer:
    """å‘½åå®ä½“è¯†åˆ«å™¨"""
    
    def __init__(self, model_name: str = "dbmdz/bert-large-cased-finetuned-conll03-english"):
        # åŠ è½½é¢„è®­ç»ƒçš„NERæ¨¡å‹
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForTokenClassification.from_pretrained(model_name)
        self.ner_pipeline = pipeline(
            "ner", 
            model=self.model, 
            tokenizer=self.tokenizer,
            aggregation_strategy="simple"
        )
        
        # åŠ è½½spaCyæ¨¡å‹ç”¨äºé¢å¤–å¤„ç†
        self.nlp = spacy.load("en_core_web_lg")
        
        # å®ä½“ç±»å‹æ˜ å°„
        self.entity_types = {
            'PERSON': 'Person',
            'ORG': 'Organization', 
            'GPE': 'Location',
            'DATE': 'Date',
            'MONEY': 'Money',
            'PERCENT': 'Percentage',
            'FACILITY': 'Facility',
            'PRODUCT': 'Product',
            'EVENT': 'Event',
            'WORK_OF_ART': 'WorkOfArt',
            'LAW': 'Law',
            'LANGUAGE': 'Language'
        }
    
    def extract_entities(self, text: str) -> List[Entity]:
        """æå–å‘½åå®ä½“"""
        entities = []
        
        # ä½¿ç”¨BERT-based NER
        ner_results = self.ner_pipeline(text)
        
        for result in ner_results:
            entity = Entity(
                text=result['word'],
                label=self.entity_types.get(result['entity_group'], result['entity_group']),
                start=result['start'],
                end=result['end'],
                confidence=result['score']
            )
            entities.append(entity)
        
        # ä½¿ç”¨spaCyè¿›è¡Œè¡¥å……å’ŒéªŒè¯
        doc = self.nlp(text)
        
        for ent in doc.ents:
            # æ£€æŸ¥æ˜¯å¦å·²ç»è¢«BERTè¯†åˆ«
            overlap = False
            for existing_entity in entities:
                if (ent.start_char >= existing_entity.start and 
                    ent.start_char <= existing_entity.end):
                    overlap = True
                    break
            
            if not overlap:
                entity = Entity(
                    text=ent.text,
                    label=self.entity_types.get(ent.label_, ent.label_),
                    start=ent.start_char,
                    end=ent.end_char,
                    confidence=0.8  # spaCyé»˜è®¤ç½®ä¿¡åº¦
                )
                entities.append(entity)
        
        # å®ä½“é“¾æ¥å’Œè§„èŒƒåŒ–
        entities = self._link_entities(entities, text)
        
        return sorted(entities, key=lambda x: x.start)
    
    def _link_entities(self, entities: List[Entity], context: str) -> List[Entity]:
        """å®ä½“é“¾æ¥å’Œè§„èŒƒåŒ–"""
        for entity in entities:
            # ç®€å•çš„è§„èŒƒåŒ–å¤„ç†
            canonical_form = entity.text.strip().title()
            
            # å®ä½“æ¶ˆæ­§ - è¿™é‡Œå¯ä»¥é›†æˆæ›´å¤æ‚çš„å®ä½“é“¾æ¥ç®—æ³•
            if entity.label == 'Person':
                canonical_form = self._normalize_person_name(entity.text)
            elif entity.label == 'Organization':
                canonical_form = self._normalize_organization_name(entity.text)
            
            entity.canonical_form = canonical_form
            entity.linked_entity = self._find_linked_entity(canonical_form, entity.label)
        
        return entities
    
    def _normalize_person_name(self, name: str) -> str:
        """äººåè§„èŒƒåŒ–"""
        # ç®€å•çš„äººåå¤„ç†é€»è¾‘
        name_parts = name.strip().split()
        if len(name_parts) >= 2:
            return f"{name_parts[-1]}, {' '.join(name_parts[:-1])}"
        return name.strip().title()
    
    def _normalize_organization_name(self, org: str) -> str:
        """ç»„ç»‡åè§„èŒƒåŒ–"""
        # ç§»é™¤å¸¸è§åç¼€
        suffixes = ['Inc.', 'Corp.', 'LLC', 'Ltd.', 'Co.']
        normalized = org.strip()
        
        for suffix in suffixes:
            if normalized.endswith(suffix):
                normalized = normalized[:-len(suffix)].strip()
                break
        
        return normalized.title()
    
    def _find_linked_entity(self, canonical_form: str, entity_type: str) -> Optional[str]:
        """æŸ¥æ‰¾é“¾æ¥å®ä½“ - è¿™é‡Œå¯ä»¥è¿æ¥åˆ°çŸ¥è¯†åº“"""
        # ç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥æŸ¥è¯¢çŸ¥è¯†åº“
        return f"KB:{entity_type}:{canonical_form.replace(' ', '_')}"

class RelationExtractor:
    """å…³ç³»æŠ½å–å™¨"""
    
    def __init__(self, model_name: str = "microsoft/DialoGPT-medium"):
        # ä½¿ç”¨é¢„è®­ç»ƒçš„å…³ç³»æŠ½å–æ¨¡å‹
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # å…³ç³»ç±»å‹å®šä¹‰
        self.relation_types = [
            'works_for', 'located_in', 'born_in', 'died_in',
            'founded_by', 'owned_by', 'part_of', 'member_of',
            'spouse', 'child_of', 'parent_of', 'sibling_of',
            'educated_at', 'nationality', 'occupation',
            'headquartered_in', 'subsidiary_of', 'competitor_of'
        ]
        
        # ç®€å•çš„æ¨¡å¼åŒ¹é…è§„åˆ™
        self.relation_patterns = {
            'works_for': [
                r'\b(?P<subject>\w+(?:\s+\w+)*)\s+works?\s+(?:for|at)\s+(?P<object>\w+(?:\s+\w+)*)',
                r'\b(?P<subject>\w+(?:\s+\w+)*)\s+(?:is|was)\s+employed\s+(?:by|at)\s+(?P<object>\w+(?:\s+\w+)*)'
            ],
            'located_in': [
                r'\b(?P<subject>\w+(?:\s+\w+)*)\s+(?:is|was)\s+located\s+in\s+(?P<object>\w+(?:\s+\w+)*)',
                r'\b(?P<subject>\w+(?:\s+\w+)*)\s+in\s+(?P<object>\w+(?:\s+\w+)*)'
            ],
            'founded_by': [
                r'\b(?P<object>\w+(?:\s+\w+)*)\s+founded\s+(?P<subject>\w+(?:\s+\w+)*)',
                r'\b(?P<subject>\w+(?:\s+\w+)*)\s+(?:was|is)\s+founded\s+by\s+(?P<object>\w+(?:\s+\w+)*)'
            ]
        }
    
    def extract_relations(self, text: str, entities: List[Entity]) -> List[Relation]:
        """æå–å®ä½“é—´å…³ç³»"""
        relations = []
        
        # åŸºäºæ¨¡å¼çš„å…³ç³»æŠ½å–
        pattern_relations = self._extract_by_patterns(text, entities)
        relations.extend(pattern_relations)
        
        # åŸºäºä¾å­˜å¥æ³•çš„å…³ç³»æŠ½å–
        dependency_relations = self._extract_by_dependency(text, entities)
        relations.extend(dependency_relations)
        
        # å»é‡å’Œç½®ä¿¡åº¦è®¡ç®—
        relations = self._deduplicate_relations(relations)
        
        return relations
    
    def _extract_by_patterns(self, text: str, entities: List[Entity]) -> List[Relation]:
        """åŸºäºæ¨¡å¼çš„å…³ç³»æŠ½å–"""
        import re
        relations = []
        
        for relation_type, patterns in self.relation_patterns.items():
            for pattern in patterns:
                matches = re.finditer(pattern, text, re.IGNORECASE)
                
                for match in matches:
                    subject_text = match.group('subject').strip()
                    object_text = match.group('object').strip()
                    
                    # æŸ¥æ‰¾å¯¹åº”çš„å®ä½“
                    subject_entity = self._find_matching_entity(subject_text, entities)
                    object_entity = self._find_matching_entity(object_text, entities)
                    
                    if subject_entity and object_entity:
                        relation = Relation(
                            subject=subject_entity,
                            predicate=relation_type,
                            object=object_entity,
                            confidence=0.8,  # æ¨¡å¼åŒ¹é…çš„åŸºç¡€ç½®ä¿¡åº¦
                            context=match.group(0)
                        )
                        relations.append(relation)
        
        return relations
    
    def _extract_by_dependency(self, text: str, entities: List[Entity]) -> List[Relation]:
        """åŸºäºä¾å­˜å¥æ³•çš„å…³ç³»æŠ½å–"""
        import spacy
        
        nlp = spacy.load("en_core_web_lg")
        doc = nlp(text)
        relations = []
        
        for sent in doc.sents:
            # æŸ¥æ‰¾å¥å­ä¸­çš„å®ä½“
            sent_entities = []
            for entity in entities:
                if entity.start >= sent.start_char and entity.end <= sent.end_char:
                    sent_entities.append(entity)
            
            # å¦‚æœå¥å­ä¸­æœ‰å¤šä¸ªå®ä½“ï¼Œå°è¯•æå–å…³ç³»
            if len(sent_entities) >= 2:
                for i, entity1 in enumerate(sent_entities):
                    for entity2 in sent_entities[i+1:]:
                        # åŸºäºä¾å­˜è·¯å¾„æå–å…³ç³»
                        relation_type = self._infer_relation_from_dependency(
                            sent, entity1, entity2
                        )
                        
                        if relation_type:
                            relation = Relation(
                                subject=entity1,
                                predicate=relation_type,
                                object=entity2,
                                confidence=0.6,  # ä¾å­˜åˆ†æçš„ç½®ä¿¡åº¦
                                context=sent.text
                            )
                            relations.append(relation)
        
        return relations
    
    def _find_matching_entity(self, text: str, entities: List[Entity]) -> Optional[Entity]:
        """æŸ¥æ‰¾åŒ¹é…çš„å®ä½“"""
        text_lower = text.lower()
        
        for entity in entities:
            if (entity.text.lower() == text_lower or 
                text_lower in entity.text.lower() or 
                entity.text.lower() in text_lower):
                return entity
        
        return None
    
    def _infer_relation_from_dependency(self, sent, entity1: Entity, entity2: Entity) -> Optional[str]:
        """ä»ä¾å­˜å…³ç³»æ¨æ–­å…³ç³»ç±»å‹"""
        # ç®€åŒ–çš„ä¾å­˜å…³ç³»æ¨æ–­
        # å®é™…å®ç°éœ€è¦æ›´å¤æ‚çš„ä¾å­˜è·¯å¾„åˆ†æ
        
        sent_text = sent.text.lower()
        
        if 'work' in sent_text and ('for' in sent_text or 'at' in sent_text):
            return 'works_for'
        elif 'located' in sent_text or 'in' in sent_text:
            return 'located_in'
        elif 'founded' in sent_text:
            return 'founded_by'
        elif 'married' in sent_text or 'spouse' in sent_text:
            return 'spouse'
        
        return None
    
    def _deduplicate_relations(self, relations: List[Relation]) -> List[Relation]:
        """å…³ç³»å»é‡"""
        unique_relations = {}
        
        for relation in relations:
            # åˆ›å»ºå…³ç³»çš„å”¯ä¸€é”®
            key = (
                relation.subject.canonical_form or relation.subject.text,
                relation.predicate,
                relation.object.canonical_form or relation.object.text
            )
            
            # ä¿ç•™ç½®ä¿¡åº¦æ›´é«˜çš„å…³ç³»
            if key not in unique_relations or relation.confidence > unique_relations[key].confidence:
                unique_relations[key] = relation
        
        return list(unique_relations.values())

class KnowledgeGraphBuilder:
    """çŸ¥è¯†å›¾è°±æ„å»ºå™¨"""
    
    def __init__(self, graph_db_uri: str = "bolt://localhost:7687"):
        from neo4j import GraphDatabase
        
        self.driver = GraphDatabase.driver(graph_db_uri, auth=("neo4j", "password"))
        self.entity_recognizer = EntityRecognizer()
        self.relation_extractor = RelationExtractor()
        
        # å›¾è°±ç»Ÿè®¡
        self.stats = {
            'entities': 0,
            'relations': 0,
            'entity_types': set(),
            'relation_types': set()
        }
    
    def process_document(self, text: str, document_id: str) -> Dict[str, int]:
        """å¤„ç†æ–‡æ¡£å¹¶æ„å»ºçŸ¥è¯†å›¾è°±"""
        
        # æå–å®ä½“å’Œå…³ç³»
        entities = self.entity_recognizer.extract_entities(text)
        relations = self.relation_extractor.extract_relations(text, entities)
        
        # å­˜å‚¨åˆ°å›¾æ•°æ®åº“
        with self.driver.session() as session:
            # åˆ›å»ºå®ä½“èŠ‚ç‚¹
            for entity in entities:
                self._create_entity_node(session, entity, document_id)
            
            # åˆ›å»ºå…³ç³»è¾¹
            for relation in relations:
                self._create_relation_edge(session, relation, document_id)
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self._update_stats(entities, relations)
        
        return {
            'entities_extracted': len(entities),
            'relations_extracted': len(relations)
        }
    
    def _create_entity_node(self, session, entity: Entity, document_id: str):
        """åˆ›å»ºå®ä½“èŠ‚ç‚¹"""
        query = """
        MERGE (e:Entity {canonical_form: $canonical_form})
        SET e.text = $text,
            e.label = $label,
            e.confidence = $confidence,
            e.linked_entity = $linked_entity,
            e.last_updated = datetime()
        WITH e
        MERGE (d:Document {id: $document_id})
        MERGE (e)-[:MENTIONED_IN]->(d)
        """
        
        session.run(query, 
            canonical_form=entity.canonical_form or entity.text,
            text=entity.text,
            label=entity.label,
            confidence=entity.confidence,
            linked_entity=entity.linked_entity,
            document_id=document_id
        )
    
    def _create_relation_edge(self, session, relation: Relation, document_id: str):
        """åˆ›å»ºå…³ç³»è¾¹"""
        query = f"""
        MATCH (s:Entity {{canonical_form: $subject}})
        MATCH (o:Entity {{canonical_form: $object}})
        MERGE (s)-[r:{relation.predicate.upper()}]->(o)
        SET r.confidence = $confidence,
            r.context = $context,
            r.document_id = $document_id,
            r.last_updated = datetime()
        """
        
        session.run(query,
            subject=relation.subject.canonical_form or relation.subject.text,
            object=relation.object.canonical_form or relation.object.text,
            confidence=relation.confidence,
            context=relation.context,
            document_id=document_id
        )
    
    def _update_stats(self, entities: List[Entity], relations: List[Relation]):
        """æ›´æ–°å›¾è°±ç»Ÿè®¡ä¿¡æ¯"""
        self.stats['entities'] += len(entities)
        self.stats['relations'] += len(relations)
        
        for entity in entities:
            self.stats['entity_types'].add(entity.label)
        
        for relation in relations:
            self.stats['relation_types'].add(relation.predicate)
    
    def query_graph(self, cypher_query: str) -> List[Dict]:
        """æ‰§è¡ŒCypheræŸ¥è¯¢"""
        with self.driver.session() as session:
            result = session.run(cypher_query)
            return [record.data() for record in result]
    
    def find_entity(self, entity_name: str) -> Optional[Dict]:
        """æŸ¥æ‰¾å®ä½“"""
        query = """
        MATCH (e:Entity)
        WHERE e.canonical_form CONTAINS $name OR e.text CONTAINS $name
        RETURN e
        LIMIT 1
        """
        
        results = self.query_graph(query)
        return results[0]['e'] if results else None
    
    def find_relations(self, entity1: str, entity2: str = None) -> List[Dict]:
        """æŸ¥æ‰¾å…³ç³»"""
        if entity2:
            query = """
            MATCH (e1:Entity)-[r]-(e2:Entity)
            WHERE (e1.canonical_form CONTAINS $entity1 OR e1.text CONTAINS $entity1)
              AND (e2.canonical_form CONTAINS $entity2 OR e2.text CONTAINS $entity2)
            RETURN e1, r, e2
            """
            return self.query_graph(query)
        else:
            query = """
            MATCH (e1:Entity)-[r]-(e2:Entity)
            WHERE e1.canonical_form CONTAINS $entity1 OR e1.text CONTAINS $entity1
            RETURN e1, r, e2
            """
            return self.query_graph(query)
    
    def get_graph_stats(self) -> Dict:
        """è·å–å›¾è°±ç»Ÿè®¡ä¿¡æ¯"""
        with self.driver.session() as session:
            # è·å–å®æ—¶ç»Ÿè®¡
            entity_count = session.run("MATCH (e:Entity) RETURN count(e) as count").single()["count"]
            
            relation_query = """
            MATCH ()-[r]->()
            RETURN type(r) as rel_type, count(r) as count
            """
            relation_stats = session.run(relation_query).data()
            
            return {
                'total_entities': entity_count,
                'total_relations': sum(r['count'] for r in relation_stats),
                'entity_types': list(self.stats['entity_types']),
                'relation_types': list(self.stats['relation_types']),
                'relation_distribution': relation_stats
            }
    
    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        self.driver.close()
```

### GraphRAGç³»ç»Ÿé›†æˆ
```python
from typing import List, Dict, Any, Optional
import numpy as np
from dataclasses import dataclass

@dataclass
class GraphContext:
    entities: List[Dict[str, Any]]
    relations: List[Dict[str, Any]]
    subgraph: Dict[str, Any]
    reasoning_path: List[str]

class GraphRAG:
    """å›¾è°±å¢å¼ºçš„æ£€ç´¢ç”Ÿæˆç³»ç»Ÿ"""
    
    def __init__(self, knowledge_graph: KnowledgeGraphBuilder, vector_store, llm_client):
        self.kg = knowledge_graph
        self.vector_store = vector_store
        self.llm = llm_client
        
        # å®ä½“åµŒå…¥ç¼“å­˜
        self.entity_embeddings = {}
        
    async def enhanced_retrieve(
        self, 
        query: str, 
        top_k: int = 10,
        include_reasoning: bool = True
    ) -> Dict[str, Any]:
        """å›¾è°±å¢å¼ºæ£€ç´¢"""
        
        # 1. ä¼ ç»Ÿå‘é‡æ£€ç´¢
        vector_results = await self._vector_retrieve(query, top_k)
        
        # 2. è¯†åˆ«æŸ¥è¯¢ä¸­çš„å®ä½“
        query_entities = self.kg.entity_recognizer.extract_entities(query)
        
        # 3. å›¾è°±æ£€ç´¢
        graph_context = await self._graph_retrieve(query_entities, query)
        
        # 4. èåˆæ£€ç´¢ç»“æœ
        enhanced_results = await self._fuse_results(
            vector_results, 
            graph_context, 
            query
        )
        
        # 5. å¤šè·³æ¨ç†(å¦‚æœéœ€è¦)
        if include_reasoning:
            reasoning_results = await self._multi_hop_reasoning(
                query_entities, 
                query, 
                max_hops=3
            )
            enhanced_results['reasoning'] = reasoning_results
        
        return enhanced_results
    
    async def _vector_retrieve(self, query: str, top_k: int) -> List[Dict]:
        """ä¼ ç»Ÿå‘é‡æ£€ç´¢"""
        # è°ƒç”¨ç°æœ‰çš„å‘é‡æ£€ç´¢ç³»ç»Ÿ
        results = await self.vector_store.similarity_search(query, k=top_k)
        return [{'content': r.page_content, 'metadata': r.metadata} for r in results]
    
    async def _graph_retrieve(self, entities: List[Entity], query: str) -> GraphContext:
        """å›¾è°±æ£€ç´¢"""
        graph_entities = []
        graph_relations = []
        
        # ä¸ºæ¯ä¸ªè¯†åˆ«çš„å®ä½“æŸ¥æ‰¾å›¾è°±ä¿¡æ¯
        for entity in entities:
            # æŸ¥æ‰¾å®ä½“è¯¦ç»†ä¿¡æ¯
            entity_info = self.kg.find_entity(entity.canonical_form or entity.text)
            if entity_info:
                graph_entities.append(entity_info)
                
                # æŸ¥æ‰¾ç›¸å…³å…³ç³»
                relations = self.kg.find_relations(entity.canonical_form or entity.text)
                graph_relations.extend(relations)
        
        # æ„å»ºæŸ¥è¯¢ç›¸å…³çš„å­å›¾
        subgraph = self._build_subgraph(graph_entities, graph_relations)
        
        return GraphContext(
            entities=graph_entities,
            relations=graph_relations,
            subgraph=subgraph,
            reasoning_path=[]
        )
    
    def _build_subgraph(self, entities: List[Dict], relations: List[Dict]) -> Dict[str, Any]:
        """æ„å»ºå­å›¾"""
        nodes = {}
        edges = []
        
        # æ·»åŠ å®ä½“èŠ‚ç‚¹
        for entity in entities:
            node_id = entity.get('canonical_form', entity.get('text'))
            nodes[node_id] = {
                'id': node_id,
                'label': entity.get('label'),
                'type': 'entity',
                'properties': entity
            }
        
        # æ·»åŠ å…³ç³»è¾¹
        for rel in relations:
            if 'r' in rel and 'e1' in rel and 'e2' in rel:
                relation_info = rel['r']
                source = rel['e1'].get('canonical_form', rel['e1'].get('text'))
                target = rel['e2'].get('canonical_form', rel['e2'].get('text'))
                
                edges.append({
                    'source': source,
                    'target': target,
                    'relation': relation_info.get('type'),
                    'properties': relation_info
                })
        
        return {'nodes': nodes, 'edges': edges}
    
    async def _fuse_results(
        self, 
        vector_results: List[Dict], 
        graph_context: GraphContext, 
        query: str
    ) -> Dict[str, Any]:
        """èåˆå‘é‡æ£€ç´¢å’Œå›¾è°±æ£€ç´¢ç»“æœ"""
        
        # åŸºäºå›¾è°±ä¸Šä¸‹æ–‡é‡æ–°æ’åºå‘é‡ç»“æœ
        enhanced_vector_results = []
        
        for result in vector_results:
            # è®¡ç®—æ–‡æ¡£ä¸å›¾è°±ä¸Šä¸‹æ–‡çš„ç›¸å…³æ€§
            graph_relevance = self._calculate_graph_relevance(
                result, 
                graph_context
            )
            
            result['graph_relevance'] = graph_relevance
            result['enhanced_score'] = result.get('score', 0.5) * 0.7 + graph_relevance * 0.3
            enhanced_vector_results.append(result)
        
        # é‡æ–°æ’åº
        enhanced_vector_results.sort(key=lambda x: x['enhanced_score'], reverse=True)
        
        # æ·»åŠ å›¾è°±ç‰¹æœ‰çš„ä¸Šä¸‹æ–‡
        graph_facts = self._extract_graph_facts(graph_context)
        
        return {
            'documents': enhanced_vector_results,
            'graph_context': graph_context,
            'graph_facts': graph_facts,
            'entities': graph_context.entities,
            'relations': graph_context.relations[:10]  # é™åˆ¶å…³ç³»æ•°é‡
        }
    
    def _calculate_graph_relevance(
        self, 
        document: Dict, 
        graph_context: GraphContext
    ) -> float:
        """è®¡ç®—æ–‡æ¡£ä¸å›¾è°±ä¸Šä¸‹æ–‡çš„ç›¸å…³æ€§"""
        content = document.get('content', '')
        relevance_score = 0.0
        
        # å®ä½“åŒ¹é…åº¦
        entity_matches = 0
        for entity in graph_context.entities:
            entity_text = entity.get('canonical_form', entity.get('text', ''))
            if entity_text.lower() in content.lower():
                entity_matches += 1
        
        entity_relevance = entity_matches / max(1, len(graph_context.entities))
        
        # å…³ç³»ç›¸å…³åº¦
        relation_matches = 0
        for rel in graph_context.relations[:5]:  # æ£€æŸ¥å‰5ä¸ªå…³ç³»
            if 'r' in rel:
                relation_type = rel['r'].get('type', '').replace('_', ' ')
                if relation_type.lower() in content.lower():
                    relation_matches += 1
        
        relation_relevance = relation_matches / max(1, min(5, len(graph_context.relations)))
        
        # ç»„åˆç›¸å…³æ€§åˆ†æ•°
        relevance_score = entity_relevance * 0.6 + relation_relevance * 0.4
        
        return min(1.0, relevance_score)
    
    def _extract_graph_facts(self, graph_context: GraphContext) -> List[str]:
        """ä»å›¾è°±ä¸Šä¸‹æ–‡æå–äº‹å®"""
        facts = []
        
        for rel in graph_context.relations[:10]:
            if 'r' in rel and 'e1' in rel and 'e2' in rel:
                subject = rel['e1'].get('canonical_form', rel['e1'].get('text'))
                predicate = rel['r'].get('type', '').replace('_', ' ')
                obj = rel['e2'].get('canonical_form', rel['e2'].get('text'))
                
                fact = f"{subject} {predicate} {obj}"
                facts.append(fact)
        
        return facts
    
    async def _multi_hop_reasoning(
        self, 
        entities: List[Entity], 
        query: str, 
        max_hops: int = 3
    ) -> Dict[str, Any]:
        """å¤šè·³æ¨ç†"""
        reasoning_paths = []
        
        for entity in entities:
            entity_name = entity.canonical_form or entity.text
            paths = self._find_reasoning_paths(entity_name, query, max_hops)
            reasoning_paths.extend(paths)
        
        # è¯„ä¼°æ¨ç†è·¯å¾„çš„ç›¸å…³æ€§
        scored_paths = []
        for path in reasoning_paths:
            score = await self._score_reasoning_path(path, query)
            scored_paths.append({
                'path': path,
                'score': score,
                'explanation': self._explain_reasoning_path(path)
            })
        
        # æ’åºå¹¶è¿”å›æœ€ä½³æ¨ç†è·¯å¾„
        scored_paths.sort(key=lambda x: x['score'], reverse=True)
        
        return {
            'reasoning_paths': scored_paths[:5],  # è¿”å›å‰5ä¸ªæœ€ä½³è·¯å¾„
            'total_paths_found': len(reasoning_paths)
        }
    
    def _find_reasoning_paths(
        self, 
        start_entity: str, 
        query: str, 
        max_hops: int
    ) -> List[List[str]]:
        """æŸ¥æ‰¾æ¨ç†è·¯å¾„"""
        
        # ä½¿ç”¨BFSæŸ¥æ‰¾è·¯å¾„
        from collections import deque
        
        queue = deque([(start_entity, [start_entity])])
        paths = []
        visited = set()
        
        while queue and len(paths) < 100:  # é™åˆ¶è·¯å¾„æ•°é‡
            current_entity, path = queue.popleft()
            
            if len(path) > max_hops:
                continue
            
            if current_entity in visited:
                continue
                
            visited.add(current_entity)
            
            # è·å–ç›¸å…³å…³ç³»
            relations = self.kg.find_relations(current_entity)
            
            for rel in relations:
                if 'e2' in rel:
                    next_entity = rel['e2'].get('canonical_form', rel['e2'].get('text'))
                    if next_entity not in path:  # é¿å…å¾ªç¯
                        new_path = path + [rel['r'].get('type', ''), next_entity]
                        paths.append(new_path)
                        
                        if len(new_path) < max_hops * 2:  # pathåŒ…å«å®ä½“å’Œå…³ç³»
                            queue.append((next_entity, new_path))
        
        return paths
    
    async def _score_reasoning_path(self, path: List[str], query: str) -> float:
        """è¯„ä¼°æ¨ç†è·¯å¾„çš„ç›¸å…³æ€§"""
        # ç®€åŒ–è¯„åˆ†ï¼šåŸºäºè·¯å¾„ä¸­å…³é”®è¯ä¸æŸ¥è¯¢çš„åŒ¹é…åº¦
        query_words = set(query.lower().split())
        path_text = ' '.join(path).lower()
        path_words = set(path_text.split())
        
        # è®¡ç®—äº¤é›†
        common_words = query_words.intersection(path_words)
        relevance_score = len(common_words) / max(1, len(query_words))
        
        # è·¯å¾„é•¿åº¦æƒ©ç½š
        length_penalty = 1.0 / (1 + len(path) / 10)
        
        return relevance_score * length_penalty
    
    def _explain_reasoning_path(self, path: List[str]) -> str:
        """è§£é‡Šæ¨ç†è·¯å¾„"""
        if len(path) < 3:
            return "è·¯å¾„å¤ªçŸ­ï¼Œæ— æ³•è§£é‡Š"
        
        explanation_parts = []
        
        for i in range(0, len(path) - 1, 2):
            if i + 2 < len(path):
                entity1 = path[i]
                relation = path[i + 1].replace('_', ' ')
                entity2 = path[i + 2]
                
                explanation_parts.append(f"{entity1} {relation} {entity2}")
        
        return " â†’ ".join(explanation_parts)
    
    async def generate_response(
        self, 
        query: str, 
        enhanced_results: Dict[str, Any]
    ) -> str:
        """ç”Ÿæˆå›¾è°±å¢å¼ºçš„å›ç­”"""
        
        # æ„å»ºå¢å¼ºçš„ä¸Šä¸‹æ–‡
        context_parts = []
        
        # æ·»åŠ æ–‡æ¡£ä¸Šä¸‹æ–‡
        for doc in enhanced_results['documents'][:5]:
            context_parts.append(f"æ–‡æ¡£: {doc['content']}")
        
        # æ·»åŠ å›¾è°±äº‹å®
        if enhanced_results.get('graph_facts'):
            context_parts.append("ç›¸å…³äº‹å®:")
            for fact in enhanced_results['graph_facts'][:5]:
                context_parts.append(f"- {fact}")
        
        # æ·»åŠ æ¨ç†è·¯å¾„
        if enhanced_results.get('reasoning', {}).get('reasoning_paths'):
            context_parts.append("æ¨ç†è·¯å¾„:")
            for path_info in enhanced_results['reasoning']['reasoning_paths'][:2]:
                context_parts.append(f"- {path_info['explanation']}")
        
        context = "\n".join(context_parts)
        
        # æ„å»ºæç¤ºè¯
        prompt = f"""
        åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚è¯·ç»¼åˆè€ƒè™‘æ–‡æ¡£ä¿¡æ¯ã€çŸ¥è¯†å›¾è°±äº‹å®å’Œæ¨ç†è·¯å¾„ã€‚

        ä¸Šä¸‹æ–‡:
        {context}

        é—®é¢˜: {query}

        è¯·æä¾›å‡†ç¡®ã€å®Œæ•´çš„å›ç­”ï¼Œå¹¶åœ¨å¿…è¦æ—¶å¼•ç”¨å…·ä½“çš„äº‹å®å’Œæ¨ç†è¿‡ç¨‹ã€‚
        """
        
        # è°ƒç”¨LLMç”Ÿæˆå›ç­”
        response = await self.llm.generate_response(prompt)
        
        return response
```

## ğŸš¦ é£é™©è¯„ä¼°ä¸ç¼“è§£

### é«˜é£é™©é¡¹
1. **å®ä½“è¯†åˆ«å’Œå…³ç³»æŠ½å–å‡†ç¡®ç‡**
   - ç¼“è§£: ä½¿ç”¨å¤šä¸ªæ¨¡å‹ç»„åˆï¼Œäººå·¥æ ‡æ³¨éªŒè¯é›†
   - éªŒè¯: åœ¨æ ‡å‡†æ•°æ®é›†ä¸Šè¾¾åˆ°ç›®æ ‡å‡†ç¡®ç‡

2. **çŸ¥è¯†å›¾è°±è§„æ¨¡å’Œæ€§èƒ½**
   - ç¼“è§£: å›¾æ•°æ®åº“ä¼˜åŒ–ï¼Œåˆ†å±‚å­˜å‚¨ï¼ŒæŸ¥è¯¢ç¼“å­˜
   - éªŒè¯: ç™¾ä¸‡çº§å®ä½“çš„æŸ¥è¯¢æ€§èƒ½æµ‹è¯•

3. **GraphRAGç³»ç»Ÿå¤æ‚æ€§**
   - ç¼“è§£: é€æ­¥é›†æˆï¼Œå……åˆ†æµ‹è¯•ï¼Œé™çº§æ–¹æ¡ˆ
   - éªŒè¯: A/Bæµ‹è¯•éªŒè¯æ•ˆæœæå‡

### ä¸­é£é™©é¡¹
1. **å¤šè¯­è¨€æ”¯æŒå¤æ‚åº¦**
   - ç¼“è§£: å…ˆæ”¯æŒä¸­è‹±æ–‡ï¼Œé€æ­¥æ‰©å±•
   - éªŒè¯: å„è¯­è¨€çš„æŠ½å–è´¨é‡æµ‹è¯•

2. **å›¾è°±è´¨é‡ç»´æŠ¤**
   - ç¼“è§£: è‡ªåŠ¨è´¨é‡æ£€æµ‹ï¼Œäººå·¥å®¡æ ¸æœºåˆ¶
   - éªŒè¯: å›¾è°±ä¸€è‡´æ€§å’Œå®Œæ•´æ€§è¯„ä¼°

## ğŸ“… å®æ–½è·¯çº¿å›¾

### Phase 1: åŸºç¡€æŠ½å–èƒ½åŠ› (Week 1-4)
- å®ä½“è¯†åˆ«ä¸å…³ç³»æŠ½å–å¼•æ“
- åŠ¨æ€çŸ¥è¯†å›¾è°±å­˜å‚¨ç³»ç»Ÿ
- åŸºç¡€å›¾è°±æ„å»ºæµç¨‹

### Phase 2: æ¨ç†å’ŒæŸ¥è¯¢ (Week 5-8)
- å›¾è°±æ¨ç†å¼•æ“
- çŸ¥è¯†ç®¡ç†å’ŒAPIæ¥å£
- æŸ¥è¯¢ä¼˜åŒ–å’Œæ€§èƒ½è°ƒä¼˜

### Phase 3: GraphRAGé›†æˆ (Week 9-10)
- GraphRAGç³»ç»Ÿé›†æˆ
- å¤šè·³æ¨ç†ç®—æ³•
- æ•ˆæœè¯„ä¼°å’Œä¼˜åŒ–

### Phase 4: å¯è§†åŒ–å’Œéƒ¨ç½² (Week 11-12)
- çŸ¥è¯†å›¾è°±å¯è§†åŒ–ç•Œé¢
- ç³»ç»Ÿé›†æˆæµ‹è¯•
- ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

---

**æ–‡æ¡£çŠ¶æ€**: âœ… å®Œæˆ  
**ä¸‹ä¸€æ­¥**: å¼€å§‹Story 8.1çš„å®ä½“è¯†åˆ«ä¸å…³ç³»æŠ½å–å¼•æ“å®æ–½  
**ä¾èµ–Epic**: å¯ä¸ç°æœ‰RAGç³»ç»Ÿå¹¶è¡Œå¼€å‘ï¼Œæœ€åé˜¶æ®µé›†æˆ