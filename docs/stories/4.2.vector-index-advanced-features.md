# Story 4.2: 向量索引高级功能实现

## Status
Draft

## Story
**As a** AI系统学习者,
**I want** 在pgvector 0.8基础上实现多样化的向量索引高级功能和搜索策略,
**so that** 我可以学习和掌握各种向量检索技术，包括语义搜索、图像搜索、混合搜索、时序向量等多种应用场景

## Acceptance Criteria
1. 实现多种向量索引类型和搜索策略（HNSW、IVF、LSH等）
2. 实现语义相似度搜索和关键词混合搜索
3. 支持图像向量搜索和多模态向量融合
4. 实现时序向量索引和轨迹相似度搜索
5. 支持向量聚类和异常检测功能
6. 实现向量可视化和降维展示（t-SNE、UMAP）
7. 支持自定义距离度量（余弦、欧氏、曼哈顿等）
8. 实现向量数据的导入导出和迁移工具

## Tasks / Subtasks
- [ ] 多种索引类型实现 (AC: 1)
  - [ ] 实现HNSW索引的完整功能
  - [ ] 实现IVF（倒排文件）索引
  - [ ] 实现LSH（局部敏感哈希）索引
  - [ ] 实现索引类型的动态切换
- [ ] 语义和混合搜索 (AC: 2)
  - [ ] 实现纯语义向量搜索
  - [ ] 实现BM25关键词搜索集成
  - [ ] 实现混合搜索评分融合
  - [ ] 实现查询扩展和同义词处理
- [ ] 多模态向量搜索 (AC: 3)
  - [ ] 实现图像向量编码和搜索
  - [ ] 实现文本-图像跨模态搜索
  - [ ] 实现音频向量搜索支持
  - [ ] 实现多模态向量融合策略
- [ ] 时序向量功能 (AC: 4)
  - [ ] 实现时间序列向量索引
  - [ ] 实现轨迹相似度计算
  - [ ] 实现时序模式匹配
  - [ ] 实现向量变化趋势分析
- [ ] 向量聚类和异常检测 (AC: 5)
  - [ ] 实现K-means向量聚类
  - [ ] 实现DBSCAN密度聚类
  - [ ] 实现异常向量检测算法
  - [ ] 实现聚类结果可视化
- [ ] 向量可视化工具 (AC: 6)
  - [ ] 实现t-SNE降维可视化
  - [ ] 实现UMAP降维算法
  - [ ] 实现PCA主成分分析
  - [ ] 实现交互式向量探索界面
- [ ] 自定义距离度量 (AC: 7)
  - [ ] 实现余弦相似度计算
  - [ ] 实现欧氏距离计算
  - [ ] 实现曼哈顿距离计算
  - [ ] 实现自定义距离函数接口
- [ ] 数据导入导出工具 (AC: 8)
  - [ ] 实现CSV/JSON向量导入
  - [ ] 实现向量数据批量导出
  - [ ] 实现跨数据库迁移工具
  - [ ] 实现向量数据备份恢复

## Dev Notes

### Previous Story Insights
基于Story 4.1已完成的pgvector 0.8升级和量化压缩基础：
- pgvector已升级到0.8版本，支持高级索引特性
- 已实现INT8/INT4量化，减少内存使用
- 已建立基础的性能监控框架
- 已实现向量缓存和基础检索

现在需要在此基础上实现多样化的向量功能，专注于学习不同的向量技术而非性能优化。

### Tech Stack Context
[Source: docs/architecture/tech-stack.md#vector-features]
- **Database**: PostgreSQL 15+ with pgvector 0.8
- **Vector Libraries**: Faiss, Annoy, scikit-learn
- **Visualization**: matplotlib, plotly, t-SNE, UMAP
- **ML Libraries**: transformers (CLIP), sentence-transformers
- **Image Processing**: PIL, OpenCV
- **Language**: Python 3.11+

### Project Structure Context
[Source: docs/architecture/unified-project-structure.md#vector-features]
向量高级功能相关文件位置：
- **多索引管理**: `apps/api/src/ai/rag/multi_index.py` (新增)
- **混合搜索**: `apps/api/src/ai/rag/hybrid_search_advanced.py` (新增)
- **多模态搜索**: `apps/api/src/ai/rag/multimodal_search.py` (新增)
- **时序向量**: `apps/api/src/ai/rag/temporal_vectors.py` (新增)
- **向量聚类**: `apps/api/src/ai/rag/vector_clustering.py` (新增)
- **可视化工具**: `apps/api/src/ai/rag/vector_visualization.py` (新增)
- **距离度量**: `apps/api/src/ai/rag/distance_metrics.py` (新增)
- **数据工具**: `apps/api/src/ai/rag/data_tools.py` (新增)
- **相关测试**: `apps/api/tests/ai/rag/advanced_features/` (新增目录)

### Integration with Existing Code
基于Story 4.1已实现的组件，需要扩展：
- **扩展**: `apps/api/src/ai/rag/pgvector_optimizer.py` - 添加多索引支持
- **扩展**: `apps/api/src/ai/rag/hybrid_retrieval.py` - 增强混合搜索
- **修改**: `apps/api/src/services/rag_service.py` - 集成新功能
- **更新**: `apps/api/src/api/v1/rag.py` - 添加新的API端点

### Advanced Vector Features Implementation

```python
import numpy as np
from typing import Dict, Any, List, Optional, Tuple, Union
from enum import Enum
from dataclasses import dataclass
import hashlib
from sklearn.cluster import KMeans, DBSCAN
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap
import faiss
from annoy import AnnoyIndex
from sentence_transformers import SentenceTransformer
import torch
from PIL import Image
import io
import base64

# ============= 多种索引类型实现 =============

class MultiIndexManager:
    """多种索引类型管理器"""
    
    def __init__(self, dimension: int = 384):
        self.dimension = dimension
        self.indexes = {}
        self.index_metadata = {}
        
    def create_hnsw_index(self, vectors: np.ndarray, M: int = 16) -> str:
        """创建HNSW索引"""
        index_id = self._generate_index_id("hnsw")
        index = faiss.IndexHNSWFlat(self.dimension, M)
        index.add(vectors)
        
        self.indexes[index_id] = index
        self.index_metadata[index_id] = {
            "type": "hnsw",
            "dimension": self.dimension,
            "M": M,
            "num_vectors": len(vectors)
        }
        return index_id
    
    def create_ivf_index(self, vectors: np.ndarray, nlist: int = 100) -> str:
        """创建IVF索引"""
        index_id = self._generate_index_id("ivf")
        quantizer = faiss.IndexFlatL2(self.dimension)
        index = faiss.IndexIVFFlat(quantizer, self.dimension, nlist)
        index.train(vectors)
        index.add(vectors)
        
        self.indexes[index_id] = index
        self.index_metadata[index_id] = {
            "type": "ivf",
            "dimension": self.dimension,
            "nlist": nlist,
            "num_vectors": len(vectors)
        }
        return index_id
    
    def create_lsh_index(self, vectors: np.ndarray, n_bits: int = 8) -> str:
        """创建LSH索引"""
        index_id = self._generate_index_id("lsh")
        index = faiss.IndexLSH(self.dimension, n_bits)
        index.add(vectors)
        
        self.indexes[index_id] = index
        self.index_metadata[index_id] = {
            "type": "lsh",
            "dimension": self.dimension,
            "n_bits": n_bits,
            "num_vectors": len(vectors)
        }
        return index_id
    
    def create_annoy_index(self, vectors: np.ndarray, n_trees: int = 10) -> str:
        """创建Annoy索引"""
        index_id = self._generate_index_id("annoy")
        index = AnnoyIndex(self.dimension, 'angular')
        
        for i, vector in enumerate(vectors):
            index.add_item(i, vector)
        
        index.build(n_trees)
        
        self.indexes[index_id] = index
        self.index_metadata[index_id] = {
            "type": "annoy",
            "dimension": self.dimension,
            "n_trees": n_trees,
            "num_vectors": len(vectors)
        }
        return index_id
    
    def search(self, index_id: str, query_vector: np.ndarray, k: int = 10) -> Tuple[np.ndarray, np.ndarray]:
        """在指定索引中搜索"""
        if index_id not in self.indexes:
            raise ValueError(f"Index {index_id} not found")
        
        index = self.indexes[index_id]
        index_type = self.index_metadata[index_id]["type"]
        
        if index_type == "annoy":
            indices, distances = index.get_nns_by_vector(query_vector, k, include_distances=True)
            return np.array(indices), np.array(distances)
        else:
            distances, indices = index.search(query_vector.reshape(1, -1), k)
            return indices[0], distances[0]
    
    def _generate_index_id(self, index_type: str) -> str:
        """生成索引ID"""
        import time
        return f"{index_type}_{int(time.time() * 1000)}"

# ============= 语义和混合搜索 =============

class HybridSearchEngine:
    """混合搜索引擎"""
    
    def __init__(self):
        self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.bm25_scores = {}
        
    async def semantic_search(
        self, 
        query: str, 
        documents: List[str], 
        top_k: int = 10
    ) -> List[Dict[str, Any]]:
        """纯语义搜索"""
        # 编码查询和文档
        query_embedding = self.semantic_model.encode(query)
        doc_embeddings = self.semantic_model.encode(documents)
        
        # 计算余弦相似度
        similarities = np.dot(doc_embeddings, query_embedding) / (
            np.linalg.norm(doc_embeddings, axis=1) * np.linalg.norm(query_embedding)
        )
        
        # 获取top-k结果
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            results.append({
                "document": documents[idx],
                "score": float(similarities[idx]),
                "type": "semantic"
            })
        
        return results
    
    async def bm25_search(
        self, 
        query: str, 
        documents: List[str], 
        top_k: int = 10
    ) -> List[Dict[str, Any]]:
        """BM25关键词搜索"""
        from rank_bm25 import BM25Okapi
        
        # 分词
        tokenized_docs = [doc.lower().split() for doc in documents]
        tokenized_query = query.lower().split()
        
        # 创建BM25模型
        bm25 = BM25Okapi(tokenized_docs)
        scores = bm25.get_scores(tokenized_query)
        
        # 获取top-k结果
        top_indices = np.argsort(scores)[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            results.append({
                "document": documents[idx],
                "score": float(scores[idx]),
                "type": "keyword"
            })
        
        return results
    
    async def hybrid_search(
        self, 
        query: str, 
        documents: List[str], 
        top_k: int = 10,
        semantic_weight: float = 0.6
    ) -> List[Dict[str, Any]]:
        """混合搜索（语义+关键词）"""
        # 执行两种搜索
        semantic_results = await self.semantic_search(query, documents, top_k * 2)
        keyword_results = await self.bm25_search(query, documents, top_k * 2)
        
        # 融合评分
        doc_scores = {}
        
        for result in semantic_results:
            doc = result["document"]
            doc_scores[doc] = doc_scores.get(doc, 0) + result["score"] * semantic_weight
        
        for result in keyword_results:
            doc = result["document"]
            normalized_score = result["score"] / (1 + result["score"])  # 归一化BM25分数
            doc_scores[doc] = doc_scores.get(doc, 0) + normalized_score * (1 - semantic_weight)
        
        # 排序并返回top-k
        sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]
        
        results = []
        for doc, score in sorted_docs:
            results.append({
                "document": doc,
                "score": score,
                "type": "hybrid"
            })
        
        return results

# ============= 多模态向量搜索 =============

class MultiModalSearch:
    """多模态搜索引擎"""
    
    def __init__(self):
        # 使用CLIP模型进行图文编码
        from transformers import CLIPProcessor, CLIPModel
        self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        
    async def encode_image(self, image_path: str) -> np.ndarray:
        """编码图像为向量"""
        image = Image.open(image_path)
        inputs = self.clip_processor(images=image, return_tensors="pt")
        
        with torch.no_grad():
            image_features = self.clip_model.get_image_features(**inputs)
        
        return image_features.numpy().flatten()
    
    async def encode_text(self, text: str) -> np.ndarray:
        """编码文本为向量"""
        inputs = self.clip_processor(text=text, return_tensors="pt", padding=True)
        
        with torch.no_grad():
            text_features = self.clip_model.get_text_features(**inputs)
        
        return text_features.numpy().flatten()
    
    async def image_to_image_search(
        self, 
        query_image_path: str, 
        image_database: List[str], 
        top_k: int = 10
    ) -> List[Dict[str, Any]]:
        """图像搜索图像"""
        query_vector = await self.encode_image(query_image_path)
        
        results = []
        for img_path in image_database:
            img_vector = await self.encode_image(img_path)
            similarity = np.dot(query_vector, img_vector) / (
                np.linalg.norm(query_vector) * np.linalg.norm(img_vector)
            )
            results.append({
                "image_path": img_path,
                "similarity": float(similarity)
            })
        
        results.sort(key=lambda x: x["similarity"], reverse=True)
        return results[:top_k]
    
    async def text_to_image_search(
        self, 
        query_text: str, 
        image_database: List[str], 
        top_k: int = 10
    ) -> List[Dict[str, Any]]:
        """文本搜索图像（跨模态）"""
        query_vector = await self.encode_text(query_text)
        
        results = []
        for img_path in image_database:
            img_vector = await self.encode_image(img_path)
            similarity = np.dot(query_vector, img_vector) / (
                np.linalg.norm(query_vector) * np.linalg.norm(img_vector)
            )
            results.append({
                "image_path": img_path,
                "similarity": float(similarity),
                "query_text": query_text
            })
        
        results.sort(key=lambda x: x["similarity"], reverse=True)
        return results[:top_k]

# ============= 时序向量功能 =============

class TemporalVectorManager:
    """时序向量管理器"""
    
    def __init__(self):
        self.temporal_index = {}
        self.trajectories = {}
        
    async def index_temporal_vector(
        self, 
        vector_id: str, 
        vector: np.ndarray, 
        timestamp: datetime
    ):
        """索引时序向量"""
        if vector_id not in self.temporal_index:
            self.temporal_index[vector_id] = []
        
        self.temporal_index[vector_id].append({
            "vector": vector,
            "timestamp": timestamp
        })
        
        # 按时间排序
        self.temporal_index[vector_id].sort(key=lambda x: x["timestamp"])
    
    async def compute_trajectory_similarity(
        self, 
        trajectory1: List[np.ndarray], 
        trajectory2: List[np.ndarray]
    ) -> float:
        """计算轨迹相似度（DTW动态时间规整）"""
        from fastdtw import fastdtw
        from scipy.spatial.distance import euclidean
        
        distance, _ = fastdtw(trajectory1, trajectory2, dist=euclidean)
        
        # 归一化距离到相似度
        similarity = 1.0 / (1.0 + distance)
        return similarity
    
    async def detect_temporal_pattern(
        self, 
        vector_id: str, 
        pattern_window: int = 10
    ) -> Dict[str, Any]:
        """检测时序模式"""
        if vector_id not in self.temporal_index:
            return {"pattern": "no_data"}
        
        temporal_data = self.temporal_index[vector_id]
        if len(temporal_data) < pattern_window:
            return {"pattern": "insufficient_data"}
        
        # 提取最近的向量序列
        recent_vectors = [item["vector"] for item in temporal_data[-pattern_window:]]
        
        # 计算向量变化
        changes = []
        for i in range(1, len(recent_vectors)):
            change = np.linalg.norm(recent_vectors[i] - recent_vectors[i-1])
            changes.append(change)
        
        # 分析模式
        avg_change = np.mean(changes)
        std_change = np.std(changes)
        
        if std_change < 0.1:
            pattern = "stable"
        elif avg_change > 0.5:
            pattern = "rapidly_changing"
        else:
            pattern = "gradual_drift"
        
        return {
            "pattern": pattern,
            "avg_change": float(avg_change),
            "std_change": float(std_change)
        }
    
    async def predict_next_vector(
        self, 
        vector_id: str, 
        method: str = "linear"
    ) -> Optional[np.ndarray]:
        """预测下一个向量"""
        if vector_id not in self.temporal_index or len(self.temporal_index[vector_id]) < 2:
            return None
        
        recent = self.temporal_index[vector_id][-2:]
        
        if method == "linear":
            # 简单线性外推
            v1, v2 = recent[0]["vector"], recent[1]["vector"]
            predicted = v2 + (v2 - v1)
            return predicted
        
        return None

# ============= 向量聚类和异常检测 =============

class VectorClusteringAnalyzer:
    """向量聚类分析器"""
    
    def __init__(self):
        self.clusterers = {}
        self.cluster_results = {}
        
    async def kmeans_clustering(
        self, 
        vectors: np.ndarray, 
        n_clusters: int = 5
    ) -> Dict[str, Any]:
        """K-means聚类"""
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        labels = kmeans.fit_predict(vectors)
        
        # 计算聚类质量指标
        from sklearn.metrics import silhouette_score, davies_bouldin_score
        
        silhouette = silhouette_score(vectors, labels)
        davies_bouldin = davies_bouldin_score(vectors, labels)
        
        # 计算每个聚类的统计信息
        cluster_stats = []
        for i in range(n_clusters):
            cluster_vectors = vectors[labels == i]
            cluster_stats.append({
                "cluster_id": i,
                "size": len(cluster_vectors),
                "center": kmeans.cluster_centers_[i].tolist(),
                "radius": float(np.max(np.linalg.norm(
                    cluster_vectors - kmeans.cluster_centers_[i], axis=1
                ))) if len(cluster_vectors) > 0 else 0
            })
        
        return {
            "labels": labels.tolist(),
            "n_clusters": n_clusters,
            "silhouette_score": float(silhouette),
            "davies_bouldin_score": float(davies_bouldin),
            "cluster_stats": cluster_stats
        }
    
    async def dbscan_clustering(
        self, 
        vectors: np.ndarray, 
        eps: float = 0.5, 
        min_samples: int = 5
    ) -> Dict[str, Any]:
        """DBSCAN密度聚类"""
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        labels = dbscan.fit_predict(vectors)
        
        # 统计聚类结果
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        n_noise = list(labels).count(-1)
        
        # 计算每个聚类的密度
        cluster_densities = []
        for cluster_id in set(labels):
            if cluster_id != -1:
                cluster_vectors = vectors[labels == cluster_id]
                # 计算聚类内平均距离
                if len(cluster_vectors) > 1:
                    distances = []
                    for i in range(len(cluster_vectors)):
                        for j in range(i+1, len(cluster_vectors)):
                            distances.append(np.linalg.norm(
                                cluster_vectors[i] - cluster_vectors[j]
                            ))
                    avg_distance = np.mean(distances) if distances else 0
                else:
                    avg_distance = 0
                
                cluster_densities.append({
                    "cluster_id": int(cluster_id),
                    "size": len(cluster_vectors),
                    "avg_internal_distance": float(avg_distance)
                })
        
        return {
            "labels": labels.tolist(),
            "n_clusters": n_clusters,
            "n_noise_points": n_noise,
            "cluster_densities": cluster_densities
        }
    
    async def detect_anomalies(
        self, 
        vectors: np.ndarray, 
        contamination: float = 0.1
    ) -> Dict[str, Any]:
        """异常向量检测（使用Isolation Forest）"""
        from sklearn.ensemble import IsolationForest
        
        iso_forest = IsolationForest(contamination=contamination, random_state=42)
        predictions = iso_forest.fit_predict(vectors)
        
        # -1表示异常，1表示正常
        anomaly_indices = np.where(predictions == -1)[0]
        normal_indices = np.where(predictions == 1)[0]
        
        # 计算异常分数
        anomaly_scores = iso_forest.score_samples(vectors)
        
        return {
            "anomaly_indices": anomaly_indices.tolist(),
            "normal_indices": normal_indices.tolist(),
            "anomaly_scores": anomaly_scores.tolist(),
            "n_anomalies": len(anomaly_indices),
            "contamination_rate": len(anomaly_indices) / len(vectors)
        }

# ============= 向量可视化工具 =============

class VectorVisualizer:
    """向量可视化器"""
    
    def __init__(self):
        self.reduction_methods = {}
        
    async def tsne_visualization(
        self, 
        vectors: np.ndarray, 
        n_components: int = 2,
        perplexity: int = 30
    ) -> np.ndarray:
        """t-SNE降维可视化"""
        tsne = TSNE(n_components=n_components, perplexity=perplexity, random_state=42)
        reduced_vectors = tsne.fit_transform(vectors)
        return reduced_vectors
    
    async def umap_visualization(
        self, 
        vectors: np.ndarray, 
        n_components: int = 2,
        n_neighbors: int = 15
    ) -> np.ndarray:
        """UMAP降维可视化"""
        reducer = umap.UMAP(n_components=n_components, n_neighbors=n_neighbors, random_state=42)
        reduced_vectors = reducer.fit_transform(vectors)
        return reduced_vectors
    
    async def pca_visualization(
        self, 
        vectors: np.ndarray, 
        n_components: int = 2
    ) -> Tuple[np.ndarray, Dict[str, Any]]:
        """PCA主成分分析"""
        pca = PCA(n_components=n_components)
        reduced_vectors = pca.fit_transform(vectors)
        
        # 返回额外的PCA信息
        pca_info = {
            "explained_variance_ratio": pca.explained_variance_ratio_.tolist(),
            "total_variance_explained": float(np.sum(pca.explained_variance_ratio_)),
            "components": pca.components_.tolist()
        }
        
        return reduced_vectors, pca_info
    
    async def create_interactive_plot(
        self, 
        reduced_vectors: np.ndarray,
        labels: Optional[List[str]] = None,
        colors: Optional[List[str]] = None
    ) -> str:
        """创建交互式图表（返回HTML）"""
        import plotly.graph_objects as go
        
        if reduced_vectors.shape[1] == 2:
            fig = go.Figure(data=[go.Scatter(
                x=reduced_vectors[:, 0],
                y=reduced_vectors[:, 1],
                mode='markers',
                marker=dict(
                    size=8,
                    color=colors if colors else 'blue',
                    colorscale='Viridis',
                    showscale=True
                ),
                text=labels if labels else None,
                hovertemplate='<b>%{text}</b><br>X: %{x}<br>Y: %{y}'
            )])
            
            fig.update_layout(
                title="Vector Visualization",
                xaxis_title="Component 1",
                yaxis_title="Component 2",
                hovermode='closest'
            )
        else:  # 3D
            fig = go.Figure(data=[go.Scatter3d(
                x=reduced_vectors[:, 0],
                y=reduced_vectors[:, 1],
                z=reduced_vectors[:, 2],
                mode='markers',
                marker=dict(
                    size=5,
                    color=colors if colors else 'blue',
                    colorscale='Viridis',
                    showscale=True
                ),
                text=labels if labels else None
            )])
            
            fig.update_layout(
                title="3D Vector Visualization",
                scene=dict(
                    xaxis_title="Component 1",
                    yaxis_title="Component 2",
                    zaxis_title="Component 3"
                )
            )
        
        return fig.to_html()

# ============= 自定义距离度量 =============

class DistanceMetrics:
    """距离度量计算器"""
    
    @staticmethod
    def cosine_similarity(v1: np.ndarray, v2: np.ndarray) -> float:
        """余弦相似度"""
        return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))
    
    @staticmethod
    def euclidean_distance(v1: np.ndarray, v2: np.ndarray) -> float:
        """欧氏距离"""
        return float(np.linalg.norm(v1 - v2))
    
    @staticmethod
    def manhattan_distance(v1: np.ndarray, v2: np.ndarray) -> float:
        """曼哈顿距离"""
        return float(np.sum(np.abs(v1 - v2)))
    
    @staticmethod
    def dot_product(v1: np.ndarray, v2: np.ndarray) -> float:
        """点积"""
        return float(np.dot(v1, v2))
    
    @staticmethod
    def hamming_distance(v1: np.ndarray, v2: np.ndarray) -> float:
        """汉明距离（适用于二进制向量）"""
        return float(np.sum(v1 != v2))
    
    @staticmethod
    def custom_distance(
        v1: np.ndarray, 
        v2: np.ndarray, 
        distance_func: Callable[[np.ndarray, np.ndarray], float]
    ) -> float:
        """自定义距离函数"""
        return distance_func(v1, v2)

# ============= 数据导入导出工具 =============

class VectorDataTools:
    """向量数据工具"""
    
    @staticmethod
    async def import_from_csv(file_path: str) -> Tuple[List[str], np.ndarray]:
        """从CSV导入向量"""
        import pandas as pd
        
        df = pd.read_csv(file_path)
        
        # 假设第一列是ID，其余列是向量数据
        ids = df.iloc[:, 0].tolist()
        vectors = df.iloc[:, 1:].values.astype(np.float32)
        
        return ids, vectors
    
    @staticmethod
    async def export_to_csv(
        ids: List[str], 
        vectors: np.ndarray, 
        file_path: str
    ):
        """导出向量到CSV"""
        import pandas as pd
        
        # 创建DataFrame
        df_data = {"id": ids}
        for i in range(vectors.shape[1]):
            df_data[f"dim_{i}"] = vectors[:, i]
        
        df = pd.DataFrame(df_data)
        df.to_csv(file_path, index=False)
    
    @staticmethod
    async def import_from_json(file_path: str) -> Dict[str, np.ndarray]:
        """从JSON导入向量"""
        import json
        
        with open(file_path, 'r') as f:
            data = json.load(f)
        
        vectors = {}
        for key, value in data.items():
            vectors[key] = np.array(value, dtype=np.float32)
        
        return vectors
    
    @staticmethod
    async def export_to_json(vectors: Dict[str, np.ndarray], file_path: str):
        """导出向量到JSON"""
        import json
        
        # 转换numpy数组为列表
        data = {}
        for key, value in vectors.items():
            data[key] = value.tolist()
        
        with open(file_path, 'w') as f:
            json.dump(data, f, indent=2)
    
    @staticmethod
    async def migrate_vectors(
        source_db_config: Dict[str, Any],
        target_db_config: Dict[str, Any],
        batch_size: int = 1000
    ) -> Dict[str, Any]:
        """跨数据库迁移向量"""
        import asyncpg
        
        # 连接源数据库
        source_conn = await asyncpg.connect(**source_db_config)
        target_conn = await asyncpg.connect(**target_db_config)
        
        try:
            # 获取向量总数
            count_query = "SELECT COUNT(*) FROM vector_table"
            total_count = await source_conn.fetchval(count_query)
            
            migrated = 0
            offset = 0
            
            while offset < total_count:
                # 批量读取
                fetch_query = f"""
                SELECT id, embedding 
                FROM vector_table 
                LIMIT {batch_size} OFFSET {offset}
                """
                rows = await source_conn.fetch(fetch_query)
                
                # 批量插入
                for row in rows:
                    insert_query = """
                    INSERT INTO vector_table (id, embedding) 
                    VALUES ($1, $2)
                    ON CONFLICT (id) DO UPDATE SET embedding = $2
                    """
                    await target_conn.execute(insert_query, row['id'], row['embedding'])
                
                migrated += len(rows)
                offset += batch_size
            
            return {
                "status": "success",
                "total_migrated": migrated,
                "source": source_db_config.get("database"),
                "target": target_db_config.get("database")
            }
            
        finally:
            await source_conn.close()
            await target_conn.close()

# ============= API集成接口 =============

class VectorAdvancedAPI:
    """向量高级功能API"""
    
    def __init__(self):
        self.index_manager = MultiIndexManager()
        self.hybrid_search = HybridSearchEngine()
        self.multimodal = MultiModalSearch()
        self.temporal = TemporalVectorManager()
        self.clustering = VectorClusteringAnalyzer()
        self.visualizer = VectorVisualizer()
        self.metrics = DistanceMetrics()
        self.data_tools = VectorDataTools()
    
    async def process_request(
        self, 
        operation: str, 
        params: Dict[str, Any]
    ) -> Dict[str, Any]:
        """处理API请求"""
        
        operations = {
            "create_index": self._handle_create_index,
            "search": self._handle_search,
            "cluster": self._handle_clustering,
            "visualize": self._handle_visualization,
            "import_data": self._handle_import,
            "export_data": self._handle_export
        }
        
        if operation in operations:
            return await operations[operation](params)
        else:
            return {"error": f"Unknown operation: {operation}"}
    
    async def _handle_create_index(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """处理创建索引请求"""
        vectors = np.array(params["vectors"])
        index_type = params.get("index_type", "hnsw")
        
        if index_type == "hnsw":
            index_id = self.index_manager.create_hnsw_index(vectors)
        elif index_type == "ivf":
            index_id = self.index_manager.create_ivf_index(vectors)
        elif index_type == "lsh":
            index_id = self.index_manager.create_lsh_index(vectors)
        elif index_type == "annoy":
            index_id = self.index_manager.create_annoy_index(vectors)
        else:
            return {"error": f"Unknown index type: {index_type}"}
        
        return {
            "status": "success",
            "index_id": index_id,
            "index_type": index_type,
            "num_vectors": len(vectors)
        }
```

### Testing Requirements
[Source: docs/architecture/testing-strategy.md#functional-testing]
- **功能测试覆盖率**: ≥90%
- **多样性测试**: 验证所有索引类型和搜索模式
- **集成测试**: 验证各功能模块的集成
- **可视化测试**: 验证降维和可视化输出

#### Testing Standards
- 使用pytest进行功能测试
- 使用示例数据集验证各种功能
- 使用matplotlib/plotly验证可视化输出
- 测试各种距离度量的正确性

#### Specific Test Scenarios for This Story
- 多种索引类型创建和搜索测试
- 语义搜索和混合搜索测试
- 多模态向量编码和搜索测试
- 时序向量功能测试
- 聚类和异常检测测试
- 可视化降维算法测试
- 数据导入导出测试

### Testing
#### Test File Locations
- **索引管理测试**: `apps/api/tests/ai/rag/advanced_features/test_multi_index.py`
- **混合搜索测试**: `apps/api/tests/ai/rag/advanced_features/test_hybrid_search.py`
- **多模态测试**: `apps/api/tests/ai/rag/advanced_features/test_multimodal.py`
- **时序向量测试**: `apps/api/tests/ai/rag/advanced_features/test_temporal.py`
- **聚类测试**: `apps/api/tests/ai/rag/advanced_features/test_clustering.py`
- **可视化测试**: `apps/api/tests/ai/rag/advanced_features/test_visualization.py`
- **距离度量测试**: `apps/api/tests/ai/rag/advanced_features/test_metrics.py`
- **数据工具测试**: `apps/api/tests/ai/rag/advanced_features/test_data_tools.py`

#### Testing Requirements for This Story
- 验证所有索引类型的创建和搜索
- 验证混合搜索的融合效果
- 验证多模态向量的编码质量
- 验证聚类算法的正确性
- 验证可视化输出的有效性
- 验证数据导入导出的完整性

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-08-15 | 1.0 | Initial story creation for Epic EPM-004 Phase 3.1 vector advanced features | Bob (SM) |
| 2025-08-15 | 1.1 | Refocused from performance optimization to feature diversity for learning | Bob (SM) |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

## QA Results
*This section will be populated by the QA agent after implementation review*