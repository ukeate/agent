# Story 7.1: 实时语音转文本系统

## Status
Draft

## Story
**As a** AI系统用户,
**I want** 使用语音输入与AI Agent进行实时交互，
**so that** 我可以通过自然语音对话的方式与系统交互，无需手动输入文本，获得更加便捷和自然的用户体验

## Acceptance Criteria

1. **实时语音识别集成**
   - 集成OpenAI Whisper或Azure Speech Services进行语音转文本
   - 实现实时流式音频处理，支持连续语音输入
   - 语音识别准确率在清晰环境下达到≥95%
   - 识别延迟控制在<2秒内

2. **多语言支持**
   - 支持中文(zh-CN)、英文(en-US)、日文(ja-JP)等主要语言
   - 实现自动语言检测功能
   - 支持语言切换和混合语言识别
   - 各语言识别准确率均达到≥90%

3. **音频预处理和优化**
   - 实现噪音抑制和背景音过滤
   - 支持回声消除和自动增益控制
   - 实现语音活动检测(VAD)功能
   - 音频格式标准化处理(16kHz, 16bit, mono)

4. **流式处理架构**
   - 实现WebSocket或WebRTC音频流传输
   - 支持音频分片和实时处理
   - 实现音频缓冲区管理和内存优化
   - 支持流式转录结果返回

5. **错误处理和降级**
   - 实现网络中断的重连机制
   - 支持音频质量检测和警告
   - 提供语音识别失败的用户提示
   - 实现音频录制失败的降级处理

## Tasks / Subtasks

- [ ] **Task 1: 语音识别服务集成** (AC: 1)
  - [ ] 集成OpenAI Whisper本地模型和Azure Speech服务
  - [ ] 创建`apps/api/src/ai/voice/speech_recognition.py`语音识别模块
  - [ ] 实现异步语音转文本API
  - [ ] 配置模型参数和优化策略
  - [ ] 添加语音识别配置到`apps/api/src/core/config.py`

- [ ] **Task 2: 实时流式处理** (AC: 4)
  - [ ] 创建`apps/api/src/ai/voice/stream_processor.py`流处理模块
  - [ ] 实现WebSocket音频流处理端点
  - [ ] 设计音频分片和缓冲机制
  - [ ] 实现流式转录结果推送
  - [ ] 优化音频流传输性能

- [ ] **Task 3: 音频预处理系统** (AC: 3)
  - [ ] 创建`apps/api/src/ai/voice/audio_processor.py`音频处理模块
  - [ ] 实现VAD语音活动检测
  - [ ] 集成噪音抑制和回声消除算法
  - [ ] 实现音频格式标准化
  - [ ] 添加音频质量检测和优化

- [ ] **Task 4: 多语言识别系统** (AC: 2)
  - [ ] 创建`apps/api/src/ai/voice/language_detector.py`语言检测模块
  - [ ] 实现多语言模型配置和切换
  - [ ] 支持自动语言检测功能
  - [ ] 实现混合语言处理策略
  - [ ] 优化各语言识别性能

- [ ] **Task 5: WebSocket通信层** (AC: 4)
  - [ ] 创建`apps/api/src/api/v1/voice_websocket.py`WebSocket端点
  - [ ] 实现客户端音频流接收
  - [ ] 设计实时通信协议
  - [ ] 实现连接管理和状态维护
  - [ ] 添加WebSocket错误处理

- [ ] **Task 6: 前端语音输入组件** (AC: 1, 4)
  - [ ] 创建`apps/web/src/components/voice/VoiceInput.tsx`语音输入组件
  - [ ] 实现浏览器媒体流API集成
  - [ ] 设计语音录制UI和控制逻辑
  - [ ] 实现实时音量显示和波形可视化
  - [ ] 添加语音输入状态管理

- [ ] **Task 7: 错误处理和监控** (AC: 5)
  - [ ] 实现语音识别错误处理机制
  - [ ] 添加音频质量监控和告警
  - [ ] 创建语音识别性能统计
  - [ ] 实现用户友好的错误提示
  - [ ] 集成到系统监控框架

- [ ] **Task 8: 测试和集成** (AC: 1, 2, 3, 4)
  - [ ] 创建语音识别单元测试
  - [ ] 实现多语言识别准确率测试
  - [ ] 进行音频处理性能测试
  - [ ] 执行端到端语音交互测试
  - [ ] 验证系统集成和兼容性

## Dev Notes

### Epic Context
这是Epic 7: 实时语音交互系统的第一个故事，为整个语音交互能力奠定基础。基于Epic 7的技术规格，需要实现高质量的实时语音转文本能力。

### Tech Stack Requirements  
**语音处理技术栈** [Source: docs/prd/upgrade-2025/epics/epic-007-real-time-voice-interaction.md]:
- **语音识别**: OpenAI Whisper (本地) + Azure Speech Services (云端)
- **音频处理**: librosa, webrtcvad, pyaudio
- **流式通信**: WebSocket, WebRTC
- **前端媒体**: Web Audio API, MediaRecorder API
- **语言支持**: 中文、英文、日文等主要语言

### Data Models
**语音识别相关数据结构** [Source: Epic 7技术实现]:
```python
from typing import TypedDict, Optional, List, Dict, Any
from datetime import datetime
from enum import Enum

class SpeechLanguage(str, Enum):
    """支持的语言类型"""
    CHINESE = "zh-CN"
    ENGLISH = "en-US"
    JAPANESE = "ja-JP"
    AUTO_DETECT = "auto"

class AudioFormat(TypedDict):
    """音频格式配置"""
    sample_rate: int  # 16000
    channels: int     # 1 (mono)
    bit_depth: int    # 16
    encoding: str     # "pcm"

class SpeechRecognitionConfig(TypedDict):
    """语音识别配置"""
    language: SpeechLanguage
    model_type: str  # "whisper-base", "azure-standard"
    enable_vad: bool
    noise_reduction: bool
    auto_language_detect: bool
    confidence_threshold: float

class TranscriptionResult(TypedDict):
    """转录结果"""
    text: str
    confidence: float
    language: str
    is_partial: bool  # 是否为部分结果
    duration_ms: int
    timestamp: datetime

class AudioSegment(TypedDict):
    """音频片段"""
    audio_data: bytes
    sample_rate: int
    duration_ms: int
    is_speech: bool   # VAD检测结果
    language_hint: Optional[str]
```

### API Specifications
**语音识别API端点** [Source: 基于Epic 7的API设计]:
```python
# WebSocket语音流处理
WS /api/v1/voice/stream - 实时语音流处理WebSocket
# 消息格式:
# Client -> Server: {"type": "audio", "data": base64_audio, "config": {...}}
# Server -> Client: {"type": "transcription", "text": "...", "confidence": 0.95}

# HTTP语音识别API
POST /api/v1/voice/recognize - 单次语音识别
POST /api/v1/voice/upload - 音频文件上传识别
GET /api/v1/voice/languages - 获取支持的语言列表
GET /api/v1/voice/models - 获取可用的识别模型

# 语音配置API
GET /api/v1/voice/config - 获取语音识别配置
PUT /api/v1/voice/config - 更新语音识别配置
POST /api/v1/voice/test - 语音识别测试接口
```

### File Locations
基于项目结构 [Source: architecture/unified-project-structure.md]:
- **后端语音模块**: `apps/api/src/ai/voice/`
  - `speech_recognition.py` - 语音识别核心引擎（新建）
  - `stream_processor.py` - 实时流处理器（新建）
  - `audio_processor.py` - 音频预处理模块（新建）
  - `language_detector.py` - 语言检测器（新建）
  - `__init__.py` - 模块初始化（新建）
- **API端点**: `apps/api/src/api/v1/`
  - `voice_websocket.py` - WebSocket语音端点（新建）
  - `voice.py` - HTTP语音API端点（新建）
- **前端语音组件**: `apps/web/src/components/voice/`
  - `VoiceInput.tsx` - 语音输入组件（新建）
  - `VoiceRecorder.tsx` - 录音功能组件（新建）
  - `AudioVisualizer.tsx` - 音频可视化组件（新建）
- **测试文件**: `apps/api/tests/ai/voice/`
  - `test_speech_recognition.py` - 语音识别测试（新建）
  - `test_audio_processing.py` - 音频处理测试（新建）

### Technical Constraints
**语音识别技术要求** [Source: Epic 7成功标准]:
- **识别准确率**: 清晰环境≥95%，噪音环境≥85%
- **响应延迟**: 语音转文本<2秒，流式处理<500ms
- **音频格式**: 16kHz采样率，16bit深度，单声道
- **语言支持**: 至少支持中、英、日3种语言
- **并发能力**: 支持50+同时语音识别会话

### Whisper Integration Architecture
**Whisper模型集成策略**:
```python
import whisper
import asyncio
from typing import AsyncGenerator

class WhisperASREngine:
    """Whisper语音识别引擎"""
    
    def __init__(self, model_size: str = "base"):
        self.model = whisper.load_model(model_size)
        self.supported_languages = ["zh", "en", "ja", "auto"]
    
    async def transcribe_stream(
        self, 
        audio_stream: AsyncGenerator[bytes, None],
        language: str = "auto"
    ) -> AsyncGenerator[TranscriptionResult, None]:
        """流式转录"""
        audio_buffer = bytearray()
        
        async for audio_chunk in audio_stream:
            audio_buffer.extend(audio_chunk)
            
            # 当缓冲区达到一定大小时进行转录
            if len(audio_buffer) >= 16000:  # 1秒音频
                result = await self._transcribe_chunk(
                    audio_buffer, language
                )
                if result:
                    yield result
                
                # 保留重叠部分避免切断单词
                overlap_size = 8000  # 0.5秒重叠
                audio_buffer = audio_buffer[-overlap_size:]
    
    async def _transcribe_chunk(
        self, 
        audio_data: bytearray, 
        language: str
    ) -> Optional[TranscriptionResult]:
        """转录音频块"""
        # 转换为whisper需要的格式
        audio_np = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0
        
        # 异步执行转录避免阻塞
        result = await asyncio.get_event_loop().run_in_executor(
            None, 
            self.model.transcribe, 
            audio_np, 
            {"language": language if language != "auto" else None}
        )
        
        return TranscriptionResult(
            text=result["text"].strip(),
            confidence=1.0,  # Whisper不提供confidence，设为1.0
            language=result["language"],
            is_partial=False,
            duration_ms=len(audio_data) // 32,  # 16kHz 16bit
            timestamp=datetime.utcnow()
        )
```

### Audio Processing Pipeline
**音频处理管道设计**:
1. **音频采集**: 浏览器MediaRecorder -> WebSocket传输
2. **格式转换**: Opus/WebM -> PCM 16kHz
3. **预处理**: 噪音抑制 -> VAD检测 -> 语言检测
4. **识别处理**: Whisper/Azure模型推理
5. **结果处理**: 置信度过滤 -> 文本后处理 -> 客户端推送

### Performance Optimization
**性能优化策略**:
- **模型优化**: 使用Whisper base模型平衡准确率和速度
- **流式处理**: 音频分片处理，避免长时间等待
- **缓存机制**: 相同音频片段缓存识别结果
- **批处理**: 多个短音频合并处理提升效率
- **模型池**: 维护多个模型实例支持并发

### Testing Requirements
基于测试策略 [Source: architecture/testing-strategy.md]:
- **语音识别准确率测试**: 不同语言、环境下的识别准确率验证
- **性能测试**: 响应延迟、并发能力、内存使用测试
- **兼容性测试**: 不同浏览器、设备的音频API兼容性
- **错误恢复测试**: 网络中断、模型异常等场景的处理验证
- **端到端测试**: 完整语音输入到文本显示的流程测试

### Testing
**位置**: apps/api/tests/ai/voice/
**框架**: pytest + pytest-asyncio + pytest-benchmark
**覆盖率**: 语音识别模块需要≥85%测试覆盖率
**重点测试**:
- Whisper模型集成和参数优化验证
- 多语言识别准确率和自动检测功能
- 实时流式处理性能和稳定性测试
- 音频预处理和VAD检测准确性验证
- WebSocket通信和错误恢复机制测试

## Dev Agent Record

### Agent Model Used
[待开发时填写]

### Debug Log References
[待开发时填写]

### Completion Notes List
[待开发时填写]

### File List
[待开发时填写]

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|---------|
| 2025-08-22 | 1.0 | Initial story creation for real-time speech-to-text system | Bob (Scrum Master) |

## QA Results
[待QA审查后填写]