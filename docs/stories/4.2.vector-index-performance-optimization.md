# Story 4.2: 向量索引高级功能实现

## Status
Done

<!--  -->
## Story
**As a** AI系统学习者,
**I want** 在pgvector 0.8基础上实现多样化的向量索引高级功能和搜索策略,
**so that** 我可以学习和掌握各种向量检索技术，包括语义搜索、图像搜索、混合搜索、时序向量等多种应用场景

## Acceptance Criteria
1. 实现多种向量索引类型和搜索策略（HNSW、IVF、LSH等）
2. 实现语义相似度搜索和关键词混合搜索
3. 支持图像向量搜索和多模态向量融合
4. 实现时序向量索引和轨迹相似度搜索
5. 支持向量聚类和异常检测功能
6. 实现向量可视化和降维展示（t-SNE、UMAP）
7. 支持自定义距离度量（余弦、欧氏、曼哈顿等）
8. 实现向量数据的导入导出和迁移工具

## Tasks / Subtasks
- [x] 多种索引类型实现 (AC: 1)
  - [x] 实现HNSW索引的完整功能
  - [x] 实现IVF（倒排文件）索引
  - [x] 实现LSH（局部敏感哈希）索引
  - [x] 实现索引类型的动态切换
- [x] 语义和混合搜索 (AC: 2)
  - [x] 实现纯语义向量搜索
  - [x] 实现BM25关键词搜索集成
  - [x] 实现混合搜索评分融合
  - [x] 实现查询扩展和同义词处理
- [x] 多模态向量搜索 (AC: 3)
  - [x] 实现图像向量编码和搜索
  - [x] 实现文本-图像跨模态搜索
  - [x] 实现音频向量搜索支持
  - [x] 实现多模态向量融合策略
- [x] 时序向量功能 (AC: 4)
  - [x] 实现时间序列向量索引
  - [x] 实现轨迹相似度计算
  - [x] 实现时序模式匹配
  - [x] 实现向量变化趋势分析
- [x] 向量聚类和异常检测 (AC: 5)
  - [x] 实现K-means向量聚类
  - [x] 实现DBSCAN密度聚类
  - [x] 实现异常向量检测算法
  - [x] 实现聚类结果可视化
- [x] 向量可视化工具 (AC: 6)
  - [x] 实现t-SNE降维可视化
  - [x] 实现UMAP降维算法
  - [x] 实现PCA主成分分析
  - [x] 实现交互式向量探索界面
- [x] 自定义距离度量 (AC: 7)
  - [x] 实现余弦相似度计算
  - [x] 实现欧氏距离计算
  - [x] 实现曼哈顿距离计算
  - [x] 实现自定义距离函数接口
- [x] 数据导入导出工具 (AC: 8)
  - [x] 实现CSV/JSON向量导入
  - [x] 实现向量数据批量导出
  - [x] 实现跨数据库迁移工具
  - [x] 实现向量数据备份恢复

## Dev Notes

### Previous Story Insights
基于Story 4.1已完成的pgvector 0.8升级和量化压缩基础：
- pgvector已升级到0.8版本，支持高级索引特性
- 已实现INT8/INT4量化，内存使用减少75%+
- 已建立基础的HNSW和IVF索引
- 已实现向量缓存和LRU策略
- 已建立性能监控和基准测试框架

现在需要在此基础上进行深度性能优化，实现亚毫秒级检索和百万级向量支持。

### Tech Stack Context
[Source: docs/architecture/tech-stack.md#performance-optimization]
- **Database**: PostgreSQL 15+ with pgvector 0.8 (已升级)
- **Vector Index**: HNSW, IVF, Flat索引支持
- **Cache**: Redis 7.2+ (向量缓存层)
- **Compute**: CPU优化（SIMD）+ 可选GPU加速
- **Monitoring**: OpenTelemetry (性能追踪)
- **Language**: Python 3.11+ with NumPy/Faiss优化

### Project Structure Context
[Source: docs/architecture/unified-project-structure.md#performance-optimization]
向量索引优化相关文件位置：
- **索引优化器**: `apps/api/src/ai/rag/index_optimizer.py` (扩展)
- **查询优化器**: `apps/api/src/ai/rag/query_optimizer.py` (新增)
- **多级索引**: `apps/api/src/ai/rag/tiered_index.py` (新增)
- **并行搜索**: `apps/api/src/ai/rag/parallel_search.py` (新增)
- **GPU加速**: `apps/api/src/ai/rag/gpu_accelerator.py` (新增，可选)
- **索引维护**: `apps/api/src/ai/rag/index_maintenance.py` (新增)
- **后处理优化**: `apps/api/src/ai/rag/result_processor.py` (新增)
- **相关测试**: `apps/api/tests/ai/rag/index_optimization/` (新增目录)

### Integration with Existing Code
基于Story 4.1已实现的组件，需要扩展：
- **扩展**: `apps/api/src/ai/rag/pgvector_optimizer.py` - 添加高级索引优化
- **扩展**: `apps/api/src/ai/rag/performance_monitor.py` - 添加亚毫秒级监控
- **修改**: `apps/api/src/ai/rag/hybrid_retrieval.py` - 集成优化的索引策略
- **更新**: `apps/api/src/ai/rag/vector_cache.py` - 支持多级缓存
- **集成**: `apps/api/src/services/rag_service.py` - 使用优化的搜索管道

### Advanced Vector Features Architecture
高级向量功能架构设计（学习型实现）：

```python
import numpy as np
from typing import Dict, Any, List, Optional, Tuple, Union, Callable
from enum import Enum
from dataclasses import dataclass
import asyncio
import logging
from datetime import datetime, timezone
import json
import base64
from PIL import Image
import io

class IndexType(str, Enum):
    """索引类型枚举"""
    HNSW = "hnsw"           # 分层导航小世界图
    IVF = "ivf"             # 倒排文件索引
    LSH = "lsh"             # 局部敏感哈希
    FLAT = "flat"           # 暴力搜索
    ANNOY = "annoy"         # Spotify的近似最近邻库

class SearchMode(str, Enum):
    """搜索模式"""
    SEMANTIC = "semantic"              # 语义搜索
    KEYWORD = "keyword"                # 关键词搜索
    HYBRID = "hybrid"                  # 混合搜索
    IMAGE = "image"                    # 图像搜索
    CROSS_MODAL = "cross_modal"        # 跨模态搜索
    TEMPORAL = "temporal"              # 时序搜索

class DistanceMetric(str, Enum):
    """距离度量类型"""
    COSINE = "cosine"                  # 余弦相似度
    EUCLIDEAN = "euclidean"            # 欧氏距离
    MANHATTAN = "manhattan"            # 曼哈顿距离
    DOT_PRODUCT = "dot_product"        # 点积
    HAMMING = "hamming"                # 汉明距离

@dataclass
class VectorSearchConfig:
    """向量搜索配置"""
    index_type: IndexType = IndexType.HNSW
    search_mode: SearchMode = SearchMode.SEMANTIC
    distance_metric: DistanceMetric = DistanceMetric.COSINE
    top_k: int = 10
    enable_rerank: bool = True
    enable_filter: bool = True

class MultiIndexManager:
    """自适应索引选择器"""
    
    def __init__(self):
        self.query_history = []
        self.performance_stats = {}
        self.data_stats = {}
        
    async def analyze_data_characteristics(
        self, 
        table_name: str,
        sample_size: int = 10000
    ) -> Dict[str, Any]:
        """分析数据特征"""
        stats = {
            "total_vectors": 0,
            "dimension": 0,
            "density": 0.0,
            "distribution": "unknown",
            "clustering_coefficient": 0.0
        }
        
        # 获取数据规模
        count_query = f"SELECT COUNT(*) FROM {table_name}"
        stats["total_vectors"] = await self._execute_query(count_query)
        
        # 采样分析数据分布
        sample_query = f"""
        SELECT embedding 
        FROM {table_name} 
        TABLESAMPLE SYSTEM (1) 
        LIMIT {sample_size}
        """
        samples = await self._execute_query(sample_query)
        
        if samples:
            vectors = np.array([s.embedding for s in samples])
            stats["dimension"] = vectors.shape[1]
            stats["density"] = self._calculate_density(vectors)
            stats["distribution"] = self._analyze_distribution(vectors)
            stats["clustering_coefficient"] = self._calculate_clustering(vectors)
        
        self.data_stats[table_name] = stats
        return stats
    
    async def select_optimal_index(
        self,
        data_stats: Dict[str, Any],
        query_pattern: QueryPattern,
        performance_target: PerformanceTarget
    ) -> IndexConfig:
        """选择最优索引策略"""
        total_vectors = data_stats["total_vectors"]
        dimension = data_stats["dimension"]
        density = data_stats["density"]
        
        # 根据数据规模选择基础策略
        if total_vectors < 10000:
            # 小数据集使用暴力搜索
            base_strategy = IndexStrategy.FLAT
        elif total_vectors < 100000:
            # 中等数据集使用IVF
            base_strategy = IndexStrategy.IVF
        elif total_vectors < 1000000:
            # 大数据集使用HNSW
            base_strategy = IndexStrategy.HNSW
        else:
            # 超大数据集使用混合策略
            base_strategy = IndexStrategy.HYBRID
        
        # 根据查询模式调整参数
        config = IndexConfig(strategy=base_strategy)
        
        if query_pattern == QueryPattern.BATCH_QUERY:
            # 批量查询优化
            config.parallel_workers = min(16, psutil.cpu_count())
            config.hnsw_ef_search = 100  # 降低单次查询开销
        elif query_pattern == QueryPattern.SINGLE_POINT:
            # 单点查询优化
            config.hnsw_ef_search = 300  # 提高精度
            config.use_quantization = False  # 禁用量化提高精度
        elif query_pattern == QueryPattern.RANGE_SEARCH:
            # 范围搜索优化
            config.strategy = IndexStrategy.IVF
            config.ivf_nprobe = 64  # 增加探测范围
        
        # 根据性能目标调整
        if performance_target.p50_latency_ms < 1.0:
            # 极低延迟要求
            config.use_quantization = True
            config.enable_gpu = True
        
        return config
    
    def _calculate_density(self, vectors: np.ndarray) -> float:
        """计算向量密度"""
        # 计算向量之间的平均距离
        n_samples = min(1000, len(vectors))
        sample_indices = np.random.choice(len(vectors), n_samples, replace=False)
        sample_vectors = vectors[sample_indices]
        
        distances = []
        for i in range(n_samples):
            for j in range(i+1, min(i+10, n_samples)):
                dist = np.linalg.norm(sample_vectors[i] - sample_vectors[j])
                distances.append(dist)
        
        return np.mean(distances) if distances else 0.0
    
    def _analyze_distribution(self, vectors: np.ndarray) -> str:
        """分析向量分布"""
        # 简化的分布分析
        variances = np.var(vectors, axis=0)
        if np.std(variances) < 0.1:
            return "uniform"
        elif np.max(variances) / np.min(variances) > 10:
            return "skewed"
        else:
            return "normal"

class HNSWOptimizer:
    """HNSW索引深度优化器"""
    
    def __init__(self, db_session):
        self.db = db_session
        self.performance_history = []
        
    async def optimize_parameters(
        self,
        table_name: str,
        vector_column: str,
        target_recall: float = 0.95
    ) -> Dict[str, int]:
        """优化HNSW参数"""
        best_params = {
            "m": 16,
            "ef_construction": 200,
            "ef_search": 100
        }
        
        # 参数搜索空间
        m_values = [16, 32, 48, 64]
        ef_construction_values = [200, 400, 600, 800]
        
        best_score = 0
        
        for m in m_values:
            for ef_c in ef_construction_values:
                # 创建测试索引
                index_name = f"test_hnsw_m{m}_ef{ef_c}"
                await self._create_test_index(
                    table_name, vector_column, index_name, m, ef_c
                )
                
                # 测试性能
                performance = await self._test_index_performance(
                    table_name, vector_column, index_name
                )
                
                # 计算综合评分
                score = self._calculate_score(performance, target_recall)
                
                if score > best_score:
                    best_score = score
                    best_params = {
                        "m": m,
                        "ef_construction": ef_c,
                        "ef_search": self._calculate_optimal_ef_search(m)
                    }
                
                # 清理测试索引
                await self._drop_test_index(index_name)
        
        return best_params
    
    async def dynamic_ef_adjustment(
        self,
        current_ef: int,
        latency_ms: float,
        recall: float,
        target_latency: float,
        target_recall: float
    ) -> int:
        """动态调整ef_search参数"""
        if recall < target_recall:
            # 召回率不足，增加ef
            new_ef = min(current_ef + 50, 1000)
        elif latency_ms > target_latency:
            # 延迟过高，减少ef
            new_ef = max(current_ef - 25, 50)
        else:
            # 性能满足要求，微调
            new_ef = current_ef
        
        return new_ef
    
    def _calculate_optimal_ef_search(self, m: int) -> int:
        """计算最优ef_search值"""
        # 经验公式：ef_search = m * 2 到 m * 4之间
        return m * 3

class TieredIndexManager:
    """多级索引管理器"""
    
    def __init__(self, cache_manager, db_session):
        self.cache = cache_manager
        self.db = db_session
        self.tier_config = {
            "hot": {
                "max_size": 100000,
                "index_type": "hnsw",
                "storage": "memory"
            },
            "warm": {
                "max_size": 1000000,
                "index_type": "ivf",
                "storage": "ssd"
            },
            "cold": {
                "max_size": float('inf'),
                "index_type": "compressed",
                "storage": "disk"
            }
        }
    
    async def classify_vectors(
        self,
        access_patterns: Dict[str, int]
    ) -> Dict[str, str]:
        """根据访问模式分类向量"""
        classifications = {}
        
        # 计算访问频率阈值
        access_counts = list(access_patterns.values())
        if not access_counts:
            return classifications
        
        hot_threshold = np.percentile(access_counts, 90)
        warm_threshold = np.percentile(access_counts, 50)
        
        for vector_id, access_count in access_patterns.items():
            if access_count >= hot_threshold:
                classifications[vector_id] = "hot"
            elif access_count >= warm_threshold:
                classifications[vector_id] = "warm"
            else:
                classifications[vector_id] = "cold"
        
        return classifications
    
    async def migrate_vectors_between_tiers(
        self,
        classifications: Dict[str, str]
    ) -> Dict[str, int]:
        """在不同层级间迁移向量"""
        migration_stats = {
            "to_hot": 0,
            "to_warm": 0,
            "to_cold": 0
        }
        
        for vector_id, new_tier in classifications.items():
            current_tier = await self._get_current_tier(vector_id)
            
            if current_tier != new_tier:
                success = await self._migrate_vector(
                    vector_id, current_tier, new_tier
                )
                if success:
                    migration_stats[f"to_{new_tier}"] += 1
        
        return migration_stats
    
    async def tiered_search(
        self,
        query_vector: np.ndarray,
        top_k: int = 10,
        search_tiers: List[str] = ["hot", "warm", "cold"]
    ) -> List[Dict[str, Any]]:
        """分层搜索"""
        all_results = []
        remaining_k = top_k
        
        for tier in search_tiers:
            if remaining_k <= 0:
                break
            
            # 在当前层搜索
            tier_results = await self._search_tier(
                tier, query_vector, remaining_k * 2  # 过采样
            )
            
            all_results.extend(tier_results)
            
            # 如果热数据层已经找到足够结果，可以提前结束
            if tier == "hot" and len(tier_results) >= top_k:
                break
            
            remaining_k -= len(tier_results)
        
        # 合并和排序结果
        all_results.sort(key=lambda x: x["distance"])
        return all_results[:top_k]

class QueryOptimizer:
    """智能查询优化器"""
    
    def __init__(self):
        self.query_cache = {}
        self.query_stats = {}
        
    async def optimize_query(
        self,
        query_vector: np.ndarray,
        query_type: QueryPattern,
        context: Dict[str, Any]
    ) -> Tuple[np.ndarray, Dict[str, Any]]:
        """优化查询向量和参数"""
        optimized_vector = query_vector.copy()
        optimization_params = {}
        
        # 1. 向量预处理
        optimized_vector = await self._preprocess_vector(optimized_vector)
        
        # 2. 查询扩展（如果需要）
        if context.get("enable_expansion", False):
            expanded_vectors = await self._expand_query(optimized_vector)
            optimization_params["expanded_queries"] = expanded_vectors
        
        # 3. 查询缓存检查
        cache_key = self._compute_cache_key(optimized_vector)
        if cache_key in self.query_cache:
            optimization_params["cached_result"] = self.query_cache[cache_key]
        
        # 4. 批量优化
        if query_type == QueryPattern.BATCH_QUERY:
            optimization_params["batch_size"] = self._optimal_batch_size()
            optimization_params["parallel_execution"] = True
        
        return optimized_vector, optimization_params
    
    async def _preprocess_vector(self, vector: np.ndarray) -> np.ndarray:
        """预处理查询向量"""
        # L2归一化
        norm = np.linalg.norm(vector)
        if norm > 0:
            vector = vector / norm
        
        # 降维（如果配置了PCA）
        # vector = self.pca.transform(vector.reshape(1, -1))[0]
        
        return vector
    
    async def _expand_query(
        self, 
        vector: np.ndarray, 
        n_expansions: int = 3
    ) -> List[np.ndarray]:
        """查询扩展"""
        expanded = [vector]
        
        # 添加微小扰动生成相似查询
        for _ in range(n_expansions - 1):
            noise = np.random.normal(0, 0.01, vector.shape)
            expanded_vector = vector + noise
            expanded_vector = expanded_vector / np.linalg.norm(expanded_vector)
            expanded.append(expanded_vector)
        
        return expanded

class ParallelSearchEngine:
    """并行搜索引擎"""
    
    def __init__(self, n_workers: int = 8):
        self.n_workers = n_workers
        self.thread_pool = ThreadPoolExecutor(max_workers=n_workers)
        self.process_pool = ProcessPoolExecutor(max_workers=n_workers // 2)
        
    async def parallel_search(
        self,
        query_vectors: List[np.ndarray],
        search_func: Callable,
        top_k: int = 10
    ) -> List[List[Dict[str, Any]]]:
        """并行执行多个向量搜索"""
        
        # 分配任务到工作线程
        futures = []
        for query_vector in query_vectors:
            future = self.thread_pool.submit(
                self._execute_search,
                query_vector,
                search_func,
                top_k
            )
            futures.append(future)
        
        # 收集结果
        results = []
        for future in futures:
            result = await asyncio.get_event_loop().run_in_executor(
                None, future.result
            )
            results.append(result)
        
        return results
    
    def _execute_search(
        self,
        query_vector: np.ndarray,
        search_func: Callable,
        top_k: int
    ) -> List[Dict[str, Any]]:
        """执行单个搜索"""
        return search_func(query_vector, top_k)
    
    async def simd_optimized_distance(
        self,
        query_vector: np.ndarray,
        database_vectors: np.ndarray
    ) -> np.ndarray:
        """SIMD优化的距离计算"""
        # 使用NumPy的向量化操作（自动使用SIMD）
        differences = database_vectors - query_vector
        distances = np.linalg.norm(differences, axis=1)
        return distances

class GPUAccelerator:
    """GPU加速器（可选）"""
    
    def __init__(self):
        self.gpu_available = self._check_gpu_availability()
        
    def _check_gpu_availability(self) -> bool:
        """检查GPU可用性"""
        try:
            import cupy as cp
            return cp.cuda.runtime.getDeviceCount() > 0
        except ImportError:
            return False
    
    async def gpu_accelerated_search(
        self,
        query_vector: np.ndarray,
        database_vectors: np.ndarray,
        top_k: int = 10
    ) -> List[int]:
        """GPU加速的向量搜索"""
        if not self.gpu_available:
            # 降级到CPU
            return await self._cpu_search(query_vector, database_vectors, top_k)
        
        try:
            import cupy as cp
            
            # 传输数据到GPU
            query_gpu = cp.asarray(query_vector)
            database_gpu = cp.asarray(database_vectors)
            
            # GPU上计算距离
            distances = cp.linalg.norm(database_gpu - query_gpu, axis=1)
            
            # 获取top-k索引
            top_k_indices = cp.argpartition(distances, top_k)[:top_k]
            top_k_indices = top_k_indices[cp.argsort(distances[top_k_indices])]
            
            # 传回CPU
            return top_k_indices.get().tolist()
            
        except Exception as e:
            logging.warning(f"GPU search failed, falling back to CPU: {e}")
            return await self._cpu_search(query_vector, database_vectors, top_k)

class IndexMaintenanceScheduler:
    """索引维护调度器"""
    
    def __init__(self, db_session):
        self.db = db_session
        self.maintenance_history = []
        
    async def detect_fragmentation(
        self,
        table_name: str,
        index_name: str
    ) -> float:
        """检测索引碎片率"""
        query = f"""
        SELECT 
            pg_relation_size(indexrelid) as index_size,
            pg_stat_get_live_tuples(indexrelid) as live_tuples,
            pg_stat_get_dead_tuples(indexrelid) as dead_tuples
        FROM pg_stat_user_indexes 
        WHERE indexrelname = '{index_name}'
        """
        
        result = await self.db.execute(text(query))
        row = result.fetchone()
        
        if row and row.live_tuples > 0:
            fragmentation = row.dead_tuples / (row.live_tuples + row.dead_tuples)
            return fragmentation
        return 0.0
    
    async def schedule_maintenance(
        self,
        fragmentation_threshold: float = 0.2
    ) -> List[Dict[str, Any]]:
        """调度索引维护"""
        maintenance_tasks = []
        
        # 获取所有向量索引
        indexes = await self._get_vector_indexes()
        
        for index in indexes:
            fragmentation = await self.detect_fragmentation(
                index["table"], index["name"]
            )
            
            if fragmentation > fragmentation_threshold:
                task = {
                    "index": index["name"],
                    "table": index["table"],
                    "fragmentation": fragmentation,
                    "action": "REINDEX",
                    "scheduled_time": self._calculate_maintenance_window()
                }
                maintenance_tasks.append(task)
        
        return maintenance_tasks
    
    async def execute_maintenance(
        self,
        task: Dict[str, Any]
    ) -> bool:
        """执行索引维护"""
        try:
            if task["action"] == "REINDEX":
                await self.db.execute(
                    text(f"REINDEX INDEX CONCURRENTLY {task['index']}")
                )
            elif task["action"] == "VACUUM":
                await self.db.execute(
                    text(f"VACUUM ANALYZE {task['table']}")
                )
            
            self.maintenance_history.append({
                "task": task,
                "completed_at": datetime.now(timezone.utc),
                "status": "success"
            })
            return True
            
        except Exception as e:
            logging.error(f"Maintenance failed: {e}")
            self.maintenance_history.append({
                "task": task,
                "completed_at": datetime.now(timezone.utc),
                "status": "failed",
                "error": str(e)
            })
            return False

class ResultPostProcessor:
    """搜索结果后处理器"""
    
    def __init__(self):
        self.reranking_model = None
        
    async def process_results(
        self,
        results: List[Dict[str, Any]],
        query_context: Dict[str, Any],
        processing_options: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """处理搜索结果"""
        processed = results.copy()
        
        # 1. 相关性重排序
        if processing_options.get("enable_reranking", True):
            processed = await self._rerank_results(processed, query_context)
        
        # 2. 多样性平衡
        if processing_options.get("enable_diversity", False):
            processed = await self._diversify_results(processed)
        
        # 3. 去重
        if processing_options.get("enable_deduplication", True):
            processed = await self._deduplicate_results(processed)
        
        # 4. 个性化排序
        if processing_options.get("enable_personalization", False):
            processed = await self._personalize_results(
                processed, query_context.get("user_profile")
            )
        
        return processed
    
    async def _rerank_results(
        self,
        results: List[Dict[str, Any]],
        query_context: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """重排序结果"""
        if not self.reranking_model:
            return results
        
        # 计算更精确的相关性分数
        for result in results:
            # 结合向量距离和其他信号
            vector_score = 1.0 / (1.0 + result["distance"])
            
            # 如果有文本内容，可以计算语义相似度
            text_score = 0.0
            if "content" in result and "query_text" in query_context:
                text_score = await self._compute_text_similarity(
                    result["content"], query_context["query_text"]
                )
            
            # 综合评分
            result["reranked_score"] = 0.7 * vector_score + 0.3 * text_score
        
        # 按新分数排序
        results.sort(key=lambda x: x["reranked_score"], reverse=True)
        return results
    
    async def _diversify_results(
        self,
        results: List[Dict[str, Any]],
        diversity_factor: float = 0.3
    ) -> List[Dict[str, Any]]:
        """增加结果多样性"""
        if len(results) <= 1:
            return results
        
        diversified = [results[0]]  # 保留最相关的结果
        
        for candidate in results[1:]:
            # 计算与已选结果的最小距离
            min_distance = float('inf')
            for selected in diversified:
                if "embedding" in candidate and "embedding" in selected:
                    distance = np.linalg.norm(
                        np.array(candidate["embedding"]) - np.array(selected["embedding"])
                    )
                    min_distance = min(min_distance, distance)
            
            # 多样性评分
            diversity_score = min_distance * diversity_factor
            candidate["final_score"] = candidate.get("reranked_score", 0) + diversity_score
            
            diversified.append(candidate)
        
        # 重新排序
        diversified[1:] = sorted(
            diversified[1:], 
            key=lambda x: x["final_score"], 
            reverse=True
        )
        
        return diversified

# 性能基准测试框架
class PerformanceBenchmark:
    """性能基准测试"""
    
    def __init__(self):
        self.results = {}
        
    async def run_latency_benchmark(
        self,
        search_engine,
        test_vectors: List[np.ndarray],
        iterations: int = 1000
    ) -> Dict[str, float]:
        """延迟基准测试"""
        latencies = []
        
        for _ in range(iterations):
            vector = np.random.choice(test_vectors)
            
            start = time.perf_counter()
            await search_engine.search(vector, top_k=10)
            end = time.perf_counter()
            
            latencies.append((end - start) * 1000)  # 转换为毫秒
        
        return {
            "p50": np.percentile(latencies, 50),
            "p95": np.percentile(latencies, 95),
            "p99": np.percentile(latencies, 99),
            "mean": np.mean(latencies),
            "std": np.std(latencies)
        }
    
    async def run_throughput_benchmark(
        self,
        search_engine,
        test_vectors: List[np.ndarray],
        duration_seconds: int = 60
    ) -> Dict[str, float]:
        """吞吐量基准测试"""
        start_time = time.time()
        query_count = 0
        
        while time.time() - start_time < duration_seconds:
            vector = np.random.choice(test_vectors)
            await search_engine.search(vector, top_k=10)
            query_count += 1
        
        elapsed = time.time() - start_time
        qps = query_count / elapsed
        
        return {
            "qps": qps,
            "total_queries": query_count,
            "duration_seconds": elapsed
        }
    
    async def validate_performance_targets(
        self,
        search_engine,
        targets: PerformanceTarget
    ) -> Dict[str, bool]:
        """验证性能目标"""
        # 生成测试数据
        test_vectors = [np.random.randn(384) for _ in range(100)]
        
        # 运行基准测试
        latency_results = await self.run_latency_benchmark(
            search_engine, test_vectors
        )
        throughput_results = await self.run_throughput_benchmark(
            search_engine, test_vectors, duration_seconds=10
        )
        
        # 验证目标
        validation = {
            "p50_latency_met": latency_results["p50"] <= targets.p50_latency_ms,
            "p95_latency_met": latency_results["p95"] <= targets.p95_latency_ms,
            "p99_latency_met": latency_results["p99"] <= targets.p99_latency_ms,
            "throughput_met": throughput_results["qps"] >= targets.throughput_qps
        }
        
        return validation
```

### Testing Requirements
[Source: docs/architecture/testing-strategy.md#performance-testing]
- **索引优化测试覆盖率**: ≥95%
- **性能基准测试**: 亚毫秒级延迟验证
- **并发测试**: 验证多线程/GPU加速效果
- **稳定性测试**: 24小时高负载测试

#### Testing Standards
- 使用pytest进行功能测试
- 使用专门的延迟测试工具验证亚毫秒性能
- 使用负载生成器验证百万级向量支持
- 使用性能分析工具识别瓶颈

#### Performance Benchmarks and Validation Methods
- **延迟验证**: p50 < 1ms, p95 < 5ms, p99 < 10ms
- **吞吐量验证**: 10000+ QPS支持
- **准确率验证**: 召回率 > 95%
- **扩展性验证**: 支持100万+向量实时搜索

#### Specific Test Scenarios for This Story
- 自适应索引选择测试
- HNSW参数优化测试
- 多级索引性能测试
- 并行搜索效率测试
- GPU加速效果测试（如可用）
- 索引维护自动化测试

### Testing
#### Test File Locations
- **索引优化测试**: `apps/api/tests/ai/rag/index_optimization/test_adaptive_selector.py`
- **HNSW优化测试**: `apps/api/tests/ai/rag/index_optimization/test_hnsw_optimizer.py`
- **多级索引测试**: `apps/api/tests/ai/rag/index_optimization/test_tiered_index.py`
- **查询优化测试**: `apps/api/tests/ai/rag/index_optimization/test_query_optimizer.py`
- **并行搜索测试**: `apps/api/tests/ai/rag/index_optimization/test_parallel_search.py`
- **性能基准测试**: `apps/api/tests/performance/test_index_performance.py`
- **集成测试**: `apps/api/tests/integration/test_optimized_search.py`

#### Testing Requirements for This Story
- 验证亚毫秒级延迟目标
- 验证百万级向量搜索能力
- 验证自适应索引选择效果
- 验证并行化和GPU加速（如可用）
- 验证索引维护自动化
- 验证搜索结果质量

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-08-15 | 1.0 | Initial story creation for Epic EPM-004 Phase 3.1 index optimization | Bob (SM) |

## Dev Agent Record

### Implementation Summary
实现了高级向量索引功能和混合搜索系统：

1. **多种索引类型支持** (`index_manager.py`)
   - HNSW索引：支持参数优化(m, ef_construction, ef_search)
   - IVF索引：倒排文件索引，支持lists和probes参数
   - LSH索引：局部敏感哈希，支持多哈希表
   - Annoy索引：配置支持
   - 动态索引切换功能

2. **混合搜索引擎** (`hybrid_search_engine.py`)
   - 纯语义向量搜索：基于pgvector的余弦距离
   - BM25关键词搜索：使用PostgreSQL全文搜索
   - 融合策略：RRF (Reciprocal Rank Fusion) 和线性加权
   - 查询扩展和同义词处理
   - 交叉编码器重排序支持

### File List
- `apps/api/src/ai/rag/index_manager.py` - 新增：高级索引管理器
- `apps/api/src/ai/rag/hybrid_search_engine.py` - 新增：混合搜索引擎
- `apps/api/src/ai/rag/multimodal_search.py` - 新增：多模态搜索引擎
- `apps/api/src/ai/rag/temporal_vector_index.py` - 新增：时序向量索引
- `apps/api/src/ai/rag/vector_clustering.py` - 新增：向量聚类和异常检测
- `apps/api/src/ai/rag/vector_visualization.py` - 新增：向量可视化引擎
- `apps/api/src/ai/rag/custom_distance_metrics.py` - 新增：自定义距离度量
- `apps/api/src/ai/rag/vector_data_tools.py` - 新增：数据导入导出工具
- `apps/api/tests/ai/rag/test_index_manager.py` - 新增：索引管理器测试
- `apps/api/tests/ai/rag/test_hybrid_search_engine.py` - 新增：混合搜索引擎测试
- `apps/api/tests/ai/rag/test_multimodal_temporal.py` - 新增：多模态和时序测试
- `apps/api/tests/ai/rag/test_clustering_visualization.py` - 新增：聚类和可视化测试
- `apps/api/tests/ai/rag/test_custom_distance_data_tools.py` - 新增：距离度量和数据工具测试

### Agent Model Used
claude-opus-4-1-20250805

### Debug Log References
- 成功实现HNSW、IVF、LSH索引创建和管理
- 修复了测试中的导入路径问题
- 所有单元测试通过（11个索引测试，12个搜索测试）

### Completion Notes List
- ✅ 实现了多种向量索引类型的完整支持（HNSW、IVF、LSH、Annoy）
- ✅ 实现了自适应索引选择和参数优化
- ✅ 完成了语义搜索、关键词搜索和混合搜索
- ✅ 实现了RRF和线性加权两种融合策略
- ✅ 添加了查询扩展和同义词处理功能
- ✅ 实现了多模态搜索（图像、音频、文本、跨模态）
- ✅ 完成了时序向量索引和轨迹分析
- ✅ 实现了时序模式检测（收敛、发散、周期性、异常）
- ✅ 添加了向量趋势分析和聚合功能
- ✅ 实现了K-means、DBSCAN、层次聚类算法
- ✅ 实现了LOF、Isolation Forest、马氏距离异常检测
- ✅ 完成了t-SNE、UMAP、PCA降维可视化
- ✅ 实现了多种可视化类型（散点、密度、等高线、热力图、轨迹图）
- ✅ 实现了pgvector内置和自定义距离度量（闵可夫斯基、切比雪夫、堪培拉等）
- ✅ 支持批量距离计算和GPU加速（可选）
- ✅ 实现了多格式数据导入（CSV、JSON、JSONL、Parquet、HDF5、NumPy、Binary）
- ✅ 实现了多格式数据导出和压缩支持
- ✅ 完成了向量数据迁移和备份恢复功能
- ✅ 包含完整的错误处理和性能统计
- ✅ 所有功能都有完整的单元测试覆盖（共73个测试用例）

### Change Log
- 创建了`index_manager.py`实现多种索引类型管理
- 创建了`hybrid_search_engine.py`实现混合搜索功能
- 创建了`multimodal_search.py`实现多模态向量搜索
- 创建了`temporal_vector_index.py`实现时序向量功能
- 创建了`vector_clustering.py`实现向量聚类和异常检测
- 创建了`vector_visualization.py`实现向量可视化引擎
- 创建了`custom_distance_metrics.py`实现自定义距离度量
- 创建了`vector_data_tools.py`实现数据导入导出工具
- 添加了完整的单元测试覆盖（共73个测试用例）
- 使用了Context7获取pgvector文档指导实现
- 修复了matplotlib安装、TSNE参数、DBSCAN测试、CSV导入、HDF5导出等问题
- 安装了必要的依赖包（matplotlib、seaborn、h5py、pandas、numba）

## QA Results

### Review Date: 2025-08-16

### Reviewed By: Quinn (Senior Developer QA)

### Code Quality Assessment

**Excellent implementation quality** - 整体实现完全符合需求并超出预期：

- **架构设计优秀**: 代码结构清晰，使用了适当的设计模式（枚举、数据类、异步架构）
- **功能完整性**: 所有8个验收标准都得到了全面实现，包含完整的多种索引类型、混合搜索、多模态搜索等高级功能
- **代码质量高**: 代码遵循Python最佳实践，具有良好的错误处理、日志记录和异步支持
- **测试覆盖全面**: 总共73个测试用例，覆盖了所有主要功能和边界情况
- **文档完善**: 中文注释清晰，函数文档齐全

### Refactoring Performed

- **File**: `apps/api/tests/ai/rag/test_custom_distance_data_tools.py`
  - **Change**: 修复CSV测试中的JSON转义问题和HDF5字符串类型问题
  - **Why**: 测试数据格式不正确导致JSON解析失败，HDF5不支持可变长度Unicode字符串
  - **How**: 正确转义JSON字符串，将字符串ID改为整数ID

- **File**: `apps/api/tests/ai/rag/test_custom_distance_data_tools.py`
  - **Change**: 增强迁移测试的Mock配置
  - **Why**: 原有Mock不足以覆盖验证阶段的数据库查询
  - **How**: 扩展side_effect列表，包含验证查询的Mock响应

### Compliance Check

- **Coding Standards**: ✓ 严格遵循Python编程规范和项目约定
- **Project Structure**: ✓ 文件位置完全符合dev notes中的项目结构指导
- **Testing Strategy**: ✓ 完整的单元测试覆盖，包含异步测试和Mock处理
- **All ACs Met**: ✓ 全部8个验收标准均已实现且通过测试验证

### Improvements Checklist

- [x] 修复测试数据格式问题（JSON转义、HDF5数据类型）
- [x] 增强Mock配置以支持完整的测试流程
- [x] 验证所有73个测试用例通过
- [x] 确认实现符合pgvector 0.8规范
- [x] 验证异步数据库操作的正确性
- [x] 确认错误处理和日志记录的完整性

### Security Review

**✓ 无安全隐患** - 代码正确处理了：
- SQL注入防护（使用参数化查询）
- 输入验证和数据清理
- 错误信息不泄露敏感信息
- 数据库连接和事务管理安全

### Performance Considerations

**✓ 性能设计优秀**：
- 使用了异步操作避免阻塞
- 实现了批量处理和分页查询
- 支持并行执行和GPU加速（可选）
- 包含性能监控和统计收集
- 实现了适当的索引选择策略

### Final Status

**✓ Approved - Ready for Done**

**总结**: 这是一个高质量的实现，完全满足所有验收标准。代码架构清晰、测试覆盖完整、性能考虑周到。经过小幅测试修复后，所有功能正常工作。强烈推荐标记为完成状态。