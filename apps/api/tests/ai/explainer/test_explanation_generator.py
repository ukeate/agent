"""è§£é‡Šç”Ÿæˆå™¨å•å…ƒæµ‹è¯•"""

import pytest
from unittest.mock import Mock, patch
from datetime import datetime, timezone

from src.ai.explainer.explanation_generator import ExplanationGenerator
from src.ai.explainer.explanation_formatter import ExplanationFormatter, ExplanationExporter
from src.ai.explainer.decision_tracker import DecisionTracker
from src.models.schemas.explanation import (
    ExplanationType,
    ExplanationLevel,
    EvidenceType,
    ConfidenceMetrics,
    ConfidenceSource
)


class TestExplanationGenerator:
    """æµ‹è¯•è§£é‡Šç”Ÿæˆå™¨"""
    
    @pytest.fixture
    def mock_openai_client(self):
        """æ¨¡æ‹ŸOpenAIå®¢æˆ·ç«¯"""
        mock_client = Mock()
        mock_response = Mock()
        mock_response.choices = [Mock()]
        mock_response.choices[0].message.content = "è¿™æ˜¯ä¸€ä¸ªç”Ÿæˆçš„è§£é‡Šã€‚"
        mock_client.chat_completion.return_value = mock_response
        return mock_client
    
    @pytest.fixture
    def generator(self, mock_openai_client):
        """åˆ›å»ºæµ‹è¯•ç”¨çš„è§£é‡Šç”Ÿæˆå™¨"""
        return ExplanationGenerator(mock_openai_client)
    
    @pytest.fixture
    def sample_decision_tracker(self):
        """åˆ›å»ºæ ·æœ¬å†³ç­–è·Ÿè¸ªå™¨"""
        tracker = DecisionTracker("test_decision_001", "æµ‹è¯•å†³ç­–ä¸Šä¸‹æ–‡")
        
        # æ·»åŠ ä¸€äº›å†³ç­–æ•°æ®
        tracker.create_node("start", "å¼€å§‹å†³ç­–", {"input": "test"})
        tracker.add_confidence_factor(
            factor_name="user_age",
            factor_value=25,
            weight=0.8,
            impact=0.7,
            source="user_profile"
        )
        tracker.add_confidence_factor(
            factor_name="credit_score",
            factor_value=750,
            weight=0.9,
            impact=0.8,
            source="credit_bureau"
        )
        tracker.finalize_decision("approved", 0.85, "ç”¨æˆ·ç¬¦åˆæ¡ä»¶")
        
        return tracker
    
    def test_create_explanation_generator(self, generator):
        """æµ‹è¯•åˆ›å»ºè§£é‡Šç”Ÿæˆå™¨"""
        assert generator is not None
        assert generator.openai_client is not None
        assert generator.template_manager is not None
        assert generator.confidence_calculator is not None
        assert "technical" in generator.style_templates
        assert "business" in generator.style_templates
        assert "user_friendly" in generator.style_templates
    
    def test_generate_explanation_basic(self, generator, sample_decision_tracker):
        """æµ‹è¯•åŸºæœ¬è§£é‡Šç”Ÿæˆ"""
        explanation = generator.generate_explanation(
            sample_decision_tracker,
            ExplanationType.DECISION,
            ExplanationLevel.SUMMARY
        )
        
        assert explanation is not None
        assert explanation.decision_id == "test_decision_001"
        assert explanation.explanation_type == ExplanationType.DECISION
        assert explanation.explanation_level == ExplanationLevel.SUMMARY
        assert explanation.decision_outcome == "approved"
        assert explanation.summary_explanation is not None
        assert len(explanation.components) > 0
        assert explanation.confidence_metrics is not None
    
    def test_generate_explanation_detailed(self, generator, sample_decision_tracker):
        """æµ‹è¯•è¯¦ç»†è§£é‡Šç”Ÿæˆ"""
        explanation = generator.generate_explanation(
            sample_decision_tracker,
            ExplanationType.DECISION,
            ExplanationLevel.DETAILED,
            style="technical"
        )
        
        assert explanation.summary_explanation is not None
        assert explanation.detailed_explanation is not None
        assert explanation.metadata["generation_style"] == "technical"
    
    def test_generate_explanation_technical(self, generator, sample_decision_tracker):
        """æµ‹è¯•æŠ€æœ¯è§£é‡Šç”Ÿæˆ"""
        explanation = generator.generate_explanation(
            sample_decision_tracker,
            ExplanationType.DECISION,
            ExplanationLevel.TECHNICAL,
            style="technical"
        )
        
        assert explanation.summary_explanation is not None
        assert explanation.detailed_explanation is not None
        assert explanation.technical_explanation is not None
    
    def test_generate_explanation_with_custom_context(self, generator, sample_decision_tracker):
        """æµ‹è¯•å¸¦è‡ªå®šä¹‰ä¸Šä¸‹æ–‡çš„è§£é‡Šç”Ÿæˆ"""
        custom_context = {
            "temporal_distance_days": 30,
            "context_similarity": 0.8,
            "domain": "financial_services"
        }
        
        explanation = generator.generate_explanation(
            sample_decision_tracker,
            ExplanationType.DECISION,
            ExplanationLevel.SUMMARY,
            custom_context=custom_context
        )
        
        assert explanation is not None
        assert explanation.metadata is not None
    
    def test_generate_explanation_components(self, generator, sample_decision_tracker):
        """æµ‹è¯•è§£é‡Šç»„ä»¶ç”Ÿæˆ"""
        explanation = generator.generate_explanation(
            sample_decision_tracker,
            ExplanationType.DECISION,
            ExplanationLevel.SUMMARY
        )
        
        assert len(explanation.components) == 2  # user_age and credit_score
        
        age_component = next(
            (comp for comp in explanation.components if comp.factor_name == "user_age"),
            None
        )
        assert age_component is not None
        assert age_component.factor_value == 25
        assert age_component.weight == 0.8
        assert age_component.impact_score == 0.7
    
    def test_generate_counterfactual_scenarios(self, generator, sample_decision_tracker):
        """æµ‹è¯•åäº‹å®åœºæ™¯ç”Ÿæˆ"""
        explanation = generator.generate_explanation(
            sample_decision_tracker,
            ExplanationType.DECISION,
            ExplanationLevel.DETAILED
        )
        
        assert len(explanation.counterfactuals) > 0
        
        scenario = explanation.counterfactuals[0]
        assert scenario.scenario_name is not None
        assert scenario.predicted_outcome is not None
        assert scenario.impact_difference is not None
    
    def test_generate_visualization_data(self, generator, sample_decision_tracker):
        """æµ‹è¯•å¯è§†åŒ–æ•°æ®ç”Ÿæˆ"""
        explanation = generator.generate_explanation(
            sample_decision_tracker,
            ExplanationType.DECISION,
            ExplanationLevel.SUMMARY
        )
        
        viz_data = explanation.visualization_data
        assert viz_data is not None
        assert "factor_importance" in viz_data
        assert "confidence_breakdown" in viz_data
        assert "decision_path" in viz_data
        
        # æ£€æŸ¥å› ç´ é‡è¦æ€§å›¾è¡¨
        factor_chart = viz_data["factor_importance"]
        assert factor_chart["chart_type"] == "bar"
        assert len(factor_chart["data"]) > 0
    
    def test_different_explanation_styles(self, generator, sample_decision_tracker):
        """æµ‹è¯•ä¸åŒè§£é‡Šé£æ ¼"""
        styles = ["technical", "business", "user_friendly", "regulatory"]
        
        for style in styles:
            explanation = generator.generate_explanation(
                sample_decision_tracker,
                ExplanationType.DECISION,
                ExplanationLevel.SUMMARY,
                style=style
            )
            
            assert explanation is not None
            assert explanation.metadata["generation_style"] == style
    
    def test_calculate_explanation_confidence(self, generator, sample_decision_tracker):
        """æµ‹è¯•è§£é‡Šç½®ä¿¡åº¦è®¡ç®—"""
        confidence_metrics = generator._calculate_explanation_confidence(
            sample_decision_tracker,
            None
        )
        
        assert isinstance(confidence_metrics, ConfidenceMetrics)
        assert 0.0 <= confidence_metrics.overall_confidence <= 1.0
        assert 0.0 <= confidence_metrics.uncertainty_score <= 1.0
        assert len(confidence_metrics.confidence_sources) > 0
    
    def test_build_explanation_prompt(self, generator, sample_decision_tracker):
        """æµ‹è¯•è§£é‡Šæç¤ºæ„å»º"""
        context_data = generator._prepare_explanation_context(
            sample_decision_tracker,
            ConfidenceMetrics(overall_confidence=0.8, confidence_sources=[]),
            None
        )
        
        prompt = generator._build_explanation_prompt(
            ExplanationLevel.SUMMARY,
            ExplanationType.DECISION,
            "user_friendly",
            context_data
        )
        
        assert "å†³ç­–ID" in prompt
        assert "test_decision_001" in prompt
        assert "approved" in prompt
        assert "user_age" in prompt
        assert "credit_score" in prompt
    
    def test_openai_api_failure_fallback(self, sample_decision_tracker):
        """æµ‹è¯•OpenAI APIå¤±è´¥æ—¶çš„é™çº§å¤„ç†"""
        # åˆ›å»ºä¸€ä¸ªä¼šå¤±è´¥çš„mockå®¢æˆ·ç«¯
        mock_client = Mock()
        mock_client.chat_completion.side_effect = Exception("API Error")
        
        generator = ExplanationGenerator(mock_client)
        
        explanation = generator.generate_explanation(
            sample_decision_tracker,
            ExplanationType.DECISION,
            ExplanationLevel.SUMMARY
        )
        
        assert explanation is not None
        assert "è§£é‡Šç”Ÿæˆå¤±è´¥" in explanation.summary_explanation
    
    def test_format_key_factors(self, generator):
        """æµ‹è¯•å…³é”®å› ç´ æ ¼å¼åŒ–"""
        factors = [
            {"factor_name": "age", "factor_value": 25, "weight": 0.8, "impact": 0.7},
            {"factor_name": "income", "factor_value": 50000, "weight": 0.6, "impact": 0.5}
        ]
        
        formatted = generator._format_key_factors(factors)
        
        assert "1. age: 25" in formatted
        assert "2. income: 50000" in formatted
        assert "æƒé‡: 0.80" in formatted
        assert "å½±å“: 0.70" in formatted
    
    def test_determine_evidence_type(self, generator):
        """æµ‹è¯•è¯æ®ç±»å‹ç¡®å®š"""
        from src.models.schemas.explanation import EvidenceType
        
        test_cases = [
            ({"source": "user_input"}, EvidenceType.INPUT_DATA),
            ({"source": "context_data"}, EvidenceType.CONTEXT),
            ({"source": "memory_store"}, EvidenceType.MEMORY),
            ({"source": "reasoning_step"}, EvidenceType.REASONING_STEP),
            ({"source": "external_api"}, EvidenceType.EXTERNAL_SOURCE),
            ({"source": "domain_expert"}, EvidenceType.DOMAIN_KNOWLEDGE)
        ]
        
        for factor, expected_type in test_cases:
            result = generator._determine_evidence_type(factor)
            assert result == expected_type
    
    def test_generate_alternative_value(self, generator):
        """æµ‹è¯•æ›¿ä»£å€¼ç”Ÿæˆ"""
        test_cases = [
            {"factor_value": 100},  # æ•°å€¼
            {"factor_value": True},  # å¸ƒå°”å€¼
            {"factor_value": "test"},  # å­—ç¬¦ä¸²
            {"factor_value": [1, 2, 3]}  # å…¶ä»–ç±»å‹
        ]
        
        for factor in test_cases:
            alternative = generator._generate_alternative_value(factor)
            assert alternative != factor["factor_value"]
    
    def test_fallback_explanation_generation(self, generator, sample_decision_tracker):
        """æµ‹è¯•é™çº§è§£é‡Šç”Ÿæˆ"""
        from uuid import uuid4
        explanation_id = uuid4()
        error_message = "Test error"
        
        fallback = generator._generate_fallback_explanation(
            explanation_id,
            sample_decision_tracker,
            ExplanationType.DECISION,
            ExplanationLevel.SUMMARY,
            error_message
        )
        
        assert fallback.id == explanation_id
        assert fallback.metadata["generation_error"] == error_message
        assert fallback.metadata["fallback_mode"] is True


class TestExplanationFormatter:
    """æµ‹è¯•è§£é‡Šæ ¼å¼åŒ–å™¨"""
    
    @pytest.fixture
    def formatter(self):
        """åˆ›å»ºæµ‹è¯•ç”¨çš„æ ¼å¼åŒ–å™¨"""
        return ExplanationFormatter()
    
    @pytest.fixture
    def sample_explanation(self):
        """åˆ›å»ºæ ·æœ¬è§£é‡Š"""
        from src.models.schemas.explanation import (
            DecisionExplanation,
            ExplanationComponent,
            ConfidenceMetrics,
            CounterfactualScenario
        )
        
        return DecisionExplanation(
            decision_id="test_001",
            explanation_type=ExplanationType.DECISION,
            explanation_level=ExplanationLevel.SUMMARY,
            decision_description="æµ‹è¯•å†³ç­–",
            decision_outcome="approved",
            summary_explanation="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•è§£é‡Šã€‚",
            components=[
                ExplanationComponent(
                    factor_name="test_factor",
                    factor_value="test_value",
                    weight=0.8,
                    impact_score=0.7,
                    evidence_type=EvidenceType.INPUT_DATA,
                    evidence_source="test_source",
                    evidence_content="æµ‹è¯•å› ç´ è¯´æ˜"
                )
            ],
            confidence_metrics=ConfidenceMetrics(
                overall_confidence=0.85,
                uncertainty_score=0.15,
                confidence_sources=[ConfidenceSource.MODEL_PROBABILITY]
            ),
            counterfactuals=[
                CounterfactualScenario(
                    scenario_name="å¦‚æœæ¡ä»¶ä¸åŒ",
                    changed_factors={"test_factor": "alternative_value"},
                    predicted_outcome="å¯èƒ½çš„ä¸åŒç»“æœ",
                    probability=0.8,
                    impact_difference=-0.1,
                    explanation="å½±å“åˆ†æ"
                )
            ]
        )
    
    def test_create_formatter(self, formatter):
        """æµ‹è¯•åˆ›å»ºæ ¼å¼åŒ–å™¨"""
        assert formatter is not None
        assert formatter.html_templates is not None
        assert formatter.markdown_templates is not None
        assert "html" in formatter.format_config
        assert "markdown" in formatter.format_config
        assert "json" in formatter.format_config
    
    def test_format_to_html(self, formatter, sample_explanation):
        """æµ‹è¯•HTMLæ ¼å¼åŒ–"""
        html_output = formatter.format_explanation(sample_explanation, "html")
        
        assert html_output is not None
        assert "<!DOCTYPE html>" in html_output
        assert "test_001" in html_output
        assert "approved" in html_output
        assert "85.0%" in html_output
        assert "test_factor" in html_output
    
    def test_format_to_markdown(self, formatter, sample_explanation):
        """æµ‹è¯•Markdownæ ¼å¼åŒ–"""
        markdown_output = formatter.format_explanation(sample_explanation, "markdown")
        
        assert markdown_output is not None
        assert "# " in markdown_output  # æ ‡é¢˜
        assert "test_001" in markdown_output
        assert "approved" in markdown_output
        assert "|" in markdown_output  # è¡¨æ ¼
    
    def test_format_to_json(self, formatter, sample_explanation):
        """æµ‹è¯•JSONæ ¼å¼åŒ–"""
        json_output = formatter.format_explanation(sample_explanation, "json")
        
        assert json_output is not None
        assert "test_001" in json_output
        assert "approved" in json_output
        assert "_formatted" in json_output  # åŒ…å«æ ¼å¼åŒ–å…ƒæ•°æ®
        
        # éªŒè¯JSONæœ‰æ•ˆæ€§
        import json
        parsed = json.loads(json_output)
        assert parsed["decision_id"] == "test_001"
    
    def test_format_to_text(self, formatter, sample_explanation):
        """æµ‹è¯•çº¯æ–‡æœ¬æ ¼å¼åŒ–"""
        text_output = formatter.format_explanation(sample_explanation, "text")
        
        assert text_output is not None
        assert "=" in text_output  # åˆ†éš”çº¿
        assert "test_001" in text_output
        assert "approved" in text_output
        assert "ğŸ“‹ åŸºæœ¬ä¿¡æ¯" in text_output
        assert "ğŸ” å…³é”®å› ç´ " in text_output
    
    def test_format_to_xml(self, formatter, sample_explanation):
        """æµ‹è¯•XMLæ ¼å¼åŒ–"""
        xml_output = formatter.format_explanation(sample_explanation, "xml")
        
        assert xml_output is not None
        assert "<?xml version" in xml_output
        assert "<decision_explanation>" in xml_output
        assert "<id>" in xml_output
        assert "test_001" in xml_output
        assert "</decision_explanation>" in xml_output
    
    def test_format_with_custom_config(self, formatter, sample_explanation):
        """æµ‹è¯•è‡ªå®šä¹‰é…ç½®æ ¼å¼åŒ–"""
        custom_config = {
            "include_styles": False,
            "pretty_print": False
        }
        
        html_output = formatter.format_explanation(
            sample_explanation, "html", custom_config=custom_config
        )
        
        assert html_output is not None
        # ä¸åº”åŒ…å«å†…è”æ ·å¼
        assert "<style>" not in html_output
    
    def test_format_with_different_templates(self, formatter, sample_explanation):
        """æµ‹è¯•ä¸åŒæ¨¡æ¿æ ¼å¼åŒ–"""
        templates = ["default", "minimal", "dashboard"]
        
        for template in templates:
            try:
                html_output = formatter.format_explanation(
                    sample_explanation, "html", template_name=template
                )
                assert html_output is not None
            except Exception as e:
                # æŸäº›æ¨¡æ¿å¯èƒ½ä¸å­˜åœ¨ï¼Œè¿™æ˜¯é¢„æœŸçš„
                pass
    
    def test_get_decision_emoji(self, formatter):
        """æµ‹è¯•å†³ç­–è¡¨æƒ…ç¬¦å·"""
        test_cases = [
            ("approved", "âœ…"),
            ("é€šè¿‡", "âœ…"),
            ("rejected", "âŒ"),
            ("æ‹’ç»", "âŒ"),
            ("pending", "â³"),
            ("å¾…å®š", "â³"),
            ("other", "ğŸ“‹")
        ]
        
        for outcome, expected_emoji in test_cases:
            emoji = formatter._get_decision_emoji(outcome)
            assert emoji == expected_emoji
    
    def test_get_confidence_emoji(self, formatter):
        """æµ‹è¯•ç½®ä¿¡åº¦è¡¨æƒ…ç¬¦å·"""
        test_cases = [
            (0.9, "ğŸŸ¢"),
            (0.7, "ğŸŸ¡"),
            (0.5, "ğŸŸ "),
            (0.3, "ğŸ”´")
        ]
        
        for confidence, expected_emoji in test_cases:
            emoji = formatter._get_confidence_emoji(confidence)
            assert emoji == expected_emoji
    
    def test_create_components_table(self, formatter, sample_explanation):
        """æµ‹è¯•ç»„ä»¶è¡¨æ ¼åˆ›å»º"""
        table = formatter._create_components_table(sample_explanation.components)
        
        assert table is not None
        assert "|" in table  # Markdownè¡¨æ ¼
        assert "test_factor" in table
        assert "0.80" in table  # æƒé‡
        assert "0.70" in table  # å½±å“
    
    def test_wrap_text(self, formatter):
        """æµ‹è¯•æ–‡æœ¬æ¢è¡Œ"""
        # ä½¿ç”¨è‹±æ–‡æµ‹è¯•ï¼Œå› ä¸ºä¸­æ–‡æ¢è¡Œé€»è¾‘æ›´å¤æ‚
        long_text = "This is a very long text that needs to be wrapped for proper display within specified width limits."
        wrapped = formatter._wrap_text(long_text, 20)
        
        assert len(wrapped) > 1  # åº”è¯¥è¢«åˆ†æˆå¤šè¡Œ
        for line in wrapped:
            assert len(line) <= 20
    
    def test_unsupported_format(self, formatter, sample_explanation):
        """æµ‹è¯•ä¸æ”¯æŒçš„æ ¼å¼"""
        with pytest.raises(ValueError):
            formatter.format_explanation(sample_explanation, "unsupported_format")


class TestExplanationExporter:
    """æµ‹è¯•è§£é‡Šå¯¼å‡ºå™¨"""
    
    @pytest.fixture
    def exporter(self):
        """åˆ›å»ºæµ‹è¯•ç”¨çš„å¯¼å‡ºå™¨"""
        return ExplanationExporter()
    
    @pytest.fixture
    def sample_explanation(self):
        """åˆ›å»ºæ ·æœ¬è§£é‡Š"""
        from src.models.schemas.explanation import (
            DecisionExplanation,
            ConfidenceMetrics,
            ConfidenceSource
        )
        
        return DecisionExplanation(
            decision_id="export_test_001",
            explanation_type=ExplanationType.DECISION,
            explanation_level=ExplanationLevel.SUMMARY,
            decision_description="å¯¼å‡ºæµ‹è¯•",
            decision_outcome="approved",
            summary_explanation="è¿™æ˜¯ä¸€ä¸ªå¯¼å‡ºæµ‹è¯•è§£é‡Šã€‚",
            components=[],
            confidence_metrics=ConfidenceMetrics(
                overall_confidence=0.8,
                uncertainty_score=0.2,
                confidence_sources=[ConfidenceSource.MODEL_PROBABILITY]
            ),
            counterfactuals=[]
        )
    
    def test_create_exporter(self, exporter):
        """æµ‹è¯•åˆ›å»ºå¯¼å‡ºå™¨"""
        assert exporter is not None
        assert exporter.formatter is not None
    
    @patch('builtins.open')
    def test_export_to_file_html(self, mock_open, exporter, sample_explanation):
        """æµ‹è¯•å¯¼å‡ºHTMLæ–‡ä»¶"""
        mock_file = Mock()
        mock_open.return_value.__enter__.return_value = mock_file
        
        success = exporter.export_to_file(
            sample_explanation,
            "/tmp/test.html",
            "html"
        )
        
        assert success is True
        mock_open.assert_called_once_with("/tmp/test.html", 'w', encoding='utf-8')
        mock_file.write.assert_called_once()
    
    @patch('builtins.open')
    def test_export_to_file_auto_detect(self, mock_open, exporter, sample_explanation):
        """æµ‹è¯•è‡ªåŠ¨æ£€æµ‹æ ¼å¼å¯¼å‡º"""
        mock_file = Mock()
        mock_open.return_value.__enter__.return_value = mock_file
        
        # æµ‹è¯•ä¸åŒæ‰©å±•åçš„è‡ªåŠ¨æ£€æµ‹
        test_cases = [
            ("/tmp/test.html", "html"),
            ("/tmp/test.md", "markdown"),
            ("/tmp/test.json", "json"),
            ("/tmp/test.xml", "xml"),
            ("/tmp/test.txt", "text")
        ]
        
        for file_path, expected_format in test_cases:
            mock_open.reset_mock()
            mock_file.reset_mock()
            
            success = exporter.export_to_file(sample_explanation, file_path)
            assert success is True
            mock_file.write.assert_called_once()
    
    @patch('builtins.open')
    def test_export_error_handling(self, mock_open, exporter, sample_explanation):
        """æµ‹è¯•å¯¼å‡ºé”™è¯¯å¤„ç†"""
        mock_open.side_effect = IOError("æ–‡ä»¶å†™å…¥å¤±è´¥")
        
        success = exporter.export_to_file(
            sample_explanation,
            "/tmp/test.html"
        )
        
        assert success is False
    
    @patch.object(ExplanationExporter, 'export_to_file')
    def test_batch_export(self, mock_export, exporter, sample_explanation):
        """æµ‹è¯•æ‰¹é‡å¯¼å‡º"""
        mock_export.return_value = True
        
        explanations = [sample_explanation, sample_explanation]
        results = exporter.batch_export(
            explanations,
            "/tmp/export",
            formats=['html', 'json']
        )
        
        # åº”è¯¥è°ƒç”¨4æ¬¡å¯¼å‡º (2ä¸ªè§£é‡Š Ã— 2ç§æ ¼å¼)
        assert mock_export.call_count == 4
        # ä½†ç»“æœå­—å…¸ä¸­ä¼šåˆå¹¶ç›¸åŒçš„keyï¼Œæ‰€ä»¥åªæœ‰2ä¸ªå”¯ä¸€çš„æ–‡ä»¶è·¯å¾„
        assert len(results) == 2
        
        # æ£€æŸ¥æ–‡ä»¶è·¯å¾„
        expected_paths = [
            "/tmp/export/export_test_001.html",
            "/tmp/export/export_test_001.json"
        ]
        
        for path in expected_paths:
            assert path in results
            assert results[path] is True


if __name__ == "__main__":
    pytest.main([__file__])