"""
pgvector性能监控模块
收集和分析向量数据库的性能指标
"""

import logging
import asyncio
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
import asyncpg
import json
from dataclasses import dataclass, asdict
import time

from ...core.config import get_settings

logger = logging.getLogger(__name__)
settings = get_settings()


@dataclass
class VectorQueryMetrics:
    """向量查询性能指标"""
    query_id: str
    collection_name: str
    query_type: str  # similarity_search, hybrid_search, etc.
    query_vector_dimension: int
    result_count: int
    execution_time_ms: float
    index_scan_time_ms: Optional[float]
    distance_metric: str
    filters_applied: bool
    cache_hit: bool
    timestamp: datetime


@dataclass
class VectorIndexMetrics:
    """向量索引性能指标"""
    collection_name: str
    index_name: str
    index_type: str  # hnsw, ivfflat
    index_size_bytes: int
    tuples_total: int
    index_scans: int
    tuples_read: int
    tuples_fetched: int
    build_time_ms: Optional[float]
    last_vacuum_time: Optional[datetime]
    fragmentation_ratio: float
    timestamp: datetime


@dataclass
class VectorSystemMetrics:
    """向量系统整体指标"""
    total_collections: int
    total_vectors: int
    total_storage_size_bytes: int
    active_connections: int
    queries_per_second: float
    average_query_time_ms: float
    cache_hit_ratio: float
    error_rate: float
    timestamp: datetime


class VectorMetricsCollector:
    """向量指标收集器"""
    
    def __init__(self, connection_url: str = None):
        self.connection_url = connection_url or settings.DATABASE_URL
        self.pool: Optional[asyncpg.Pool] = None
        self.metrics_buffer: List[Dict[str, Any]] = []
        self.is_running = False
        
    async def initialize(self):
        """初始化数据库连接和监控表"""
        self.pool = await asyncpg.create_pool(
            self.connection_url,
            min_size=1,
            max_size=3,
            command_timeout=30
        )
        
        async with self.pool.acquire() as conn:
            # 创建指标存储表
            await self._create_metrics_tables(conn)
            
        logger.info("向量性能监控初始化完成")
        
    async def _create_metrics_tables(self, conn: asyncpg.Connection):
        """创建监控相关的数据表"""
        
        # 查询指标表
        await conn.execute("""
        CREATE TABLE IF NOT EXISTS vector_query_metrics (
            id SERIAL PRIMARY KEY,
            query_id VARCHAR(255) NOT NULL,
            collection_name VARCHAR(255) NOT NULL,
            query_type VARCHAR(100) NOT NULL,
            query_vector_dimension INTEGER,
            result_count INTEGER,
            execution_time_ms REAL NOT NULL,
            index_scan_time_ms REAL,
            distance_metric VARCHAR(50),
            filters_applied BOOLEAN DEFAULT FALSE,
            cache_hit BOOLEAN DEFAULT FALSE,
            timestamp TIMESTAMP DEFAULT NOW(),
            created_at TIMESTAMP DEFAULT NOW()
        )
        """)
        
        # 索引指标表
        await conn.execute("""
        CREATE TABLE IF NOT EXISTS vector_index_metrics (
            id SERIAL PRIMARY KEY,
            collection_name VARCHAR(255) NOT NULL,
            index_name VARCHAR(255) NOT NULL,
            index_type VARCHAR(50) NOT NULL,
            index_size_bytes BIGINT,
            tuples_total BIGINT,
            index_scans BIGINT,
            tuples_read BIGINT,
            tuples_fetched BIGINT,
            build_time_ms REAL,
            last_vacuum_time TIMESTAMP,
            fragmentation_ratio REAL,
            timestamp TIMESTAMP DEFAULT NOW(),
            created_at TIMESTAMP DEFAULT NOW(),
            UNIQUE(collection_name, index_name, timestamp)
        )
        """)
        
        # 系统指标表
        await conn.execute("""
        CREATE TABLE IF NOT EXISTS vector_system_metrics (
            id SERIAL PRIMARY KEY,
            total_collections INTEGER,
            total_vectors BIGINT,
            total_storage_size_bytes BIGINT,
            active_connections INTEGER,
            queries_per_second REAL,
            average_query_time_ms REAL,
            cache_hit_ratio REAL,
            error_rate REAL,
            timestamp TIMESTAMP DEFAULT NOW(),
            created_at TIMESTAMP DEFAULT NOW()
        )
        """)
        
        # 创建时间序列索引
        await conn.execute("CREATE INDEX IF NOT EXISTS idx_query_metrics_timestamp ON vector_query_metrics(timestamp)")
        await conn.execute("CREATE INDEX IF NOT EXISTS idx_index_metrics_timestamp ON vector_index_metrics(timestamp)")
        await conn.execute("CREATE INDEX IF NOT EXISTS idx_system_metrics_timestamp ON vector_system_metrics(timestamp)")
        await conn.execute("CREATE INDEX IF NOT EXISTS idx_query_metrics_collection ON vector_query_metrics(collection_name)")
        
    async def record_query_metrics(self, metrics: VectorQueryMetrics):
        """记录查询性能指标"""
        if not self.pool:
            await self.initialize()
            
        try:
            async with self.pool.acquire() as conn:
                await conn.execute("""
                INSERT INTO vector_query_metrics (
                    query_id, collection_name, query_type, query_vector_dimension,
                    result_count, execution_time_ms, index_scan_time_ms, 
                    distance_metric, filters_applied, cache_hit, timestamp
                ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11)
                """, 
                metrics.query_id, metrics.collection_name, metrics.query_type,
                metrics.query_vector_dimension, metrics.result_count,
                metrics.execution_time_ms, metrics.index_scan_time_ms,
                metrics.distance_metric, metrics.filters_applied,
                metrics.cache_hit, metrics.timestamp
                )
                
        except Exception as e:
            logger.error(f"记录查询指标失败: {e}")
            
    async def collect_index_metrics(self) -> List[VectorIndexMetrics]:
        """收集索引性能指标"""
        if not self.pool:
            await self.initialize()
            
        metrics = []
        
        try:
            async with self.pool.acquire() as conn:
                # 获取向量索引统计信息
                index_stats = await conn.fetch("""
                SELECT 
                    t.tablename as collection_name,
                    i.indexname as index_name,
                    CASE 
                        WHEN i.indexdef LIKE '%hnsw%' THEN 'hnsw'
                        WHEN i.indexdef LIKE '%ivfflat%' THEN 'ivfflat'
                        ELSE 'other'
                    END as index_type,
                    pg_relation_size(i.indexname::regclass) as index_size_bytes,
                    s.n_tup_ins + s.n_tup_upd + s.n_tup_del as tuples_total,
                    COALESCE(si.idx_scan, 0) as index_scans,
                    COALESCE(si.idx_tup_read, 0) as tuples_read,
                    COALESCE(si.idx_tup_fetch, 0) as tuples_fetched,
                    s.last_vacuum,
                    CASE 
                        WHEN s.n_live_tup > 0 
                        THEN s.n_dead_tup::float / s.n_live_tup::float
                        ELSE 0.0
                    END as fragmentation_ratio
                FROM pg_indexes i
                JOIN pg_stat_user_tables s ON s.tablename = i.tablename
                LEFT JOIN pg_stat_user_indexes si ON si.indexrelname = i.indexname
                WHERE i.schemaname = 'public'
                AND (i.indexdef LIKE '%vector%' OR i.indexdef LIKE '%hnsw%' OR i.indexdef LIKE '%ivfflat%')
                """)
                
                timestamp = datetime.now()
                
                for row in index_stats:
                    metrics.append(VectorIndexMetrics(
                        collection_name=row["collection_name"],
                        index_name=row["index_name"],
                        index_type=row["index_type"],
                        index_size_bytes=row["index_size_bytes"] or 0,
                        tuples_total=row["tuples_total"] or 0,
                        index_scans=row["index_scans"],
                        tuples_read=row["tuples_read"],
                        tuples_fetched=row["tuples_fetched"],
                        build_time_ms=None,  # 构建时间需要单独跟踪
                        last_vacuum_time=row["last_vacuum"],
                        fragmentation_ratio=row["fragmentation_ratio"],
                        timestamp=timestamp
                    ))
                    
                # 保存指标到数据库
                for metric in metrics:
                    await conn.execute("""
                    INSERT INTO vector_index_metrics (
                        collection_name, index_name, index_type, index_size_bytes,
                        tuples_total, index_scans, tuples_read, tuples_fetched,
                        last_vacuum_time, fragmentation_ratio, timestamp
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11)
                    ON CONFLICT (collection_name, index_name, timestamp) DO NOTHING
                    """,
                    metric.collection_name, metric.index_name, metric.index_type,
                    metric.index_size_bytes, metric.tuples_total, metric.index_scans,
                    metric.tuples_read, metric.tuples_fetched, metric.last_vacuum_time,
                    metric.fragmentation_ratio, metric.timestamp
                    )
                    
        except Exception as e:
            logger.error(f"收集索引指标失败: {e}")
            
        return metrics
        
    async def collect_system_metrics(self) -> VectorSystemMetrics:
        """收集系统整体指标"""
        if not self.pool:
            await self.initialize()
            
        try:
            async with self.pool.acquire() as conn:
                # 获取系统统计信息
                system_stats = await conn.fetchrow("""
                WITH collection_stats AS (
                    SELECT 
                        COUNT(DISTINCT t.tablename) as total_collections,
                        COALESCE(SUM(s.n_tup_ins + s.n_tup_upd), 0) as total_vectors,
                        COALESCE(SUM(pg_total_relation_size(t.tablename::regclass)), 0) as total_storage_size
                    FROM information_schema.tables t
                    LEFT JOIN pg_stat_user_tables s ON s.tablename = t.table_name
                    WHERE t.table_schema = 'public'
                    AND EXISTS (
                        SELECT 1 FROM information_schema.columns c 
                        WHERE c.table_name = t.table_name 
                        AND c.data_type LIKE '%vector%'
                    )
                ),
                connection_stats AS (
                    SELECT COUNT(*) as active_connections
                    FROM pg_stat_activity
                    WHERE state = 'active'
                    AND application_name LIKE '%vector%'
                ),
                query_stats AS (
                    SELECT 
                        COUNT(*)::float / EXTRACT(EPOCH FROM (MAX(timestamp) - MIN(timestamp) + INTERVAL '1 second')) as qps,
                        AVG(execution_time_ms) as avg_query_time,
                        (COUNT(*) FILTER (WHERE cache_hit = true))::float / NULLIF(COUNT(*), 0) as cache_hit_ratio
                    FROM vector_query_metrics
                    WHERE timestamp >= NOW() - INTERVAL '5 minutes'
                )
                SELECT 
                    cs.total_collections,
                    cs.total_vectors,
                    cs.total_storage_size,
                    cons.active_connections,
                    COALESCE(qs.qps, 0.0) as queries_per_second,
                    COALESCE(qs.avg_query_time, 0.0) as average_query_time_ms,
                    COALESCE(qs.cache_hit_ratio, 0.0) as cache_hit_ratio
                FROM collection_stats cs
                CROSS JOIN connection_stats cons
                CROSS JOIN query_stats qs
                """)
                
                timestamp = datetime.now()
                
                metrics = VectorSystemMetrics(
                    total_collections=system_stats["total_collections"] or 0,
                    total_vectors=system_stats["total_vectors"] or 0,
                    total_storage_size_bytes=system_stats["total_storage_size"] or 0,
                    active_connections=system_stats["active_connections"] or 0,
                    queries_per_second=float(system_stats["queries_per_second"] or 0.0),
                    average_query_time_ms=float(system_stats["average_query_time_ms"] or 0.0),
                    cache_hit_ratio=float(system_stats["cache_hit_ratio"] or 0.0),
                    error_rate=0.0,  # 错误率需要单独跟踪
                    timestamp=timestamp
                )
                
                # 保存系统指标
                await conn.execute("""
                INSERT INTO vector_system_metrics (
                    total_collections, total_vectors, total_storage_size_bytes,
                    active_connections, queries_per_second, average_query_time_ms,
                    cache_hit_ratio, error_rate, timestamp
                ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
                """,
                metrics.total_collections, metrics.total_vectors,
                metrics.total_storage_size_bytes, metrics.active_connections,
                metrics.queries_per_second, metrics.average_query_time_ms,
                metrics.cache_hit_ratio, metrics.error_rate, metrics.timestamp
                )
                
                return metrics
                
        except Exception as e:
            logger.error(f"收集系统指标失败: {e}")
            return VectorSystemMetrics(
                total_collections=0, total_vectors=0, total_storage_size_bytes=0,
                active_connections=0, queries_per_second=0.0, average_query_time_ms=0.0,
                cache_hit_ratio=0.0, error_rate=0.0, timestamp=datetime.now()
            )
            
    async def get_performance_report(
        self, 
        collection_name: Optional[str] = None,
        time_range_hours: int = 24
    ) -> Dict[str, Any]:
        """生成性能报告"""
        if not self.pool:
            await self.initialize()
            
        try:
            async with self.pool.acquire() as conn:
                since_time = datetime.now() - timedelta(hours=time_range_hours)
                
                # 查询性能统计
                query_where = "WHERE timestamp >= $1"
                params = [since_time]
                
                if collection_name:
                    query_where += " AND collection_name = $2"
                    params.append(collection_name)
                
                query_stats = await conn.fetchrow(f"""
                SELECT 
                    COUNT(*) as total_queries,
                    AVG(execution_time_ms) as avg_execution_time,
                    MIN(execution_time_ms) as min_execution_time,
                    MAX(execution_time_ms) as max_execution_time,
                    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY execution_time_ms) as median_execution_time,
                    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY execution_time_ms) as p95_execution_time,
                    COUNT(*) FILTER (WHERE cache_hit = true) as cache_hits,
                    COUNT(DISTINCT query_type) as query_types_used,
                    AVG(result_count) as avg_result_count
                FROM vector_query_metrics 
                {query_where}
                """, *params)
                
                # 最慢查询
                slow_queries = await conn.fetch(f"""
                SELECT collection_name, query_type, execution_time_ms, result_count, timestamp
                FROM vector_query_metrics 
                {query_where}
                ORDER BY execution_time_ms DESC 
                LIMIT 10
                """, *params)
                
                # 索引使用情况
                index_usage = await conn.fetch(f"""
                SELECT DISTINCT 
                    collection_name, index_name, index_type,
                    MAX(index_scans) as total_scans,
                    MAX(index_size_bytes) as size_bytes
                FROM vector_index_metrics 
                WHERE timestamp >= $1
                GROUP BY collection_name, index_name, index_type
                ORDER BY total_scans DESC
                """, since_time)
                
                # 系统趋势
                system_trend = await conn.fetch("""
                SELECT 
                    DATE_TRUNC('hour', timestamp) as hour,
                    AVG(queries_per_second) as avg_qps,
                    AVG(average_query_time_ms) as avg_query_time,
                    AVG(cache_hit_ratio) as avg_cache_hit_ratio
                FROM vector_system_metrics 
                WHERE timestamp >= $1
                GROUP BY hour
                ORDER BY hour
                """, since_time)
                
                return {
                    "report_period": {
                        "start_time": since_time.isoformat(),
                        "end_time": datetime.now().isoformat(),
                        "collection_name": collection_name
                    },
                    "query_performance": {
                        "total_queries": query_stats["total_queries"] or 0,
                        "average_execution_time_ms": float(query_stats["avg_execution_time"] or 0),
                        "min_execution_time_ms": float(query_stats["min_execution_time"] or 0),
                        "max_execution_time_ms": float(query_stats["max_execution_time"] or 0),
                        "median_execution_time_ms": float(query_stats["median_execution_time"] or 0),
                        "p95_execution_time_ms": float(query_stats["p95_execution_time"] or 0),
                        "cache_hit_rate": (query_stats["cache_hits"] or 0) / max(query_stats["total_queries"] or 1, 1),
                        "query_types_used": query_stats["query_types_used"] or 0,
                        "average_result_count": float(query_stats["avg_result_count"] or 0)
                    },
                    "slow_queries": [
                        {
                            "collection_name": row["collection_name"],
                            "query_type": row["query_type"],
                            "execution_time_ms": float(row["execution_time_ms"]),
                            "result_count": row["result_count"],
                            "timestamp": row["timestamp"].isoformat()
                        }
                        for row in slow_queries
                    ],
                    "index_usage": [
                        {
                            "collection_name": row["collection_name"],
                            "index_name": row["index_name"],
                            "index_type": row["index_type"],
                            "total_scans": row["total_scans"],
                            "size_bytes": row["size_bytes"]
                        }
                        for row in index_usage
                    ],
                    "system_trends": [
                        {
                            "hour": row["hour"].isoformat(),
                            "avg_qps": float(row["avg_qps"] or 0),
                            "avg_query_time_ms": float(row["avg_query_time"] or 0),
                            "avg_cache_hit_ratio": float(row["avg_cache_hit_ratio"] or 0)
                        }
                        for row in system_trend
                    ]
                }
                
        except Exception as e:
            logger.error(f"生成性能报告失败: {e}")
            return {"error": str(e)}
            
    async def start_monitoring(self):
        """启动性能监控"""
        if self.is_running:
            return
            
        self.is_running = True
        logger.info("启动向量数据库性能监控")
        
        while self.is_running:
            try:
                # 收集指标
                await self.collect_index_metrics()
                await self.collect_system_metrics()
                
                # 等待下一次收集
                await asyncio.sleep(settings.VECTOR_METRICS_COLLECTION_INTERVAL)
                
            except Exception as e:
                logger.error(f"性能监控错误: {e}")
                await asyncio.sleep(60)  # 错误时等待1分钟
                
    async def stop_monitoring(self):
        """停止性能监控"""
        self.is_running = False
        if self.pool:
            await self.pool.close()
        logger.info("向量数据库性能监控已停止")
        
    async def cleanup_old_metrics(self, retention_days: int = 30):
        """清理过期指标数据"""
        if not self.pool:
            await self.initialize()
            
        try:
            cutoff_time = datetime.now() - timedelta(days=retention_days)
            
            async with self.pool.acquire() as conn:
                # 清理查询指标
                deleted_queries = await conn.fetchval("""
                DELETE FROM vector_query_metrics 
                WHERE timestamp < $1
                RETURNING COUNT(*)
                """, cutoff_time)
                
                # 清理索引指标
                deleted_indexes = await conn.fetchval("""
                DELETE FROM vector_index_metrics 
                WHERE timestamp < $1
                RETURNING COUNT(*)
                """, cutoff_time)
                
                # 清理系统指标
                deleted_system = await conn.fetchval("""
                DELETE FROM vector_system_metrics 
                WHERE timestamp < $1
                RETURNING COUNT(*)
                """, cutoff_time)
                
                logger.info(f"清理过期指标: 查询指标 {deleted_queries}, 索引指标 {deleted_indexes}, 系统指标 {deleted_system}")
                
        except Exception as e:
            logger.error(f"清理过期指标失败: {e}")


# 全局指标收集器实例
metrics_collector = VectorMetricsCollector()


async def get_metrics_collector() -> VectorMetricsCollector:
    """获取指标收集器实例"""
    if not metrics_collector.pool:
        await metrics_collector.initialize()
    return metrics_collector