# AIæ¨¡å‹æ³¨å†Œè¡¨ç³»ç»Ÿ (ModelRegistry)

ä¸€ä¸ªç»Ÿä¸€çš„AIæ¨¡å‹ç®¡ç†ç³»ç»Ÿï¼Œæ”¯æŒPyTorchã€ONNXå’ŒHuggingFaceæ¨¡å‹çš„æ³¨å†Œã€åŠ è½½ã€ç‰ˆæœ¬æ§åˆ¶å’Œå…ƒæ•°æ®ç®¡ç†ã€‚

## ç‰¹æ€§

### ğŸ¯ æ ¸å¿ƒåŠŸèƒ½
- **å¤šæ ¼å¼æ”¯æŒ**: PyTorch (.pth, .pt), ONNX (.onnx), HuggingFace Transformers
- **å®‰å…¨åŠ è½½**: éµå¾ªPyTorch `weights_only=True`ç­‰å®‰å…¨æœ€ä½³å®è·µ
- **å…ƒæ•°æ®ç®¡ç†**: è‡ªåŠ¨æå–å’Œç®¡ç†æ¨¡å‹å…ƒæ•°æ®ï¼ˆå‚æ•°æ•°é‡ã€å¤§å°ã€æ€§èƒ½æŒ‡æ ‡ç­‰ï¼‰
- **ç‰ˆæœ¬æ§åˆ¶**: æ”¯æŒæ¨¡å‹å¤šç‰ˆæœ¬ç®¡ç†
- **å®Œæ•´æ€§éªŒè¯**: SHA256æ ¡éªŒå’Œç¡®ä¿æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§
- **RESTful API**: å®Œæ•´çš„HTTP APIæ”¯æŒ

### ğŸ”§ é«˜çº§åŠŸèƒ½
- **æ™ºèƒ½åŠ è½½**: è‡ªåŠ¨æ£€æµ‹æ¨¡å‹æ ¼å¼å’Œç±»å‹
- **å‹ç¼©æ”¯æŒ**: æ¨¡å‹å‹ç¼©å’Œé‡åŒ–ä¿¡æ¯è·Ÿè¸ª
- **åºåˆ—åŒ–**: çµæ´»çš„åºåˆ—åŒ–æ ¼å¼æ”¯æŒ
- **æ‰¹é‡æ“ä½œ**: æ”¯æŒæ‰¹é‡æ¨¡å‹ç®¡ç†
- **ä¸´æ—¶åŠ è½½**: ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ”¯æŒ
- **å¯¼å‡ºåŠŸèƒ½**: æ”¯æŒæ¨¡å‹æ ¼å¼è½¬æ¢å’Œå¯¼å‡º

## å®‰è£…

```bash
# åŸºç¡€ä¾èµ–
pip install fastapi pydantic

# PyTorchæ”¯æŒ
pip install torch torchvision

# ONNXæ”¯æŒ  
pip install onnx onnxruntime

# HuggingFaceæ”¯æŒ
pip install transformers tokenizers safetensors

# å¯é€‰ï¼šç”¨äºé«˜çº§åŠŸèƒ½
pip install numpy pillow
```

## å¿«é€Ÿå¼€å§‹

### 1. åŸºç¡€ä½¿ç”¨

```python
from model_registry import ModelRegistry, ModelFormat, ModelType

# åˆ›å»ºæ³¨å†Œè¡¨
registry = ModelRegistry("./models")

# æ³¨å†ŒPyTorchæ¨¡å‹
import torch
import torch.nn as nn

model = nn.Linear(10, 1)
entry = registry.register_model(
    name="linear_model",
    model=model,
    model_format=ModelFormat.PYTORCH,
    model_type=ModelType.CUSTOM,
    version="1.0.0",
    description="ç®€å•çº¿æ€§æ¨¡å‹"
)

# åŠ è½½æ¨¡å‹
loaded_model, tokenizer = registry.load_model("linear_model", "1.0.0")
```

### 2. HuggingFaceæ¨¡å‹

```python
from transformers import AutoModel, AutoTokenizer

# ä»HubåŠ è½½
model = AutoModel.from_pretrained("distilbert-base-uncased")
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

# æ³¨å†Œåˆ°æœ¬åœ°
entry = registry.register_model(
    name="distilbert",
    model=model,
    tokenizer=tokenizer,
    model_format=ModelFormat.HUGGINGFACE,
    model_type=ModelType.LANGUAGE_MODEL,
    description="DistilBERTæ¨¡å‹"
)
```

### 3. ONNXæ¨¡å‹

```python
import onnx

# åŠ è½½ONNXæ¨¡å‹
onnx_model = onnx.load("model.onnx")

# æ³¨å†Œ
entry = registry.register_model(
    name="onnx_model",
    model=onnx_model,
    model_format=ModelFormat.ONNX,
    description="ONNXæ ¼å¼æ¨¡å‹"
)
```

## APIæ–‡æ¡£

### REST APIç«¯ç‚¹

```
GET    /model-registry/models              # åˆ—å‡ºæ‰€æœ‰æ¨¡å‹
GET    /model-registry/models/{name}       # è·å–æ¨¡å‹ä¿¡æ¯
POST   /model-registry/models/upload       # ä¸Šä¼ æ³¨å†Œæ¨¡å‹
POST   /model-registry/models/{name}/register-from-hub  # ä»Hubæ³¨å†Œ
DELETE /model-registry/models/{name}       # åˆ é™¤æ¨¡å‹
GET    /model-registry/models/{name}/download  # ä¸‹è½½æ¨¡å‹
GET    /model-registry/models/{name}/export    # å¯¼å‡ºæ¨¡å‹
GET    /model-registry/validate             # éªŒè¯æ³¨å†Œè¡¨
GET    /model-registry/stats               # ç»Ÿè®¡ä¿¡æ¯
```

### Python API

#### æ ¸å¿ƒç±»

```python
class ModelRegistry:
    def register_model(self, name: str, model: Any, model_format: ModelFormat, **kwargs) -> ModelEntry
    def load_model(self, name: str, version: str = "latest") -> Tuple[Any, Optional[Any]]
    def list_models(self, model_type: ModelType = None) -> List[ModelEntry]
    def get_model_info(self, name: str, version: str = "latest") -> Optional[ModelEntry]
    def remove_model(self, name: str, version: str = None) -> bool
    def export_model(self, name: str, version: str, export_path: str, export_format: ModelFormat = None) -> str
    def validate_registry(self) -> Dict[str, List[str]]

class ModelMetadata:
    name: str
    version: str
    format: ModelFormat
    model_type: ModelType
    description: Optional[str]
    parameters_count: Optional[int]
    model_size_mb: Optional[float]
    # ... æ›´å¤šå­—æ®µ
```

#### ä¾¿æ·å‡½æ•°

```python
# å¿«é€Ÿæ³¨å†Œå‡½æ•°
register_pytorch_model(name, model, version="1.0.0", **kwargs)
register_onnx_model(name, model, version="1.0.0", **kwargs)  
register_huggingface_model(name, model, tokenizer=None, version="1.0.0", **kwargs)
```

## ä½¿ç”¨ç¤ºä¾‹

### å®Œæ•´å·¥ä½œæµç¨‹

```python
import torch
import torch.nn as nn
from model_registry import ModelRegistry, ModelFormat, ModelType

# 1. åˆ›å»ºæ³¨å†Œè¡¨
registry = ModelRegistry("./my_models")

# 2. åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
class MNISTClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.layers = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, 10)
        )
    
    def forward(self, x):
        x = self.flatten(x)
        return self.layers(x)

model = MNISTClassifier()
# ... è®­ç»ƒè¿‡ç¨‹ ...

# 3. æ³¨å†Œè®­ç»ƒå¥½çš„æ¨¡å‹
entry = registry.register_model(
    name="mnist_classifier",
    model=model,
    model_format=ModelFormat.PYTORCH,
    model_type=ModelType.CLASSIFICATION,
    version="1.0.0",
    description="MNISTæ‰‹å†™æ•°å­—åˆ†ç±»å™¨",
    author="Your Name",
    training_dataset="MNIST",
    training_epochs=10,
    performance_metrics={
        "accuracy": 0.98,
        "loss": 0.05,
        "f1_score": 0.97
    },
    tags=["mnist", "classification", "cnn"],
    input_shape=[1, 28, 28],
    output_shape=[10]
)

print(f"æ³¨å†ŒæˆåŠŸ! æ¨¡å‹ID: {entry.metadata.name}:{entry.metadata.version}")
print(f"å‚æ•°æ•°é‡: {entry.metadata.parameters_count:,}")
print(f"æ¨¡å‹å¤§å°: {entry.metadata.model_size_mb:.2f} MB")

# 4. ç¨ååŠ è½½ä½¿ç”¨
loaded_model, _ = registry.load_model("mnist_classifier", "1.0.0")

# 5. è¿›è¡Œæ¨ç†
test_input = torch.randn(1, 1, 28, 28)
with torch.no_grad():
    predictions = loaded_model(test_input)
    predicted_class = torch.argmax(predictions, dim=1)
    print(f"é¢„æµ‹ç±»åˆ«: {predicted_class.item()}")
```

### ç‰ˆæœ¬ç®¡ç†

```python
# æ³¨å†Œå¤šä¸ªç‰ˆæœ¬
versions = [
    {"version": "1.0.0", "accuracy": 0.95},
    {"version": "1.1.0", "accuracy": 0.97}, 
    {"version": "2.0.0", "accuracy": 0.99}
]

for ver in versions:
    registry.register_model(
        name="my_model",
        model=trained_models[ver["version"]],
        model_format=ModelFormat.PYTORCH,
        version=ver["version"],
        performance_metrics={"accuracy": ver["accuracy"]}
    )

# åˆ—å‡ºæ‰€æœ‰ç‰ˆæœ¬
models = registry.list_models()
my_models = [m for m in models if m.metadata.name == "my_model"]
for model in sorted(my_models, key=lambda x: x.metadata.version):
    acc = model.metadata.performance_metrics.get("accuracy", "N/A")
    print(f"ç‰ˆæœ¬ {model.metadata.version}: å‡†ç¡®ç‡ {acc}")

# åŠ è½½æœ€æ–°ç‰ˆæœ¬
latest_model, _ = registry.load_model("my_model", "latest")
```

### æ‰¹é‡ç®¡ç†

```python
# åˆ—å‡ºæ‰€æœ‰åˆ†ç±»æ¨¡å‹
classification_models = registry.list_models(ModelType.CLASSIFICATION)
print(f"åˆ†ç±»æ¨¡å‹æ•°é‡: {len(classification_models)}")

# éªŒè¯æ‰€æœ‰æ¨¡å‹
validation_result = registry.validate_registry()
print(f"æ€»æ¨¡å‹æ•°: {validation_result['total_models']}")
print(f"æœ‰æ•ˆæ¨¡å‹æ•°: {validation_result['valid_models']}")
if validation_result['errors']:
    print("é”™è¯¯:")
    for error in validation_result['errors']:
        print(f"  - {error}")

# ä¸´æ—¶åŠ è½½æ¨¡å‹ï¼ˆè‡ªåŠ¨æ¸…ç†ï¼‰
with registry.temporary_model("my_model") as (model, tokenizer):
    # ä½¿ç”¨æ¨¡å‹
    result = model(input_data)
    # é€€å‡ºæ—¶è‡ªåŠ¨æ¸…ç†èµ„æº
```

## æœ€ä½³å®è·µ

### 1. å®‰å…¨æ€§

```python
# ä½¿ç”¨å®‰å…¨åŠ è½½é€‰é¡¹
loaded_model, _ = registry.load_model(
    "my_model", 
    weights_only=True  # PyTorchå®‰å…¨åŠ è½½
)

# éªŒè¯æ¨¡å‹å®Œæ•´æ€§
entry = registry.get_model_info("my_model")
if not entry.verify_integrity():
    print("âš ï¸ æ¨¡å‹æ–‡ä»¶å¯èƒ½å·²æŸå!")
```

### 2. æ€§èƒ½ä¼˜åŒ–

```python
# å¤§æ¨¡å‹ä½¿ç”¨åˆ†ç‰‡ä¿å­˜
registry.register_model(
    name="large_model",
    model=large_model, 
    model_format=ModelFormat.HUGGINGFACE,
    max_shard_size="2GB"  # HuggingFaceæ¨¡å‹
)

# æŒ‡å®šè®¾å¤‡åŠ è½½
model, _ = registry.load_model(
    "my_model",
    device="cuda" if torch.cuda.is_available() else "cpu"
)
```

### 3. å…ƒæ•°æ®ç®¡ç†

```python
# è¯¦ç»†çš„å…ƒæ•°æ®
registry.register_model(
    name="production_model",
    model=model,
    model_format=ModelFormat.PYTORCH,
    model_type=ModelType.CLASSIFICATION,
    
    # è®­ç»ƒä¿¡æ¯
    training_framework="PyTorch 2.0",
    training_dataset="Custom Dataset v2.1",
    training_epochs=50,
    
    # æ€§èƒ½æŒ‡æ ‡
    performance_metrics={
        "accuracy": 0.987,
        "precision": 0.985,
        "recall": 0.989,
        "f1_score": 0.987,
        "inference_time_ms": 15.2
    },
    
    # å‹ç¼©ä¿¡æ¯
    compression_type=CompressionType.QUANTIZATION_INT8,
    compression_ratio=0.25,
    original_size_mb=400.0,
    
    # ä¾èµ–ä¿¡æ¯
    dependencies=["torch>=2.0", "torchvision>=0.15"],
    python_version="3.9",
    
    # æ ‡ç­¾å’Œæ–‡æ¡£
    tags=["production", "optimized", "quantized"],
    license="MIT",
    repository_url="https://github.com/myorg/mymodel",
    paper_url="https://arxiv.org/abs/xxxx.xxxxx"
)
```

## é”™è¯¯å¤„ç†

```python
try:
    # æ¨¡å‹æ³¨å†Œ
    entry = registry.register_model(name, model, model_format)
except ValueError as e:
    print(f"æ³¨å†Œå¤±è´¥: {e}")
except FileNotFoundError as e:
    print(f"æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
except Exception as e:
    print(f"æœªçŸ¥é”™è¯¯: {e}")

try:
    # æ¨¡å‹åŠ è½½
    model, tokenizer = registry.load_model(name, version)
except ValueError as e:
    print(f"æ¨¡å‹æœªæ‰¾åˆ°: {e}")
except RuntimeError as e:
    print(f"å®Œæ•´æ€§éªŒè¯å¤±è´¥: {e}")
except ImportError as e:
    print(f"ç¼ºå°‘ä¾èµ–: {e}")
```

## é…ç½®å’Œæ‰©å±•

### è‡ªå®šä¹‰åŠ è½½å™¨

```python
from model_registry import ModelLoader, ModelFormat

class CustomLoader(ModelLoader):
    def load(self, model_path: str, **kwargs):
        # è‡ªå®šä¹‰åŠ è½½é€»è¾‘
        pass
    
    def save(self, model, model_path: str, **kwargs):
        # è‡ªå®šä¹‰ä¿å­˜é€»è¾‘
        pass
    
    def get_metadata(self, model, model_path: str = None):
        # è‡ªå®šä¹‰å…ƒæ•°æ®æå–
        return {"framework": "custom"}
    
    def supported_formats(self):
        return [ModelFormat.CUSTOM]

# æ³¨å†Œè‡ªå®šä¹‰åŠ è½½å™¨
registry.loaders[ModelFormat.CUSTOM] = CustomLoader()
```

### ç¯å¢ƒé…ç½®

```python
# ç¯å¢ƒå˜é‡
import os
os.environ['MODEL_REGISTRY_PATH'] = '/path/to/models'
os.environ['MODEL_REGISTRY_CACHE_SIZE'] = '1000'

# é…ç½®æ–‡ä»¶
registry_config = {
    'default_format': ModelFormat.PYTORCH,
    'auto_validate': True,
    'compression_threshold': 100,  # MB
    'backup_enabled': True
}

registry = ModelRegistry(
    registry_path="./models",
    **registry_config
)
```

## æµ‹è¯•

```bash
# è¿è¡Œæµ‹è¯•
cd apps/api
python -m pytest tests/ai/model_service/ -v

# è¿è¡Œç¤ºä¾‹
python src/ai/model_service/examples.py

# éªŒè¯API
python -m pytest tests/ai/model_service/test_model_registry.py::TestModelRegistry -v
```

## ç›‘æ§å’Œæ—¥å¿—

```python
import logging

# è®¾ç½®æ—¥å¿—çº§åˆ«
logging.getLogger('model_registry').setLevel(logging.INFO)

# ç›‘æ§æ³¨å†Œè¡¨çŠ¶æ€
stats = registry.validate_registry()
print(f"å¥åº·çŠ¶æ€: {stats['valid_models']}/{stats['total_models']} æ¨¡å‹æœ‰æ•ˆ")

# è·å–è¯¦ç»†ç»Ÿè®¡
detailed_stats = registry.get_detailed_stats()  # éœ€è¦å®ç°
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **ImportError: ç¼ºå°‘ä¾èµ–**
   ```bash
   pip install torch transformers onnx
   ```

2. **FileNotFoundError: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨**
   ```python
   # éªŒè¯æ³¨å†Œè¡¨
   validation = registry.validate_registry()
   print(validation['errors'])
   ```

3. **RuntimeError: å®Œæ•´æ€§éªŒè¯å¤±è´¥**
   ```python
   # é‡æ–°è®¡ç®—æ ¡éªŒå’Œ
   entry.calculate_checksum()
   ```

4. **MemoryError: æ¨¡å‹è¿‡å¤§**
   ```python
   # ä½¿ç”¨åˆ†ç‰‡åŠ è½½
   model, _ = registry.load_model(name, max_memory={"0": "8GiB"})
   ```

## è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

## è®¸å¯è¯

MIT License - è¯¦è§LICENSEæ–‡ä»¶