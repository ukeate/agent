#!/usr/bin/env python3
"""
Q-Learningæ€§èƒ½ä¼˜åŒ–ç³»ç»ŸéªŒè¯è„šæœ¬
éªŒè¯GPUåŠ é€Ÿã€åˆ†å¸ƒå¼è®­ç»ƒã€é›†æˆæµ‹è¯•ç­‰æ ¸å¿ƒåŠŸèƒ½
"""

import sys
import time
import numpy as np
from pathlib import Path

# æ·»åŠ è·¯å¾„
sys.path.append(str(Path(__file__).parent))

def test_gpu_accelerator():
    """æµ‹è¯•GPUåŠ é€Ÿå™¨"""
    print("="*60)
    print("æµ‹è¯•GPUåŠ é€Ÿå™¨")
    print("="*60)
    
    try:
        from ai.reinforcement_learning.performance import GPUAccelerator, GPUConfig
        
        config = GPUConfig(
            enable_mixed_precision=True,
            enable_xla=True,
            batch_size_multiplier=2
        )
        
        accelerator = GPUAccelerator(config)
        
        # è·å–è®¾å¤‡ä¿¡æ¯
        device_info = accelerator.get_device_info()
        print(f"æ£€æµ‹åˆ° {len(device_info['gpu_devices'])} ä¸ªGPUè®¾å¤‡")
        print(f"CPUæ ¸å¿ƒæ•°: {device_info['cpu_count']}")
        
        # åˆ›å»ºæ€§èƒ½ç›‘æ§å™¨
        monitor = accelerator.create_performance_monitor()
        monitor.start_monitoring()
        
        time.sleep(2)  # ç›‘æ§2ç§’
        
        monitor.stop_monitoring()
        summary = monitor.get_performance_summary()
        print(f"æ€§èƒ½ç›‘æ§æ‘˜è¦: {summary}")
        
        print("âœ… GPUåŠ é€Ÿå™¨æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ GPUåŠ é€Ÿå™¨æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_optimized_replay_buffer():
    """æµ‹è¯•ä¼˜åŒ–çš„å›æ”¾ç¼“å†²åŒº"""
    print("\n" + "="*60)
    print("æµ‹è¯•ä¼˜åŒ–çš„å›æ”¾ç¼“å†²åŒº")
    print("="*60)
    
    try:
        from ai.reinforcement_learning.performance import OptimizedReplayBuffer, BufferConfig
        from ai.reinforcement_learning.qlearning.base import Experience, AgentState
        
        config = BufferConfig(
            capacity=1000,
            strategy="prioritized",
            compression=True,
            batch_size=32
        )
        
        buffer = OptimizedReplayBuffer(config)
        
        # æ·»åŠ ç»éªŒ
        print("æ·»åŠ 1000ä¸ªç»éªŒåˆ°ç¼“å†²åŒº...")
        for i in range(1000):
            state = AgentState(features=np.random.random(8).tolist())
            next_state = AgentState(features=np.random.random(8).tolist())
            
            experience = Experience(
                state=state,
                action=f"action_{i % 4}",
                reward=np.random.random(),
                next_state=next_state,
                done=np.random.random() < 0.1
            )
            
            buffer.push(experience)
        
        # é‡‡æ ·æµ‹è¯•
        print("æµ‹è¯•é‡‡æ ·æ€§èƒ½...")
        start_time = time.time()
        
        for _ in range(100):
            if hasattr(buffer, 'sample'):
                batch = buffer.sample(32)
            else:
                break
        
        sampling_time = time.time() - start_time
        print(f"100æ¬¡é‡‡æ ·è€—æ—¶: {sampling_time:.3f}ç§’")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = buffer.get_statistics()
        print(f"ç¼“å†²åŒºç»Ÿè®¡: {stats}")
        
        print("âœ… ä¼˜åŒ–å›æ”¾ç¼“å†²åŒºæµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ ä¼˜åŒ–å›æ”¾ç¼“å†²åŒºæµ‹è¯•å¤±è´¥: {e}")
        return False


def test_distributed_training():
    """æµ‹è¯•åˆ†å¸ƒå¼è®­ç»ƒ"""
    print("\n" + "="*60)
    print("æµ‹è¯•åˆ†å¸ƒå¼è®­ç»ƒ")
    print("="*60)
    
    try:
        from ai.reinforcement_learning.performance import DistributedTrainingManager, DistributedConfig
        
        config = DistributedConfig(
            strategy="data_parallel",
            num_workers=2,
            batch_size_per_worker=16
        )
        
        manager = DistributedTrainingManager(config)
        
        # è·å–é›†ç¾¤çŠ¶æ€
        status = manager.get_cluster_status()
        print(f"é›†ç¾¤çŠ¶æ€: {status}")
        
        # æµ‹è¯•é€šä¿¡æ€§èƒ½
        comm_results = manager.benchmark_communication()
        print(f"é€šä¿¡æ€§èƒ½æµ‹è¯•: {comm_results}")
        
        print("âœ… åˆ†å¸ƒå¼è®­ç»ƒæµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ åˆ†å¸ƒå¼è®­ç»ƒæµ‹è¯•å¤±è´¥: {e}")
        return False


def test_integration_framework():
    """æµ‹è¯•é›†æˆæµ‹è¯•æ¡†æ¶"""
    print("\n" + "="*60)
    print("æµ‹è¯•é›†æˆæµ‹è¯•æ¡†æ¶")
    print("="*60)
    
    try:
        from ai.reinforcement_learning.performance import (
            IntegrationTestSuite, TestConfig, TestScenario
        )
        
        # è¿è¡ŒåŸºç¡€è®­ç»ƒæµ‹è¯•
        config = TestConfig(
            scenarios=[TestScenario.BASIC_TRAINING],
            episodes=50,  # å‡å°‘episodeæ•°é‡ä»¥åŠ å¿«æµ‹è¯•
            save_results=False
        )
        
        test_suite = IntegrationTestSuite(config)
        results = test_suite.run_all_tests()
        
        print(f"é›†æˆæµ‹è¯•ç»“æœ: {results}")
        
        # æ£€æŸ¥æµ‹è¯•æ˜¯å¦é€šè¿‡
        basic_training = results.get("basic_training", {})
        if basic_training.get("status") == "completed":
            print("âœ… é›†æˆæµ‹è¯•æ¡†æ¶æµ‹è¯•é€šè¿‡")
            return True
        else:
            print("âŒ é›†æˆæµ‹è¯•æ¡†æ¶æµ‹è¯•å¤±è´¥")
            return False
        
    except Exception as e:
        print(f"âŒ é›†æˆæµ‹è¯•æ¡†æ¶æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_benchmark_optimizer():
    """æµ‹è¯•åŸºå‡†æµ‹è¯•å’Œä¼˜åŒ–å™¨"""
    print("\n" + "="*60)
    print("æµ‹è¯•åŸºå‡†æµ‹è¯•å’Œä¼˜åŒ–å™¨")
    print("="*60)
    
    try:
        from ai.reinforcement_learning.performance import (
            PerformanceBenchmark, BenchmarkConfig, 
            HyperparameterOptimizer, HyperparameterSpace, OptimizationTarget
        )
        
        # å¿«é€ŸåŸºå‡†æµ‹è¯•
        config = BenchmarkConfig(
            episodes_per_run=50,
            num_seeds=2,
            save_detailed_logs=False
        )
        
        benchmark = PerformanceBenchmark(config)
        
        # è¿è¡Œç®—æ³•å¯¹æ¯”æµ‹è¯•
        print("è¿è¡Œç®—æ³•å¯¹æ¯”åŸºå‡†æµ‹è¯•...")
        algorithm_results = benchmark.run_algorithm_comparison()
        
        print("ç®—æ³•å¯¹æ¯”ç»“æœ:")
        for algo, result in algorithm_results.items():
            performance = result.get("mean_final_performance", 0)
            print(f"  {algo}: å¹³å‡æ€§èƒ½ {performance:.3f}")
        
        # å¿«é€Ÿè¶…å‚æ•°ä¼˜åŒ–æµ‹è¯•
        print("\nè¿è¡Œå¿«é€Ÿè¶…å‚æ•°ä¼˜åŒ–...")
        hyperparameter_space = HyperparameterSpace()
        optimization_target = OptimizationTarget.FINAL_PERFORMANCE
        
        optimizer = HyperparameterOptimizer(
            hyperparameter_space=hyperparameter_space,
            optimization_target=optimization_target,
            benchmark_config=BenchmarkConfig(episodes_per_run=25, num_seeds=1)
        )
        
        # åªè¿è¡Œ3æ¬¡è¯•éªŒè¿›è¡Œå¿«é€Ÿæµ‹è¯•
        optimization_results = optimizer.optimize(n_trials=3)
        
        print(f"ä¼˜åŒ–ç»“æœ: æœ€ä½³ç›®æ ‡å€¼ {optimization_results['best_value']:.3f}")
        print(f"æœ€ä½³å‚æ•°: {optimization_results['best_params']}")
        
        print("âœ… åŸºå‡†æµ‹è¯•å’Œä¼˜åŒ–å™¨æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ åŸºå‡†æµ‹è¯•å’Œä¼˜åŒ–å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_performance_presets():
    """æµ‹è¯•æ€§èƒ½é¢„è®¾é…ç½®"""
    print("\n" + "="*60)
    print("æµ‹è¯•æ€§èƒ½é¢„è®¾é…ç½®")
    print("="*60)
    
    try:
        from ai.reinforcement_learning.performance import (
            create_performance_config, optimize_for_hardware, quick_performance_test
        )
        
        # æµ‹è¯•é¢„è®¾é…ç½®
        presets = ["high_performance", "memory_efficient", "development"]
        
        for preset in presets:
            config = create_performance_config(preset)
            print(f"{preset} é¢„è®¾é…ç½®åˆ›å»ºæˆåŠŸ")
            
            # éªŒè¯é…ç½®ç»“æ„
            required_keys = ["gpu_config", "buffer_config", "distributed_config"]
            for key in required_keys:
                if key not in config:
                    raise ValueError(f"ç¼ºå°‘é…ç½®é”®: {key}")
        
        # æµ‹è¯•ç¡¬ä»¶ä¼˜åŒ–
        print("æµ‹è¯•ç¡¬ä»¶è‡ªé€‚åº”ä¼˜åŒ–...")
        optimized_config = optimize_for_hardware()
        print("ç¡¬ä»¶ä¼˜åŒ–é…ç½®åˆ›å»ºæˆåŠŸ")
        
        print("âœ… æ€§èƒ½é¢„è®¾é…ç½®æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½é¢„è®¾é…ç½®æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_q_learning_integration():
    """æµ‹è¯•Q-Learningå®Œæ•´é›†æˆ"""
    print("\n" + "="*60)
    print("æµ‹è¯•Q-Learningå®Œæ•´é›†æˆ")
    print("="*60)
    
    try:
        from ai.reinforcement_learning.qlearning.dqn import DQNAgent
        from ai.reinforcement_learning.qlearning.base import QLearningConfig, AlgorithmType
        from ai.reinforcement_learning.performance import GPUAccelerator, OptimizedReplayBuffer
        from ai.reinforcement_learning.performance import GPUConfig, BufferConfig
        
        # åˆ›å»ºä¼˜åŒ–é…ç½®
        gpu_config = GPUConfig(enable_mixed_precision=False, enable_xla=False)  # ç®€åŒ–é…ç½®
        accelerator = GPUAccelerator(gpu_config)
        
        buffer_config = BufferConfig(capacity=1000, batch_size=32)
        replay_buffer = OptimizedReplayBuffer(buffer_config)
        
        # åˆ›å»ºDQNæ™ºèƒ½ä½“
        agent_config = QLearningConfig(
            algorithm_type=AlgorithmType.DQN,
            learning_rate=0.001,
            batch_size=32,
            buffer_size=1000
        )
        
        agent = DQNAgent(
            agent_id="test_agent",
            state_size=8,
            action_size=4,
            config=agent_config,
            action_space=["action_0", "action_1", "action_2", "action_3"]
        )
        
        print(f"æ™ºèƒ½ä½“åˆ›å»ºæˆåŠŸ: {agent.agent_id}")
        print(f"ç½‘ç»œæ‘˜è¦:\n{agent.get_network_summary()}")
        
        # æµ‹è¯•åŸºæœ¬è®­ç»ƒæ­¥éª¤
        from ai.reinforcement_learning.qlearning.base import Experience, AgentState
        
        state = AgentState(features=np.random.random(8).tolist())
        action = agent.get_action(state)
        print(f"æ™ºèƒ½ä½“åŠ¨ä½œé€‰æ‹©: {action}")
        
        next_state = AgentState(features=np.random.random(8).tolist())
        experience = Experience(
            state=state,
            action=action,
            reward=1.0,
            next_state=next_state,
            done=False
        )
        
        loss = agent.update_q_value(experience)
        print(f"è®­ç»ƒæŸå¤±: {loss}")
        
        print("âœ… Q-Learningå®Œæ•´é›†æˆæµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ Q-Learningå®Œæ•´é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹Q-Learningæ€§èƒ½ä¼˜åŒ–ç³»ç»ŸéªŒè¯")
    print("="*80)
    
    tests = [
        ("GPUåŠ é€Ÿå™¨", test_gpu_accelerator),
        ("ä¼˜åŒ–å›æ”¾ç¼“å†²åŒº", test_optimized_replay_buffer),
        ("åˆ†å¸ƒå¼è®­ç»ƒ", test_distributed_training),
        ("é›†æˆæµ‹è¯•æ¡†æ¶", test_integration_framework),
        ("åŸºå‡†æµ‹è¯•å’Œä¼˜åŒ–å™¨", test_benchmark_optimizer),
        ("æ€§èƒ½é¢„è®¾é…ç½®", test_performance_presets),
        ("Q-Learningå®Œæ•´é›†æˆ", test_q_learning_integration)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
            results.append((test_name, False))
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "="*80)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("="*80)
    
    passed = 0
    total = len(results)
    
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
        if result:
            passed += 1
    
    print(f"\næ€»ä½“ç»“æœ: {passed}/{total} æµ‹è¯•é€šè¿‡ ({passed/total*100:.1f}%)")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½éªŒè¯é€šè¿‡ï¼")
        return True
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³åŠŸèƒ½")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)