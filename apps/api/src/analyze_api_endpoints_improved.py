import os
import re
import json
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional
from src.core.logging import setup_logging

from src.core.logging import get_logger
logger = get_logger(__name__)

#!/usr/bin/env python3
"""
æ”¹è¿›ç‰ˆå‰ç«¯æœåŠ¡æ–‡ä»¶APIç«¯ç‚¹åˆ†æå™¨
"""

def extract_api_endpoints_improved(file_path: str) -> List[Dict[str, str]]:
    """ä»æ–‡ä»¶ä¸­æå–APIç«¯ç‚¹è°ƒç”¨ - æ”¹è¿›ç‰ˆæœ¬"""
    endpoints = []
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        return []
    
    # æ¸…ç†æ³¨é‡Šå’Œå­—ç¬¦ä¸²å­—é¢é‡ï¼Œé¿å…è¯¯åŒ¹é…
    lines = content.split('\n')
    clean_lines = []
    in_multiline_comment = False
    
    for line in lines:
        # è·³è¿‡å•è¡Œæ³¨é‡Š
        if '//' in line:
            line = line[:line.index('//')]
        
        # å¤„ç†å¤šè¡Œæ³¨é‡Š
        if '/*' in line and '*/' in line:
            # å•è¡Œå†…çš„å¤šè¡Œæ³¨é‡Š
            start = line.index('/*')
            end = line.index('*/') + 2
            line = line[:start] + line[end:]
        elif '/*' in line:
            in_multiline_comment = True
            line = line[:line.index('/*')]
        elif '*/' in line and in_multiline_comment:
            in_multiline_comment = False
            line = line[line.index('*/') + 2:]
        elif in_multiline_comment:
            continue
            
        clean_lines.append(line)
    
    clean_content = '\n'.join(clean_lines)
    
    # æ›´ç²¾ç¡®çš„APIè°ƒç”¨æ¨¡å¼
    patterns = [
        # apiClient.method('path', ...)
        r"apiClient\.(get|post|put|delete|patch)\s*\(\s*[`'\"]([^`'\"]+)[`'\"]\s*[,\)]",
        # axios.method('path', ...)
        r"axios\.(get|post|put|delete|patch)\s*\(\s*[`'\"]([^`'\"]+)[`'\"]\s*[,\)]",
        # this.client.method('path', ...)
        r"this\.client\.(get|post|put|delete|patch)\s*\(\s*[`'\"]([^`'\"]+)[`'\"]\s*[,\)]",
        # await fetch('path', {method: 'POST'})
        r"fetch\s*\(\s*[`'\"]([^`'\"]+)[`'\"]\s*,\s*\{[^}]*method\s*:\s*[`'\"]([^`'\"]+)[`'\"]*[^}]*\}",
        # await fetch('path') - é»˜è®¤GET
        r"fetch\s*\(\s*[`'\"]([^`'\"]+)[`'\"]\s*[,\)]",
    ]
    
    for pattern_idx, pattern in enumerate(patterns):
        matches = re.findall(pattern, clean_content, re.MULTILINE | re.IGNORECASE)
        
        for match in matches:
            if pattern_idx <= 2:  # apiClient, axios, this.client
                method, path = match
                if is_api_path(path):
                    endpoints.append({
                        'method': method.upper(),
                        'path': normalize_path(path),
                        'pattern_type': 'explicit_call',
                        'confidence': 'high'
                    })
            elif pattern_idx == 3:  # fetch with explicit method
                path, method = match
                if is_api_path(path):
                    endpoints.append({
                        'method': method.upper(),
                        'path': normalize_path(path),
                        'pattern_type': 'fetch_explicit',
                        'confidence': 'high'
                    })
            elif pattern_idx == 4:  # fetch without method (GET)
                path = match
                if is_api_path(path):
                    endpoints.append({
                        'method': 'GET',
                        'path': normalize_path(path),
                        'pattern_type': 'fetch_implicit',
                        'confidence': 'medium'
                    })
    
    # å»é‡
    seen = set()
    unique_endpoints = []
    for endpoint in endpoints:
        key = f"{endpoint['method']}:{endpoint['path']}"
        if key not in seen:
            seen.add(key)
            unique_endpoints.append(endpoint)
    
    return unique_endpoints

def is_api_path(path: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºAPIè·¯å¾„"""
    api_indicators = [
        '/api/v1/',
        '/api/',
        '/mcp/',
        '/health',
        '/metrics',
        '/status',
        '/workflows',
        '/agent',
        '/monitoring',
        '/documents',
        '/rag/',
        '/supervisor',
        '/security',
        '/events',
        '/fine-tuning',
        '/memories',
        '/entities',
        '/platform',
        '/reasoning'
    ]
    
    return any(indicator in path.lower() for indicator in api_indicators)

def normalize_path(path: str) -> str:
    """è§„èŒƒåŒ–è·¯å¾„"""
    # ç§»é™¤baseURLå˜é‡æ‹¼æ¥
    path = path.replace('${this.baseUrl}', '')
    path = path.replace('${API_BASE_URL}', '')
    
    # ç¡®ä¿ä»¥/å¼€å¤´
    if not path.startswith('/'):
        path = '/' + path
        
    return path

def find_service_usage(service_name: str, pages_dir: str) -> Dict[str, List[str]]:
    """æŸ¥æ‰¾æœåŠ¡çš„ä½¿ç”¨æƒ…å†µ"""
    usage_info = {
        'pages': [],
        'components': [],
        'hooks': []
    }
    
    pages_path = Path(pages_dir)
    if not pages_path.exists():
        return usage_info
    
    base_name = service_name.replace('.ts', '').replace('Service', '').replace('Api', '')
    
    # å¯èƒ½çš„å¯¼å…¥æ¨¡å¼
    import_patterns = [
        rf"import.*{base_name}Service",
        rf"import.*{base_name}Api", 
        rf"import.*{base_name}",
        rf"from.*{service_name.replace('.ts', '')}",
    ]
    
    try:
        # æœç´¢é¡µé¢æ–‡ä»¶
        for file_path in pages_path.rglob("*.tsx"):
            if 'pages' in str(file_path):
                file_type = 'pages'
            elif 'components' in str(file_path):
                file_type = 'components'  
            elif 'hooks' in str(file_path):
                file_type = 'hooks'
            else:
                file_type = 'pages'
                
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                for pattern in import_patterns:
                    if re.search(pattern, content, re.IGNORECASE):
                        relative_path = str(file_path.relative_to(pages_path))
                        usage_info[file_type].append(relative_path)
                        break
                        
            except Exception:
                continue
    except Exception:
        logger.exception("æ‰«æé¡µé¢æ–‡ä»¶å¤±è´¥", exc_info=True)
    
    return usage_info

def generate_summary_report(results: Dict, pages_dir: str) -> str:
    """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
    report = []
    report.append("# å‰ç«¯æœåŠ¡APIç«¯ç‚¹è°ƒç”¨åˆ†ææ±‡æ€»\n")
    
    # ç»Ÿè®¡ä¿¡æ¯
    services_with_apis = len([s for s in results if results[s]])
    total_endpoints = sum(len(endpoints) for endpoints in results.values())
    
    report.append(f"## æ€»è§ˆ")
    report.append(f"- åˆ†ææœåŠ¡æ–‡ä»¶æ•°é‡: {len(results)}")
    report.append(f"- åŒ…å«APIè°ƒç”¨çš„æœåŠ¡æ•°é‡: {services_with_apis}")
    report.append(f"- æ€»APIç«¯ç‚¹æ•°é‡: {total_endpoints}\n")
    
    # æŒ‰APIæ•°é‡æ’åºçš„æœåŠ¡åˆ—è¡¨
    sorted_services = sorted(
        [(name, endpoints) for name, endpoints in results.items() if endpoints],
        key=lambda x: len(x[1]),
        reverse=True
    )
    
    report.append("## æœåŠ¡APIè°ƒç”¨æ¦‚è§ˆ\n")
    report.append("| æœåŠ¡æ–‡ä»¶ | APIç«¯ç‚¹æ•°é‡ | ä¸»è¦APIç±»å‹ | ç›¸å…³é¡µé¢ |")
    report.append("|---------|------------|------------|---------|")
    
    for service_name, endpoints in sorted_services:
        # è·å–APIç±»å‹ç»Ÿè®¡
        api_types = {}
        for ep in endpoints:
            path_parts = ep['path'].strip('/').split('/')
            if len(path_parts) >= 2:
                api_type = f"/{path_parts[0]}/{path_parts[1]}"
            else:
                api_type = f"/{path_parts[0]}" if path_parts else "/"
            api_types[api_type] = api_types.get(api_type, 0) + 1
        
        main_api_type = max(api_types.items(), key=lambda x: x[1])[0] if api_types else "N/A"
        
        # è·å–ç›¸å…³é¡µé¢
        usage_info = find_service_usage(service_name, pages_dir)
        page_count = len(usage_info['pages'])
        page_info = f"{page_count} ä¸ªé¡µé¢" if page_count > 0 else "æ— "
        
        report.append(f"| {service_name} | {len(endpoints)} | {main_api_type} | {page_info} |")
    
    report.append("\n## APIç«¯ç‚¹åˆ†ç±»ç»Ÿè®¡\n")
    
    # ç»Ÿè®¡APIè·¯å¾„å‰ç¼€
    path_stats = {}
    method_stats = {}
    confidence_stats = {}
    
    for endpoints in results.values():
        for ep in endpoints:
            # è·¯å¾„å‰ç¼€ç»Ÿè®¡
            path_parts = ep['path'].strip('/').split('/')
            if len(path_parts) >= 2:
                prefix = f"/{path_parts[0]}/{path_parts[1]}"
            else:
                prefix = f"/{path_parts[0]}" if path_parts else "/"
            path_stats[prefix] = path_stats.get(prefix, 0) + 1
            
            # HTTPæ–¹æ³•ç»Ÿè®¡
            method_stats[ep['method']] = method_stats.get(ep['method'], 0) + 1
            
            # ç½®ä¿¡åº¦ç»Ÿè®¡
            confidence = ep.get('confidence', 'unknown')
            confidence_stats[confidence] = confidence_stats.get(confidence, 0) + 1
    
    report.append("### APIè·¯å¾„å‰ç¼€åˆ†å¸ƒ:")
    for prefix, count in sorted(path_stats.items(), key=lambda x: x[1], reverse=True):
        report.append(f"- `{prefix}`: {count} ä¸ªç«¯ç‚¹")
    
    report.append("\n### HTTPæ–¹æ³•åˆ†å¸ƒ:")
    for method, count in sorted(method_stats.items()):
        report.append(f"- `{method}`: {count} ä¸ªç«¯ç‚¹")
    
    report.append("\n### æ£€æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ:")
    for confidence, count in sorted(confidence_stats.items()):
        report.append(f"- {confidence}: {count} ä¸ªç«¯ç‚¹")
    
    # è¯¦ç»†æœåŠ¡æ˜ å°„
    report.append("\n## è¯¦ç»†æœåŠ¡APIæ˜ å°„\n")
    
    for service_name, endpoints in sorted_services:
        report.append(f"### {service_name}")
        
        usage_info = find_service_usage(service_name, pages_dir)
        if usage_info['pages']:
            report.append("**ç›¸å…³é¡µé¢:**")
            for page in usage_info['pages'][:3]:
                report.append(f"- {page}")
            if len(usage_info['pages']) > 3:
                report.append(f"- ... è¿˜æœ‰ {len(usage_info['pages']) - 3} ä¸ª")
        
        report.append("**APIç«¯ç‚¹:**")
        # æŒ‰HTTPæ–¹æ³•å’Œè·¯å¾„æ’åº
        sorted_endpoints = sorted(endpoints, key=lambda x: (x['method'], x['path']))
        for ep in sorted_endpoints:
            confidence_badge = "ğŸŸ¢" if ep.get('confidence') == 'high' else "ğŸŸ¡" if ep.get('confidence') == 'medium' else "ğŸ”´"
            report.append(f"- {confidence_badge} `{ep['method']} {ep['path']}` ({ep['pattern_type']})")
        
        report.append("")
    
    return "\n".join(report)

def main():
    """ä¸»å‡½æ•°"""
    base_path = "/Users/runout/awork/code/my_git/agent/apps/web/src"
    services_dir = os.path.join(base_path, "services")
    pages_dir = os.path.join(base_path, "pages")
    
    logger.info("å¼€å§‹æ”¹è¿›ç‰ˆAPIç«¯ç‚¹åˆ†æ...")
    
    results = {}
    services_path = Path(services_dir)
    
    if not services_path.exists():
        logger.info(f"ç›®å½•ä¸å­˜åœ¨: {services_dir}")
        return
    
    for file_path in services_path.glob("*.ts"):
        file_name = file_path.name
        logger.info(f"åˆ†ææ–‡ä»¶: {file_name}")
        
        endpoints = extract_api_endpoints_improved(str(file_path))
        results[file_name] = endpoints
        
        if endpoints:
            logger.info(f"  æ‰¾åˆ° {len(endpoints)} ä¸ªAPIç«¯ç‚¹")
    
    # ç”Ÿæˆæ”¹è¿›çš„æŠ¥å‘Š
    report = generate_summary_report(results, pages_dir)
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = "/Users/runout/awork/code/my_git/agent/frontend_api_analysis_improved.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    logger.info(f"\næ”¹è¿›ç‰ˆåˆ†æå®Œæˆ! æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    # ä¿å­˜è¯¦ç»†æ•°æ®
    json_file = "/Users/runout/awork/code/my_git/agent/frontend_api_analysis_improved.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    logger.info(f"è¯¦ç»†æ•°æ®å·²ä¿å­˜åˆ°: {json_file}")

if __name__ == "__main__":
    setup_logging()
    main()
