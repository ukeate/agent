import re
import os
from collections import defaultdict
from typing import Dict, List, Set
from src.core.logging import setup_logging

from src.core.logging import get_logger
logger = get_logger(__name__)

#!/usr/bin/env python3
"""
æ£€æŸ¥æµ‹è¯•é‡å¤æ€§è„šæœ¬
ç¡®ä¿æ²¡æœ‰é‡å¤çš„APIæµ‹è¯•ï¼Œæ¯ä¸ªç«¯ç‚¹åªæœ‰ä¸€ä¸ªå¯¹åº”çš„æµ‹è¯•ç”¨ä¾‹
"""

def extract_endpoints_from_test_file(file_path: str) -> List[str]:
    """ä»æµ‹è¯•æ–‡ä»¶ä¸­æå–APIç«¯ç‚¹"""
    if not os.path.exists(file_path):
        return []
    
    endpoints = []
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æŸ¥æ‰¾æ‰€æœ‰çš„APIç«¯ç‚¹URLæ¨¡å¼
    url_patterns = [
        r'f"{BASE_URL}([^"]+)"',  # f"{BASE_URL}/api/endpoint"
        r'"[^"]*(/[^"]*)"',       # "/api/endpoint"
        r"'[^']*(/[^']*)'",       # '/api/endpoint'
    ]
    
    for pattern in url_patterns:
        matches = re.findall(pattern, content)
        for match in matches:
            if match.startswith('/') and len(match) > 1:
                # æ¸…ç†ç«¯ç‚¹è·¯å¾„ï¼Œç§»é™¤å‚æ•°
                endpoint = re.sub(r'\{[^}]+\}', '{id}', match)  # ç»Ÿä¸€è·¯å¾„å‚æ•°
                endpoint = re.sub(r'\?.*$', '', endpoint)  # ç§»é™¤æŸ¥è¯¢å‚æ•°
                endpoints.append(endpoint)
    
    return endpoints

def analyze_test_coverage():
    """åˆ†ææµ‹è¯•è¦†ç›–æƒ…å†µ"""
    test_files = {
        'test_detailed_api_logic.py': 'æ ¸å¿ƒAPIæ¨¡å—æµ‹è¯•',
        'test_remaining_apis_logic.py': 'å‰©ä½™APIæ¨¡å—æµ‹è¯•', 
        'test_advanced_api_modules.py': 'é«˜çº§APIæ¨¡å—æµ‹è¯•',
        'test_complete_api_no_duplicates.py': 'å®Œæ•´APIæµ‹è¯•å¥—ä»¶-æ— é‡å¤ç‰ˆ'
    }
    
    all_endpoints = defaultdict(list)  # endpoint -> [file1, file2, ...]
    file_endpoints = {}  # file -> [endpoints]
    
    logger.info("ğŸ” æ£€æŸ¥APIæµ‹è¯•é‡å¤æ€§")
    logger.info("=" * 60)
    
    # åˆ†ææ¯ä¸ªæµ‹è¯•æ–‡ä»¶
    for test_file, description in test_files.items():
        endpoints = extract_endpoints_from_test_file(test_file)
        file_endpoints[test_file] = endpoints
        
        logger.info(f"\nğŸ“‹ {test_file}")
        logger.info(f"æè¿°: {description}")
        logger.info(f"ç«¯ç‚¹æ•°é‡: {len(endpoints)}")
        
        for endpoint in endpoints:
            all_endpoints[endpoint].append(test_file)
    
    # æŸ¥æ‰¾é‡å¤çš„ç«¯ç‚¹
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ” é‡å¤ç«¯ç‚¹æ£€æŸ¥")
    logger.info("=" * 60)
    
    duplicates = {}
    unique_endpoints = set()
    
    for endpoint, files in all_endpoints.items():
        if len(files) > 1:
            duplicates[endpoint] = files
        unique_endpoints.add(endpoint)
    
    if duplicates:
        logger.warning("âš ï¸  å‘ç°é‡å¤æµ‹è¯•çš„ç«¯ç‚¹:")
        for endpoint, files in duplicates.items():
            logger.info(f"  â€¢ {endpoint}")
            logger.info(f"    é‡å¤åœ¨: {', '.join(files)}")
    else:
        logger.info("âœ… æ²¡æœ‰å‘ç°é‡å¤æµ‹è¯•çš„ç«¯ç‚¹")
    
    # ç»Ÿè®¡ä¿¡æ¯
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“Š æµ‹è¯•è¦†ç›–ç»Ÿè®¡")
    logger.info("=" * 60)
    
    total_tests = sum(len(endpoints) for endpoints in file_endpoints.values())
    unique_count = len(unique_endpoints)
    duplicate_count = total_tests - unique_count
    
    logger.info(f"æ€»æµ‹è¯•æ•°é‡: {total_tests}")
    logger.info(f"å”¯ä¸€ç«¯ç‚¹æ•°: {unique_count}")
    logger.info(f"é‡å¤æµ‹è¯•æ•°: {duplicate_count}")
    logger.info(f"é‡å¤ç‡: {duplicate_count/total_tests*100:.1f}%")
    
    # è¯¦ç»†åˆ†ææ¯ä¸ªæ–‡ä»¶
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“‹ æ–‡ä»¶è¯¦ç»†åˆ†æ")
    logger.info("=" * 60)
    
    for file_name, endpoints in file_endpoints.items():
        unique_in_file = len(set(endpoints))
        duplicates_in_file = len(endpoints) - unique_in_file
        
        logger.info(f"\n{file_name}:")
        logger.info(f"  æ€»ç«¯ç‚¹: {len(endpoints)}")
        logger.info(f"  å”¯ä¸€ç«¯ç‚¹: {unique_in_file}")
        logger.info(f"  æ–‡ä»¶å†…é‡å¤: {duplicates_in_file}")
    
    # ç”Ÿæˆæ¸…ç†å»ºè®®
    if duplicates:
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ”§ æ¸…ç†å»ºè®®")
        logger.info("=" * 60)
        
        logger.info("å»ºè®®ä¿ç•™ç­–ç•¥:")
        logger.info("1. test_detailed_api_logic.py - ä¿ç•™æ ¸å¿ƒæ¨¡å—æµ‹è¯•")
        logger.info("2. test_remaining_apis_logic.py - ä¿ç•™è¡¥å……æ¨¡å—æµ‹è¯•")  
        logger.info("3. test_advanced_api_modules.py - ä¿ç•™é«˜çº§æ¨¡å—æµ‹è¯•")
        logger.info("\nå…·ä½“é‡å¤ç«¯ç‚¹å¤„ç†:")
        for endpoint, files in duplicates.items():
            # å»ºè®®ä¿ç•™å“ªä¸ªæ–‡ä»¶ä¸­çš„æµ‹è¯•
            if 'test_detailed_api_logic.py' in files:
                keep_file = 'test_detailed_api_logic.py'
            elif 'test_remaining_apis_logic.py' in files:
                keep_file = 'test_remaining_apis_logic.py'
            else:
                keep_file = files[0]
            
            remove_files = [f for f in files if f != keep_file]
            logger.info(f"  â€¢ {endpoint}")
            logger.info(f"    ä¿ç•™: {keep_file}")
            logger.info(f"    ç§»é™¤: {', '.join(remove_files)}")
    
    return {
        'total_tests': total_tests,
        'unique_endpoints': unique_count,
        'duplicates': duplicates,
        'file_endpoints': file_endpoints
    }

def create_unified_test_file():
    """åˆ›å»ºç»Ÿä¸€çš„ã€æ— é‡å¤çš„æµ‹è¯•æ–‡ä»¶"""
    analysis = analyze_test_coverage()
    
    if not analysis['duplicates']:
        logger.info("\nâœ… æ— éœ€åˆ›å»ºç»Ÿä¸€æµ‹è¯•æ–‡ä»¶ï¼Œå½“å‰æµ‹è¯•å·²æ— é‡å¤")
        return
    
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ”§ åˆ›å»ºç»Ÿä¸€æµ‹è¯•æ–‡ä»¶")
    logger.info("=" * 60)
    
    # è¿™é‡Œå¯ä»¥å®ç°åˆ›å»ºç»Ÿä¸€æµ‹è¯•æ–‡ä»¶çš„é€»è¾‘
    # ä½†ç”±äºæ¶‰åŠå¤æ‚çš„ä»£ç åˆå¹¶ï¼Œæš‚æ—¶åªæä¾›åˆ†æç»“æœ
    logger.info("ç»Ÿä¸€æµ‹è¯•æ–‡ä»¶åˆ›å»ºåŠŸèƒ½å¼€å‘ä¸­...")
    logger.info("å½“å‰å»ºè®®: æ‰‹åŠ¨ç§»é™¤é‡å¤æµ‹è¯•ï¼Œä¿ç•™æœ€å®Œæ•´çš„ç‰ˆæœ¬")

if __name__ == "__main__":
    setup_logging()
    try:
        analysis = analyze_test_coverage()
        
        # å¦‚æœæœ‰é‡å¤ï¼Œè¯¢é—®æ˜¯å¦åˆ›å»ºç»Ÿä¸€ç‰ˆæœ¬
        if analysis['duplicates']:
            logger.info(f"\nå‘ç° {len(analysis['duplicates'])} ä¸ªé‡å¤æµ‹è¯•çš„ç«¯ç‚¹")
            # create_unified_test_file()
        else:
            logger.info(f"\nğŸ‰ æµ‹è¯•è¦†ç›–è‰¯å¥½ï¼{analysis['unique_endpoints']} ä¸ªå”¯ä¸€ç«¯ç‚¹ï¼Œæ— é‡å¤æµ‹è¯•")
            
    except Exception as e:
        logger.error(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
