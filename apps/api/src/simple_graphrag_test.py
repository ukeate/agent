#!/usr/bin/env python3
"""
ç®€å•çš„GraphRAGç³»ç»ŸéªŒè¯è„šæœ¬

ä»…éªŒè¯æ ¸å¿ƒæ•°æ®æ¨¡å‹å’ŒåŸºç¡€åŠŸèƒ½ï¼Œä¸æ¶‰åŠå¤æ‚ä¾èµ–
"""

import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_data_models():
    """éªŒè¯GraphRAGæ•°æ®æ¨¡å‹"""
    try:
        # ç›´æ¥å¯¼å…¥æ•°æ®æ¨¡å‹æ¨¡å—
        from ai.graphrag.data_models import (
            GraphRAGRequest,
            GraphContext,
            ReasoningPath,
            KnowledgeSource,
            QueryDecomposition,
            FusionResult,
            EntityRecognitionResult,
            GraphRAGConfig,
            create_graph_rag_request,
            create_empty_graph_context,
            validate_graph_rag_request,
            RetrievalMode,
            QueryType
        )
        
        print("âœ“ GraphRAGæ•°æ®æ¨¡å‹å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•åˆ›å»ºGraphRAGè¯·æ±‚
        request = create_graph_rag_request(
            query="ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
            retrieval_mode=RetrievalMode.HYBRID,
            max_docs=10
        )
        print("âœ“ GraphRAGè¯·æ±‚åˆ›å»ºæˆåŠŸ")
        print(f"  - æŸ¥è¯¢: {request['query']}")
        print(f"  - æ£€ç´¢æ¨¡å¼: {request['retrieval_mode']}")
        print(f"  - æœ€å¤§æ–‡æ¡£æ•°: {request['max_docs']}")
        
        # æµ‹è¯•è¯·æ±‚éªŒè¯
        errors = validate_graph_rag_request(request)
        print(f"âœ“ è¯·æ±‚éªŒè¯å®Œæˆï¼Œé”™è¯¯æ•°: {len(errors)}")
        
        # æµ‹è¯•ç©ºä¸Šä¸‹æ–‡åˆ›å»º
        empty_context = create_empty_graph_context()
        print(f"âœ“ ç©ºä¸Šä¸‹æ–‡åˆ›å»ºæˆåŠŸ")
        print(f"  - å®ä½“æ•°: {len(empty_context.entities)}")
        print(f"  - å…³ç³»æ•°: {len(empty_context.relations)}")
        print(f"  - ç½®ä¿¡åº¦: {empty_context.confidence_score}")
        
        # æµ‹è¯•GraphContextåˆ›å»ºå’Œåºåˆ—åŒ–
        context = GraphContext(
            entities=[{"id": "1", "name": "æœºå™¨å­¦ä¹ ", "type": "CONCEPT"}],
            relations=[{"type": "PART_OF", "source": "1", "target": "2"}],
            subgraph={"nodes": 1, "edges": 1},
            reasoning_paths=[],
            expansion_depth=2,
            confidence_score=0.8
        )
        context_dict = context.to_dict()
        print("âœ“ GraphContextåˆ›å»ºå’Œåºåˆ—åŒ–æˆåŠŸ")
        print(f"  - å®ä½“æ•°: {len(context.entities)}")
        print(f"  - å…³ç³»æ•°: {len(context.relations)}")
        print(f"  - æ‰©å±•æ·±åº¦: {context.expansion_depth}")
        print(f"  - ç½®ä¿¡åº¦: {context.confidence_score}")
        
        # æµ‹è¯•æ¨ç†è·¯å¾„
        path = ReasoningPath(
            path_id="test_path_1",
            entities=["æœºå™¨å­¦ä¹ ", "äººå·¥æ™ºèƒ½"],
            relations=["IS_A"],
            path_score=0.9,
            explanation="æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯",
            evidence=[{"fact": "MLæ˜¯AIçš„å­é¢†åŸŸ", "confidence": 0.8}],
            hops_count=1
        )
        path_dict = path.to_dict()
        print("âœ“ æ¨ç†è·¯å¾„åˆ›å»ºå’Œåºåˆ—åŒ–æˆåŠŸ")
        print(f"  - è·¯å¾„ID: {path.path_id}")
        print(f"  - å®ä½“æ•°: {len(path.entities)}")
        print(f"  - è·¯å¾„è¯„åˆ†: {path.path_score}")
        print(f"  - è·³æ•°: {path.hops_count}")
        
        # æµ‹è¯•çŸ¥è¯†æº
        source = KnowledgeSource(
            source_type="vector",
            content="æœºå™¨å­¦ä¹ æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½æŠ€æœ¯",
            confidence=0.85,
            metadata={"source": "wikipedia", "section": "definition"}
        )
        source_dict = source.to_dict()
        print("âœ“ çŸ¥è¯†æºåˆ›å»ºå’Œåºåˆ—åŒ–æˆåŠŸ")
        print(f"  - æºç±»å‹: {source.source_type}")
        print(f"  - å†…å®¹é•¿åº¦: {len(source.content)}")
        print(f"  - ç½®ä¿¡åº¦: {source.confidence}")
        print(f"  - å…ƒæ•°æ®: {source.metadata}")
        
        # æµ‹è¯•æŸ¥è¯¢åˆ†è§£
        decomposition = QueryDecomposition(
            original_query="ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
            sub_queries=["æœºå™¨å­¦ä¹ å®šä¹‰", "æœºå™¨å­¦ä¹ åº”ç”¨", "æœºå™¨å­¦ä¹ ç®—æ³•"],
            entity_queries=[{"entity": "æœºå™¨å­¦ä¹ ", "type": "CONCEPT"}],
            relation_queries=[{"entity1": "æœºå™¨å­¦ä¹ ", "entity2": "äººå·¥æ™ºèƒ½", "relation": "PART_OF"}],
            decomposition_strategy="semantic_analysis",
            complexity_score=0.6
        )
        decomp_dict = decomposition.to_dict()
        print("âœ“ æŸ¥è¯¢åˆ†è§£åˆ›å»ºå’Œåºåˆ—åŒ–æˆåŠŸ")
        print(f"  - åŸå§‹æŸ¥è¯¢: {decomposition.original_query}")
        print(f"  - å­æŸ¥è¯¢æ•°: {len(decomposition.sub_queries)}")
        print(f"  - å®ä½“æŸ¥è¯¢æ•°: {len(decomposition.entity_queries)}")
        print(f"  - å…³ç³»æŸ¥è¯¢æ•°: {len(decomposition.relation_queries)}")
        print(f"  - å¤æ‚åº¦: {decomposition.complexity_score}")
        
        # æµ‹è¯•å®ä½“è¯†åˆ«ç»“æœ
        entity_result = EntityRecognitionResult(
            text="æœºå™¨å­¦ä¹ ",
            canonical_form="æœºå™¨å­¦ä¹ ",
            entity_type="CONCEPT",
            confidence=0.9,
            start_pos=0,
            end_pos=4,
            metadata={"method": "nlp_analysis"}
        )
        entity_dict = entity_result.to_dict()
        print("âœ“ å®ä½“è¯†åˆ«ç»“æœåˆ›å»ºå’Œåºåˆ—åŒ–æˆåŠŸ")
        print(f"  - è¯†åˆ«æ–‡æœ¬: {entity_result.text}")
        print(f"  - æ ‡å‡†å½¢å¼: {entity_result.canonical_form}")
        print(f"  - å®ä½“ç±»å‹: {entity_result.entity_type}")
        print(f"  - ç½®ä¿¡åº¦: {entity_result.confidence}")
        
        # æµ‹è¯•èåˆç»“æœ - TypedDictç‰ˆæœ¬
        fusion_result = FusionResult(
            final_ranking=[{"source": source.to_dict(), "rank": 1}],
            confidence_scores={"vector": 0.85, "graph": 0.78},
            conflicts_detected=[],
            resolution_strategy="weighted_consensus",
            consistency_score=0.88
        )
        # TypedDictæœ¬èº«å°±æ˜¯dictï¼Œä¸éœ€è¦to_dict()æ–¹æ³•
        print("âœ“ èåˆç»“æœåˆ›å»ºå’Œåºåˆ—åŒ–æˆåŠŸ")
        print(f"  - æœ€ç»ˆæ’åæ•°: {len(fusion_result['final_ranking'])}")
        print(f"  - ä¸€è‡´æ€§è¯„åˆ†: {fusion_result['consistency_score']}")
        print(f"  - è§£å†³ç­–ç•¥: {fusion_result['resolution_strategy']}")
        print(f"  - ç½®ä¿¡åº¦è¯„åˆ†: {fusion_result['confidence_scores']}")
        
        # æµ‹è¯•é…ç½®
        config = GraphRAGConfig()
        config_dict = config.to_dict()
        print("âœ“ GraphRAGé…ç½®åˆ›å»ºå’Œåºåˆ—åŒ–æˆåŠŸ")
        print(f"  - æœ€å¤§æ‰©å±•æ·±åº¦: {config.max_expansion_depth}")
        print(f"  - ç½®ä¿¡åº¦é˜ˆå€¼: {config.confidence_threshold}")
        print(f"  - å¯ç”¨ç¼“å­˜: {config.enable_caching}")
        print(f"  - ç¼“å­˜TTL: {config.cache_ttl}")
        
        # æµ‹è¯•post_initæ–¹æ³•
        print("\næµ‹è¯•æ•°æ®éªŒè¯å’Œåå¤„ç†...")
        
        # æµ‹è¯•ç½®ä¿¡åº¦é™åˆ¶
        high_confidence_context = GraphContext(
            entities=[],
            relations=[],
            subgraph={},
            reasoning_paths=[],
            expansion_depth=1,
            confidence_score=1.5  # è¶…è¿‡1.0ï¼Œåº”è¯¥è¢«é™åˆ¶
        )
        print(f"âœ“ ç½®ä¿¡åº¦é™åˆ¶éªŒè¯: {high_confidence_context.confidence_score} (åº”ä¸º1.0)")
        
        # æµ‹è¯•Noneå€¼è½¬æ¢
        none_context = GraphContext(
            entities=None,
            relations=None,
            subgraph=None,
            reasoning_paths=None,
            expansion_depth=1,
            confidence_score=0.5
        )
        print(f"âœ“ Noneå€¼è½¬æ¢éªŒè¯: å®ä½“={len(none_context.entities)}, å…³ç³»={len(none_context.relations)}")
        
        return True
        
    except Exception as e:
        print(f"âœ— æ•°æ®æ¨¡å‹éªŒè¯å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("GraphRAGæ•°æ®æ¨¡å‹éªŒè¯")
    print("=" * 50)
    
    success = test_data_models()
    
    print("\n" + "=" * 50)
    if success:
        print("ğŸ‰ GraphRAGæ•°æ®æ¨¡å‹éªŒè¯æˆåŠŸ!")
        return 0
    else:
        print("âŒ GraphRAGæ•°æ®æ¨¡å‹éªŒè¯å¤±è´¥!")
        return 1

if __name__ == "__main__":
    sys.exit(main())