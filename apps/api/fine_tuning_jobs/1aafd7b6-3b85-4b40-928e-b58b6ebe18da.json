{
  "job_id": "1aafd7b6-3b85-4b40-928e-b58b6ebe18da",
  "job_name": "tiny-pause",
  "status": "completed",
  "created_at": "2025-12-20T04:58:43.157031+00:00",
  "started_at": "2025-12-20T04:58:43.157715+00:00",
  "completed_at": "2025-12-20T04:58:56.437069+00:00",
  "progress": 100.0,
  "current_epoch": 50,
  "total_epochs": 50,
  "current_loss": null,
  "best_loss": null,
  "error_message": null,
  "config": {
    "model_name": "hf-internal-testing/tiny-random-LlamaForCausalLM",
    "model_architecture": "llama",
    "training_mode": "lora",
    "dataset_path": "./datasets/test_fine_tuning_ds.json",
    "output_dir": "./fine_tuned_models/1aafd7b6-3b85-4b40-928e-b58b6ebe18da",
    "learning_rate": 0.0002,
    "num_train_epochs": 50,
    "per_device_train_batch_size": 2,
    "gradient_accumulation_steps": 1,
    "warmup_steps": 0,
    "max_seq_length": 256,
    "lora_config": {
      "rank": 8,
      "alpha": 16,
      "dropout": 0.1,
      "target_modules": null,
      "bias": "none",
      "task_type": "CAUSAL_LM",
      "inference_mode": false
    },
    "quantization_config": null,
    "use_distributed": false,
    "world_size": 1,
    "use_deepspeed": false,
    "deepspeed_config": null,
    "use_flash_attention": false,
    "use_gradient_checkpointing": false,
    "fp16": false,
    "bf16": false,
    "logging_steps": 10,
    "save_steps": 500,
    "eval_steps": 500
  },
  "result": {
    "train_runtime": 8.2962,
    "train_samples_per_second": 6.027,
    "train_steps_per_second": 6.027,
    "train_loss": 9.794094390869141,
    "final_model_path": "./fine_tuned_models/1aafd7b6-3b85-4b40-928e-b58b6ebe18da",
    "total_epochs": 50,
    "total_steps": 50,
    "config_path": "./fine_tuned_models/1aafd7b6-3b85-4b40-928e-b58b6ebe18da/training_config.json"
  }
}